{"path":".obsidian/plugins/text-extractor/cache/7c76b99df1590e23010efa3f29d451ac.json","text":"CPSC 340: Machine Learning and Data Mining Probabilistic Classification Admin • Assignment 1 is due tonight: you should be almost done. – You can use 1 late days to submit Monday, 2 for Wednesday. • Waiting list people: everyone should be in. • Auditors: – Bring your forms at the end of class. • Exchange students: – If you are still having trouble registering, bring me your forms. – Contact us on Piazza about getting registered for Gradescope. • CPSC 540: – Details of project posted. Last Time: Training, Testing, and Validation • Training step: • Prediction step: • What we are interested in is the test error: – Error made by prediction step on new data. Last Time: Fundamental Trade-Off • We decomposed test error to get a fundamental trade-off: – Where Egap = (Etest – Etrain). • Etrain goes down as model gets complicated: – Training error goes down as a decision tree gets deeper. • But Egap goes up as model gets complicated: – Training error becomes a worse approximation of test error. Last Time: Validation Error • Key principle: we cannot let test data influence training. • But we can approximate Etest with a validation error: – Error on a set of training examples we “hid” during training. – Find the decision tree based on the “train” rows. – Validation error is the error of the decision tree on the “validation” rows. • We typically choose “hyper-parameters” like depth to minimize the validation error. Example of Optimization Bias • Consider a multiple-choice (a,b,c,d) “test” with 10 questions: – If you choose answers randomly, expected grade is 25% (no bias). – If you fill out two tests randomly and pick the best, expected grade is 33%. • Optimization bias of ~8%. – If you take the best among 10 random tests, expected grade is ~47%. – If you take the best among 100, expected grade is ~62%. – If you take the best among 1000, expected grade is ~73%. – If you take the best among 10000, expected grade is ~82%. • You have so many “chances” that you expect to do well. • But on new questions the “random choice” accuracy is still 25%. Factors Affecting Optimization Bias • If we instead used a 100-question test then: – Expected grade from best over 1 randomly-filled test is 25%. – Expected grade from best over 2 randomly-filled test is ~27%. – Expected grade from best over 10 randomly-filled test is ~32%. – Expected grade from best over 100 randomly-filled test is ~36%. – Expected grade from best over 1000 randomly-filled test is ~40%. – Expected grade from best over 10000 randomly-filled test is ~47%. • The optimization bias grows with the number of things we try. – “Complexity” of the set of models we search over. • But, optimization bias shrinks fast with number of validation examples. – But it’s still non-zero and growing if you over-use your validation set! Optimization Bias in Machine Learning Competitions • It is common to have machine learning “competitions”. – Some company releases a training set. • Many people try many different things to try to develop the “best” model. – At the end of the competition, the methods are compared on unseen test data. • And a “winner” or “winners” are declared based on the test set performance. • In some cases, this has led to major new insights on ML methods. – Including the rise in popularity of “deep learning” methods we’ll see later. • In most cases, many people submit very-similar methods. – Expected “best test error” from 10000 similar submissions is biased! • The “best” methods might just be the one that got the most lucky. Optimization Bias in Machine Learning Benchmarks https://arxiv.org/abs/2109.08203 Overfitting to the Validation Set? • Validation error usually has lower optimization bias than training error. – Might optimize over 20 values of “depth”, instead of millions+ of possible trees. • But we can still overfit to the validation error (common in practice): – Validation error is only an unbiased approximation if you use it once. – Once you start optimizing it, you start to overfit to the validation set. • This is most important when the validation set is “small”: – The optimization bias decreases as the number of validation examples increases. • Remember, our goal is still to do well on the test set (new data), not the validation set (where we already know the labels). Should you trust them? • Scenario 1: – “I built a model based on the data you gave me.” – “It classified your data with 98% accuracy.” – “It should get 98% accuracy on the rest of your data.” • Probably not: – They are reporting training error. – This might have nothing to do with test error. – E.g., they could have fit a very deep decision tree. • Why ‘probably’? – If they only tried a few very simple models, the 98% might be reliable. – E.g., they only considered decision stumps with simple 1-variable rules. Should you trust them? • Scenario 2: – “I built a model based on half of the data you gave me.” – “It classified the other half of the data with 98% accuracy.” – “It should get 98% accuracy on the rest of your data.” • Probably: – They computed the validation error once. – This is an unbiased approximation of the test error. – Trust them if you believe second half of data did not influence training. Should you trust them? • Scenario 3: – “I built 10 models based on half of the data you gave me.” – “One of them classified the other half of the data with 98% accuracy.” – “It should get 98% accuracy on the rest of your data.” • Probably: – They computed the validation error a small number of times. – Maximizing over these errors is a biased approximation of test error. – But they only maximized it over 10 models, so bias is probably small. – Probably know key principle of not letting test data influence training. Should you trust them? • Scenario 4: – “I built 1 billion models based on half of the data you gave me.” – “One of them classified the other half of the data with 98% accuracy.” – “It should get 98% accuracy on the rest of your data.” • Probably not: – They computed the validation error a huge number of times. – They tried so many models, one of them is likely to work by chance. • Why ‘probably’? – If the 1 billion models were all extremely-simple, 98% might be reliable. Should you trust them? • Scenario 5: – “I built 1 billion models based on the first third of the data you gave me.” – “One of them classified the second third of the data with 98% accuracy.” – “It also classified the last third of the data with 98% accuracy.” – “It should get 98% accuracy on the rest of your data.” • Probably: – They computed the first validation error a huge number of times. – But they had a second validation set that they only looked at once. – The second validation set gives unbiased test error approximation. – This is ideal, as long last third is not influencing training. – And assuming you are using IID data in the first place. Train/Validation/Test Terminology • Training set: used (a lot) to set parameters. • Validation set: used (a few times) to set hyper-parameters. • Testing set: used (once) to evaluate final performance. • Deployment (real-world): what you really care about. Validation Error and Optimization Bias • Optimization bias is small if you only compare a few models: – Best decision tree on the training set among depths 1, 2, 3,…, 10. – Risk of overfitting to validation set is low if we try 10 things. • Optimization bias is large if you compare a lot of models: – All possible decision trees of depth 10 or less. – Here we’re using the validation set to pick between a billion+ models: • Risk of overfitting to validation set is high: could have low validation error by chance. – If you did this, you might want a second validation set to detect overfitting. • And optimization bias shrinks as you grow size of validation set. Aside: Optimization Bias leads to Publication Bias • Suppose that 20 researchers perform the exact same experiment: • They each test whether their effect is “significant” (p < 0.05). – 19/20 find that it is not significant. – But the 1 group finding it’s significant publishes a paper about the effect. • This is again optimization bias, contributing to publication bias. – A contributing factor to many reported effects being wrong. Cross-Validation (CV) • Isn’t it wasteful to only use part of your data? • 5-fold cross-validation: – Train on 80% of the data, validate on the other 20%. – Repeat this 5 times with the different splits, and average the score. Cross-Validation (CV) TRAIN TRAIN TRAIN TRAIN VALIDATION TRAIN TRAIN TRAIN VALIDATION TRAIN TRAIN TRAIN VALIDATION TRAIN TRAIN TRAIN VALIDATION TRAIN TRAIN TRAIN VALIDATION TRAIN TRAIN TRAIN TRAIN Cross-Validation Pseudo-CodeCross-Validation (CV) • You can take this idea further (“k-fold cross-validation”): – 10-fold cross-validation: train on 90% of data and validate on 10%. • Repeat 10 times and average (test on fold 1, then fold 2,…, then fold 10), – Leave-one-out cross-validation: train on all but one training example. • Repeat n times and average. • Gets more accurate but more expensive with more folds. – To choose depth we compute the cross-validation score for each depth. • As before, if data is ordered then folds should be random splits. – Randomize first, then split into fixed folds. Next Topic: Probabilistic Classifiers Generalization Error • An alternative to test error is the generalization error: – Average error over all xi vectors that are not seen in the training set. • Assuming each unseen xi is equally probable. • “Error averaged over all completely unseen feature vectors”. – Different than test error, which assumes IID data from same distribution. • Test error allows some xi to be more probable, and does not exclude training xi. The “Best” Machine Learning Model • Decision trees are not always most accurate on test error. • What is the “best” machine learning model? • No free lunch theorem (proof in bonus slides): – There is no “best” model achieving the best generalization error for every problem. – If model A generalizes better to new data than model B on one dataset, there is another dataset where model B works better. • This question is like asking which is “best” among “rock”, “paper”, and “scissors”. The “Best” Machine Learning Model • Implications of the lack of a “best” model: – We need to learn about and try out multiple models. • So which ones to study in CPSC 340? – We’ll usually motivate each method by a specific application. – But we’re focusing on models that have been effective in many applications. • Caveat of no free lunch (NFL) theorem: – The world is very structured. • But proof of the no-free-lunch theorem assumes any map from xi to yi is equally likely. – Some datasets are more likely than others. – Model A really could be better than model B on every real dataset in practice. • Machine learning research: – Large focus on models that are useful across many applications. Application: E-mail Spam Filtering • Want a build a system that detects spam e-mails. – Context: spam used to be a big problem. • Can we formulate as supervised learning? Spam Filtering as Supervised Learning • Collect a large number of e-mails, gets users to label them. • We can use (yi = 1) if e-mail ‘i’ is spam, (yi = 0) if e-mail is not spam. • Extract features of each e-mail (like bag of words). – (xij = 1) if word/phrase ‘j’ is in e-mail ‘i’, (xij = 0) if it is not. $ Hi CPSC 340 Vicodin Offer … 1 1 0 0 1 0 … 0 0 0 0 1 1 … 0 1 1 1 0 0 … … … … … … … … Spam? 1 1 0 … Feature Representation for Spam • Are there better features than bag of words? – We add bigrams (sets of two words): • “CPSC 340”, “wait list”, “special deal”. – Or trigrams (sets of three words): • “Limited time offer”, “course registration deadline”, “you’re a winner”. – We might include the sender domain: • <sender domain == “mail.com”>. – We might include regular expressions: • <your first and last name>. Review of Supervised Learning Notation • We have been using the notation ‘X’ and ‘y’ for supervised learning: • X is matrix of all features, y is vector of all labels. – We use yi for the label of example ‘i’ (element ‘i’ of ‘y’). – We use xij for feature ‘j’ of example ‘i‘. – We use xi as the list of features of example ‘i’ (row ‘i’ of ‘X’). • So in the above x3 = [0 1 1 1 0 0 …]. • In practice, only store list of non-zero features for each xi (small memory requirement). $ Hi CPSC 340 Vicodin Offer … 1 1 0 0 1 0 … 0 0 0 0 1 1 … 0 1 1 1 0 0 … … … … … … … … Spam? 1 1 0 … Probabilistic Classifiers • For years, best spam filtering methods used naïve Bayes. – A probabilistic classifier based on Bayes rule. – It tends to work well with bag of words. – Recently shown to improve on state of the art for CRISPR “gene editing” (link). • Probabilistic classifiers build a model of the conditional probability, p(yi | xi). – “If a message has words xi, what is probability that message is spam?” • Classify it as spam if probability of spam is higher than not spam: – If p(yi = “spam” | xi) > p(yi = “not spam” | xi) • return “spam”. – Else • return “not spam”. Spam Filtering with Bayes Rule • To model conditional probability, naïve Bayes uses Bayes rule: • Nice video giving visual intuition for Bayes rule here: Spam Filtering with Bayes Rule • To model conditional probability, naïve Bayes uses Bayes rule: • On the right we have three terms: – Marginal probability p(yi) that an e-mail is spam. – Marginal probability p(xi) that an e-mail has the set of words xi. – Conditional probability p(xi | yi) that a spam e-mail has the words xi. • And the same for non-spam e-mails. Spam Filtering with Bayes Rule • What do these terms mean? ALL E-MAILS (including duplicates) Spam Filtering with Bayes Rule • p(yi = “spam”) is probability that a random e-mail is spam. – This is easy to approximate from data: use the proportion in your data. ALL E-MAILS (including duplicates)SPAM NOT SPAM This is an “estimate” of the true probability. In particular, this formula is a “maximum likelihood estimate” (MLE). We will cover likelihoods and MLEs later in the course. Spam Filtering with Bayes Rule • p(xi) is probability that a random e-mail has features xi: – Hard to approximate: with ‘d’ words we need to collect 2d “coupons”, and that’s just to see each word combination once. ALL E-MAILS (including duplicates) Spam Filtering with Bayes Rule • p(xi) is probability that a random e-mail has features xi: – Hard to approximate: with ‘d’ words we need to collect 2d “coupons”, but it turns out we can ignore it: Spam Filtering with Bayes Rule • p(xi | yi = “spam”) is probability that spam has features xi. ALL E-MAILS (including duplicates) NOT SPAM SPAM • Also hard to approximate. • And we need it. Naïve Bayes • Naïve Bayes makes a big assumption to make things easier: • We assume all features xi are conditionally independent give label yi. – Once you know it’s spam, probability of “vicodin” doesn’t depend on “340”. – Definitely not true, but sometimes a good approximation. • And now we only need easy quantities like p(“vicodin” = 0| yi = “spam”). Naïve Bayes • p(“vicodin” = 1 | “spam” = 1) is probability of seeing “vicodin” in spam. ALL POSSIBLE E-MAILS (including duplicates)SPAM NOT SPAM • Easy to estimate: Vicodin Again, this is a “maximum likelihood estimate” (MLE). We will cover how to derive this later. Naïve Bayes • Comparing p(x | y = c) for “spam” and “not spam”: • Even though independence is not true, these values may be enough to distinguish the classes. Summary • Optimization bias: using a validation set too much overfits. • Cross-validation: allows better use of data to estimate test error. • No free lunch theorem: there is no “best” ML model. • Probabilistic classifiers: try to estimate p(yi | xi). • Naïve Bayes: simple probabilistic classifier based on counting. – Uses conditional independence assumptions to make training practical. • Next time: – A “best” machine learning model as ‘n’ goes to ∞. Back to Decision Trees • Instead of validation set, you can use CV to select tree depth. • But you can also use these to decide whether to split: – Don’t split if validation/CV error doesn’t improve. – Different parts of the tree will have different depths. • Or fit deep decision tree and use [cross-]validation to prune: – Remove leaf nodes that don’t improve CV error. • Popular implementations that have these tricks and others. Random Subsamples • Instead of splitting into k-folds, consider “random subsample” method: – At each “round”, choose a random set of size ‘m’. • Train on all examples except these ‘m’ examples. • Compute validation error on these ‘m’ examples. • Advantages: – Still an unbiased estimator of error. – Number of “rounds” does not need to be related to “n”. • Disadvantage: – Examples that are sampled more often get more “weight”. Cross-Validation Theory • Does CV give unbiased estimate of test error? – Yes! • Since each data point is only used once in validation, expected validation error on each data point is test error. – But again, if you use CV to select among models then it is no longer unbiased. • What about variance of CV? – Hard to characterize. – CV variance on ‘n’ data points is worse than with a validation set of size ‘n’. • But we believe it is close. • Does cross-validation remove optimization bias? – No, but the bias might be smaller since you have more “test” points. Handling Data Sparsity • Do we need to store the full bag of words 0/1 variables? – No: only need list of non-zero features for each e-mail. – Math/model doesn’t change, but more efficient storage. $ Hi CPSC 340 Vicodin Offer … 1 1 0 0 1 0 … 0 0 0 0 1 1 … 0 1 1 1 0 0 … 1 1 0 0 0 1 … Non-Zeroes {1,2,5,…} {5,6,…} {2,3,4,…} {1,2,6,…} Generalization Error • An alternative measure of performance is the generalization error: – Average error over the set of xi values that are not seen in the training set. – “How well we expect to do for a completely unseen feature vector”. • Test error vs. generalization error when labels are deterministic: “Best” and the “Good” Machine Learning Models • Question 1: what is the “best” machine learning model? – The model that gets lower generalization error than all other models. • Question 2: which models always do better than random guessing? – Models with lower generalization error than “predict 0” for all problems. • No free lunch theorem: – There is no “best” model achieving the best generalization error for every problem. – If model A generalizes better to new data than model B on one dataset, there is another dataset where model B works better. No Free Lunch Theorem • Let’s show the “no free lunch” theorem in a simple setting: – The xi and yi are binary, and yi being a deterministic function of xi. • With ‘d’ features, each “learning problem” is a map from {0,1}d -> {0,1}. – Assigning a binary label to each of the 2d feature combinations. • Let’s pick one of these ‘y’ vectors (“maps” or “learning problems”) and: – Generate a set training set of ‘n’ IID samples. – Fit model A (convolutional neural network) and model B (naïve Bayes). Feature 1 Feature 2 Feature 3 0 0 0 0 0 1 0 1 0 … … … y (map 1) y (map 2) y (map 3) … 0 1 0 … 0 0 1 … 0 0 0 … … … … … No Free Lunch Theorem • Define the “unseen” examples as the (2d – n) not seen in training. – Assuming no repetitions of xi values, and n < 2d. – Generalization error is the average error on these “unseen” examples. • Suppose that model A got 1% error and model B got 60% error. – We want to show model B beats model A on another “learning problem”. • Among our set of “learning problems” find the one where: – The labels yi agree on all training examples. – The labels yi disagree on all “unseen” examples. • On this other “learning problem”: – Model A gets 99% error and model B gets 40% error. Proof of No Free Lunch Theorem • Let’s show the “no free lunch” theorem in a simple setting: – The xi and yi are binary, and yi being a deterministic function of xi. • With ‘d’ features, each “learning problem” is a map from each of the 2d feature combinations to 0 or 1: {0,1}d -> {0,1} • Let’s pick one of these maps (“learning problems”) and: – Generate a set training set of ‘n’ IID samples. – Fit model A (convolutional neural network) and model B (naïve Bayes). Feature 1 Feature 2 Feature 3 0 0 0 0 0 1 0 1 0 … … … Map 1 Map 2 Map 3 … 0 1 0 … 0 0 1 … 0 0 0 … … … … … Proof of No Free Lunch Theorem • Define the “unseen” examples as the (2d – n) not seen in training. – Assuming no repetitions of xi values, and n < 2d. – Generalization error is the average error on these “unseen” examples. • Suppose that model A got 1% error and model B got 60% error. – We want to show model B beats model A on another “learning problem”. • Among our set of “learning problems” find the one where: – The labels yi agree on all training examples. – The labels yi disagree on all “unseen” examples. • On this other “learning problem”: – Model A gets 99% error and model B gets 40% error. Proof of No Free Lunch Theorem • Further, across all “learning problems” with these ‘n’ examples: – Average generalization error of every model is 50% on unseen examples. • It’s right on each unseen example in exactly half the learning problems. – With ‘ℓ’ classes, the average error is (ℓ-1)/ℓ (random guessing). • This is kind of depressing: – For general problems, no “machine learning” is better than “predict 0”. • But the proof also reveals the problem with the NFL theorem: – Assumes every “learning problem” is equally likely. – World encourages patterns like “similar features implies similar labels”.","libVersion":"0.2.1","langs":""}