{"path":".obsidian/plugins/text-extractor/cache/2aefed7f063bebbe11a53e8815fd0c0b.json","text":"[V K R NN G ELIEGEICRIEL N A computer S is sending data to a computer R using the alternating bit protocol where the mean round-trip time is 100ms, but the actual round-trip time varies quite substantially. The underlying network protocol is quite reliable, but does occasionally lose packets with an average probability of 0.001 (on average, 1 packet in 1000 will be lost). S must choose a timeout value that it will use to detect packet loss. If it chooses a short timeout value, a lost packet will be detected quickly, but occasionally it will time out too soon, when the ACK is delayed but has not been lost. If it chooses a long timeout value, it will detect packet loss more slowly, but the chances of it timing out too soon decrease. So the choice of timeout value must balance these two pressures. We will explore two different timeout values and you will compute the costs of each. If the timeout is 0.5 seconds, the probability of timing out too soon is measured at 0.07. If the timeout is 1 second, the probability of timing out too soon is measured at 0.01. For this portion of the question, imagine that the only time we timeout is when a packet or its ACK gets lost. With a timeout of 0.5 seconds, how long will it (on average) take to send and receive 1000 packets? number (rtol=0.01, atol=1e-08) s @ Now consider timeouts that happen too soon. With a timeout of 0.5 seconds, how many extra data packets will be sent by S? For simplicity, only consider losses that happen the first time that a data packet is sent. That is, we assume that retransmitted data packets are never lost. integer times @ For this portion of the question, imagine that the only time we timeout is when a packet or its ACK gets lost. With a timeout of 1 second, how long will it (on average) take to send and receive 1000 packets? number (rtol=0.01, atol=1e-08) s @ Now consider timeouts that happen too soon. With a timeout of 1 second, how many extra packets will be sent by S? For simplicity, only consider losses that happen the first time that a data packet is sent. That is, we assume that retransmitted data packets are never lost. integer times @ Which timeout value 0.5s or 1s is better in this situation? Explain your answer in terms of the costs you computed. Your answer here","libVersion":"0.2.1","langs":"eng"}