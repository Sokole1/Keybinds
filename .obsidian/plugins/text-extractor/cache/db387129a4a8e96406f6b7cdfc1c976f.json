{"path":".obsidian/plugins/text-extractor/cache/db387129a4a8e96406f6b7cdfc1c976f.json","text":"CPSC 340: Machine Learning and Data Mining Beyond PCA Last Time: Non-Uniqueness of PCA • When k = 1, PCA has a scaling problem. – Standard fix: use normalized rows Wc of ‘W’. • When k > 1, PCA also has a label switching and rotation. – Standard fix: use normalized orthogonal rows Wc of ‘W’. Making PCA Unique • We’ve identified several reasons that optimal W is non-unique: – Unnormalized: I can multiply any wc by any non-zero α. – Rotation: I can rotate any wc almost arbitrarily within the span. – Label switching: I can switch any wc with any other wc’. • PCA implementations add constraints to make solution unique: – Normalization: we enforce that ||wc|| = 1. – Orthogonality: we enforce that wcTwc’ = 0 for all c ≠ c’. – Sequential fitting: We first fit w1 (“first principal component”) giving a line. • Then fit w2 given w1 (“second principal component”) giving a plane. • Then we fit w3 given w1 and w2 (“third principal component”) giving a space. Basis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential Fitting http://setosa.io/ev/principal-component-analysis PCA Computation: SVD • How do we fit with normalization/orthogonality/sequential-fitting? – It can be done with the “singular value decomposition” (SVD). – Take CPSC 302. • 4 lines of Python code: – mu = np.mean(X,axis=0) – X -= mu – U,s,Vh = np.linalg.svd(X) – W = Vh[:k] 9 • Computing Z is cheaper now: “Synthesis” View vs. “Analysis” View • We said that PCA finds hyper-plane minimizing distance to data xi. – This is the “synthesis” view of PCA (connects to k-means and least squares). • “Analysis” view when we have orthogonality constraints: – PCA finds hyper-plane maximizing variance in zi space. – You pick W to “explain as much variance in the data” as possible. Colour Opponency in the Human Eye • Classic model of the eye is with 4 photoreceptors: – Rods (more sensitive to brightness). – L-Cones (most sensitive to red). – M-Cones (most sensitive to green). – S-Cones (most sensitive to blue). • Two problems with this system: – Not orthogonal. • High correlation in particular between red/green. – We have 4 receptors for 3 colours. http://oneminuteastronomer.com/astro-course-day-5/ https://en.wikipedia.org/wiki/Color_visio Colour Opponency in the Human Eye • Bipolar and ganglion cells seem to code using “opponent colors”: – 3-variable orthogonal basis: • This is similar to PCA (d = 4, k = 3). http://oneminuteastronomer.com/astro-course-day-5/ https://en.wikipedia.org/wiki/Color_visio http://5sensesnews.blogspot.ca/ Next Topic: Alternatives to SVD for PCA PCA Computation: other methods • With linear regression, we had the normal equations – But we also could do it with gradient descent, SGD, etc. • With PCA we have the SVD – But we can also do it with gradient descent, SGD, etc. – These other methods typically don’t enforce the uniqueness “constraints”. • Sensitive to initialization, don’t enforce normalization, orthogonality, ordered PCs. – But you can do this in post-processing if you want. – Why would we want this? We can use our tricks from Part 3 of the course: • We can do things like “robust” PCA, “regularized” PCA, “sparse” PCA, “binary” PCA. • We can fit huge datasets where SVD is too expensive. 15 PCA Computation: Alternating Minimization • With centered data, the PCA objective is: • In k-means we tried to optimize this with alternating minimization: – Fix “cluster assignments” Z and find the optimal “means” W. – Fix “means” W and find the optimal “cluster assignments” Z. • Converges to a local optimum. – But may not find a global optimum (sensitive to initialization). PCA Computation: Alternating Minimization • With centered data, the PCA objective is: • In PCA we can also use alternating minimization: – Fix “part weights” Z and find the optimal “parts” W. – Fix “parts” W and find the optimal “part weights” Z. • Repeat until you converge to a local optimum. PCA Computation: Alternating Minimization • With centered data, the PCA objective is: • Alternating minimization steps: – If we fix Z, this is a quadratic function of W (least squares column-wise): – If we fix W, this is a quadratic function of Z (transpose due to dimensions): PCA Computation: Alternating Minimization • With centered data, the PCA objective is: • This objective is not jointly convex in W and Z. – You will find different W and Z depending on the initialization. • For example, if you initialize with all wc = 0, then they will stay at zero. – But it’s possible to show that all “stable” local optima are global optima. • You will converge to a global optimum in practice if you initialize randomly. – Randomization means you don’t start on one of the unstable non-global critical points. • E.g., sample each initial zij from a normal distribution. http://www.offconvex.org/2018/11/07/optimization-beyond-landscape/ PCA Computation: Stochastic Gradient Descent • For big X matrices, you can also use stochastic gradient descent: • Other variables stay the same, cost per iteration is only O(k). Next Topic: Variations on PCA Beyond Squared Error • Our objective for latent-factor models (LFM): • As before, there are alternatives to squared error. • If X has of +1 and -1 values, we could use the logistic loss: – And predict xij with sign(oij), which would be a binary PCA model. Beyond Squared Error • Our objective for latent-factor models (LFM): • As before, there are alternatives to squared error. • If X has many outliers, we could use the absolute loss: – Which will robust to outliers and is called robust PCA model. Regularized Matrix Factorization • Recently people have also considered L2-regularized PCA: • Replaces normalization/orthogonality/sequential-fitting. – Often gives lower reconstruction error on test data. – But requires regularization parameters λ1 and λ2. • Need to regularize W and Z because of scaling problem. – If you only regularize ‘W’ it does not do anything. • I could take unregularized solution, replace W by αW for a tiny α to shrink ||W||F as much as I want, then multiply Z by (1/α) to get same solution. – Similarly, if you only regularize ‘Z’ it does not do anything. Non-Orthogonal and Sparse Eigenfaces http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Sparse Matrix Factorization and NMF • Alternately, many works consider L1-regularization: – Called sparse coding (L1 on ‘Z’) or sparse dictionary learning (L1 on ‘W’). • Encourage values of ‘Z’ or ‘W’ to be exactly zero as we increase 𝜆1 or 𝜆2. • A related older method is non-negative matrix factorization (NMF): – Optimizes the PCA objective but forces ‘Z’ and ‘W’ to be non-negative. • In some applications negative quantities do not make sense. – The non-negative constraint also leads to sparsity (many values set to 0). • But unlike L1-regularization, you cannot control the degree of sparsity. – Details on NMF in bonus (including “cancer signatures” and “NBA shot charts”). Application: Image Restoration http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Latent-Factor Models for Image Patches • Consider building latent-factors for general image patches: Latent-Factor Models for Image Patches • Consider building latent-factors for general image patches: Typical pre-processing: 1. Usual variable centering 2. “Whiten” patches. (remove correlations - bonus) Latent-Factor Models for Image Patches http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf http://stackoverflow.com/questions/16059462/comparing-textures-with-opencv-and-gabor-filters Orthogonal bases don’t seem right: • Few PCs do almost everything. • Most PCs do almost nothing. We believe “simple cells” in visual cortex use: ‘Gabor’ filters Latent-Factor Models for Image Patches • Results from a non-orthogonal latent factor with L1-regularization: http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Latent-Factor Models for Image Patches • Results from a non-orthogonal latent factor with L1-regularization: http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Recent Work: Structured Sparsity • Basis learned with a “structured sparsity” regularizer: – “Total variation” regularization: penalizes adjacent PCs to be similar. http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Next Topic: Recommender Systems Recommender System Motivation: Netflix Prize • Netflix Prize: – 100M ratings from 0.5M users on 18k movies. – Grand prize was $1M for first team to reduce squared error by 10%. – Started on October 2nd, 2006. – Netflix’s system was first beat October 8th. – 1% error reduction achieved on October 15th. – Steady improvement after that. • ML methods soon dominated. Motivation: Other Recommender Systems • Recommender systems are now everywhere: – Music, news, books, jokes, experts, restaurants, friends, dates, etc. • Main types of approaches: 1. Content-based filtering. • Supervised learning: – Extract features xi of users and items, building model to predict rating yi given xi. – Apply model to prediction for new users/items. • Example: G-mail’s “important messages” (personalization with “local” features). 2. Collaborative filtering. • “Unsupervised” learning (have label matrix ‘Y’ but no features): – We only have labels yij (rating of user ‘i’ for movie ‘j’). • Example: Amazon recommendation algorithm. Lessons Learned from Netflix Prize • Netflix prize awarded in 2009: – Ensemble method that averaged 107 models. – Increasing diversity of models more important than improving models. • Winning entry (and most entries) used collaborative filtering: – Methods that only looks at ratings, not features of movies/users. • A simple collaborative filtering method that does really well (7%): – “Regularized matrix factorization”. Now adopted by many companies. http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/?_r=0 Collaborative Filtering Problem • Collaborative filtering is ‘filling in’ the user-item matrix: • We have some ratings available with values {1,2,3,4,5}. • We want to predict ratings “?” by looking at available ratings. Collaborative Filtering Problem • Collaborative filtering is ‘filling in’ the user-item matrix: • What rating would “Ryan Reynolds” give to “Green Lantern”? – Why is this not completely crazy? We may have similar users and movies. Summary • Recommender systems try to recommend products. • Orthogonal basis and sequential fitting of PCs (via SVD): – Leads to non-redundant PCs with unique directions. • Biological motivation for orthogonal and/or sparse latent factors. • Alternating minimization and stochastic gradient descent: – Iterative algorithms for minimizing PCA objective. • Many of our regression tricks can be used with LFMs: – Robust PCA uses absolute error to be roboust to outliers. – Regularized PCA can improve generalization or give sparse factors. • Next time: should we make a scatterplot with gradient descent? Proof: “Synthesis” View = “Analysis” View (WWT = I) • The variance of the zij (maximized in “analysis” view): • The distance to the hyper-plane (minimized in “synthesis” view): Background Subtraction with Robust PCA • Robust PCA methods use the absolute error: • Will be robust to outliers in the matrix ‘X’. • Encourages “residuals” rij to be exactly zero. – Non-zero rij are where the “outliers” are. http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf Digression: “Whitening” • With image data, features will be very redundant. – Neighbouring pixels tend to have similar values. • A standard transformation in these settings is “whitening”: – Rotate the data so features are uncorrelated. – Re-scale the rotated features so they have a variance of 1. • Using SVD approach to PCA, we can do this with: – Get ‘W’ from SVD (usually with k=d). – Z = XWT (rotate to give uncorrelated features). – Divide columns of ‘Z’ by corresponding singular values (unit variance). • Details/discussion here. Kernel PCA • From the “analysis” view (with orthogonal PCs) PCA maximizes: • It can be shown that the solution has the form (see here): • Re-parameterizing in terms of ‘U’ gives a kernelized PCA: • It’s hard to initially center data in ‘Z’ space, but you can form the centered kernel matrix (see here). VQ vs. PCA vs. NMF • How should we represent faces? – Vector quantization (k-means). • Replace face by the average face in a cluster. • ‘Grandmother cell’: one neuron = one face. • Can’t distinguish between people in the same cluster (only ‘k’ possible faces). • Almost certainly not true: too few neurons. VQ vs. PCA vs. NMF • How should we represent faces? – Vector quantization (k-means). – PCA (orthogonal basis). • Global average plus linear combination of “eigenfaces”. • “Distributed representation”. – Coded by pattern of group of neurons: can represent infinite number of faces by changing zi. • But “eigenfaces” are not intuitive ingredients for faces. – PCA tends to use positive/negative cancelling bases. VQ vs. PCA vs. NMF • How should we represent faces? – Vector quantization (k-means). – PCA (orthogonal basis). – NMF (non-negative matrix factorization): • Instead of orthogonality/ordering in W, require W and Z to be non-negativity. • Example of “sparse coding”: – The zi are sparse so each face is coded by a small number of neurons. – The wc are sparse so neurons tend to be “parts” of the object. Representing Faces • Why sparse coding? – “Parts” are intuitive, and brains seem to use sparse representation. – Energy efficiency if using sparse code. – Increase number of concepts you can memorize? • Some evidence in fruit fly olfactory system. http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf Warm-up to NMF: Non-Negative Least Squares • Consider our usual least squares problem: • But assume yi and elements of xi are non-negative: – Could be sizes (‘height’, ‘milk’, ‘km’) or counts (‘vicodin’, ‘likes’, ‘retweets’). • Assume we want elements of ‘w’ to be non-negative, too: – No physical interpretation to negative weights. – If xij is amount of product you produce, what does wj < 0 mean? • Non-negativity leads to sparsity... Sparsity and Non-Negative Least Squares • Consider 1D non-negative least squares objective: • Plotting the (constrained) objective function: • In this case, non-negative solution is least squares solution. Sparsity and Non-Negative Least Squares • Consider 1D non-negative least squares objective: • Plotting the (constrained) objective function: • In this case, non-negative solution is w = 0. Sparsity and Non-Negativity • Similar to L1-regularization, non-negativity leads to sparsity. – Also regularizes: wj are smaller since can’t “cancel” negative values. – Sparsity leads to cheaper predictions and often to more interpretability. • Non-negative weights are often also more interpretable. • How can we minimize f(w) with non-negative constraints? – Naive approach: solve least squares problem, set negative wj to 0. – This is correct when d = 1. – Can be worse than setting w = 0 when d ≥ 2. Sparsity and Non-Negativity • Similar to L1-regularization, non-negativity leads to sparsity. – Also regularizes: wj are smaller since can’t “cancel” out negative values. • How can we minimize f(w) with non-negative constraints? – A correct approach is projected gradient algorithm: • Run a gradient descent iteration: • After each step, set negative values to 0. • Repeat. Sparsity and Non-Negativity • Projected gradient algorithm: – Similar properties to gradient descent: • Guaranteed decrease of ‘f’ if αt is small enough. • Reaches local minimum under weak assumptions (global minimum for convex ‘f’). – Least squares objective is still convex when restricted to non-negative variables. • Solution is a “fixed point”: w* = max{0, w* - 𝛼t 𝛻f(w*)}. – Use this to decide when to stop. – A generalization is “proximal-gradient”: • Instead of constraints, allows non-smooth terms (“findMinL1”). Projected-Gradient for NMF • Back to the non-negative matrix factorization (NMF) objective: – Different ways to use projected gradient: • Alternate between projected gradient steps on ‘W’ and on ‘Z’. • Or run projected gradient on both at once. • Or sample a random ‘i’ and ‘j’ and do stochastic projected gradient. – Non-convex and (unlike PCA) is sensitive to initialization. • Hard to find the global optimum. • Typically use random initialization. • Also, we usually don’t center the data with NMF. Application: Sports Analytics • NBA shot charts: • NMF (using “KL divergence” loss with k=10 and smoothed data). – Negative values would not make sense here. http://jmlr.org/proceedings/papers/v32/miller14.pdf Application: Cancer “Signatures” • What are common sets of mutations in different cancers? – May lead to new treatment options. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3588146/ Sparse Eigenfaces http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Recent Work: Structured Sparsity • “Structured sparsity” considers dependencies in sparsity patterns. – Can enforce that “parts” are convex regions. http://jmlr.org/proceedings/papers/v9/jenatton10a/jenatton10a.pdf Beyond Matrix Factorization: Topic Models • For modeling categorical data, “topic models” are replacing NMF. – A “fully-Bayesian” model where sparsity arises naturally. – Most popular example is called “latent Dirichlet allocation” (CPSC 440). http://menome.com/wp/wp-content/uploads/2014/12/Blei2011.pdf Sparse Matrix Factorization • Instead of non-negativity, we could use L1-regularization: – Called sparse coding (L1 on ‘Z’) or sparse dictionary learning (L1 on ‘W’). • Disadvantage of using L1-regularization over non-negativity: – Sparsity controlled by λ1 and λ2 so you need to set these. • Advantage of using L1-regularization: – Sparsity controlled by λ1 and λ2, so you can control amount of sparsity. – Negative coefficients often do make sense. Sparse Matrix Factorization • Instead of non-negativity, we could use L1-regularization: – Called sparse coding (L1 on ‘Z’) or sparse dictionary learning (L1 on ‘W’). • Many variations exist: – Mixing L2-regularization and L1-regularization. • Or normalizing ‘W’ (in L2-norm or L1-norm) and regularizing ‘Z’. – K-SVD constrains each zi to have at most ‘k’ non-zeroes: • K-means is special case where k = 1. • PCA is special case where k = d. Canonical Correlation Analysis (CCA) • Suppose we have two matrices, ‘X’ and ‘Y’. • Want to find matrices WX and WY that maximize correlation. – “What are the latent factors in common between these datasets?” • Define the correlation matrices: • Canonical correlation analysis (CCA) maximizes – Subject to WX and WY having orthogonal rows. • Computationally, equivalent to PCA with a different matrix. – Using the “analysis” view that PCA maximizes Tr(WTWXTX). Probabilistic PCA • With zero-mean (“centered”) data, in PCA we assume that • In probabilistic PCA we assume that • Integrating over ‘Z’ the marginal likelihood given ‘W’ is Gaussian, • Regular PCA is obtained as the limit of σ2 going to 0. Generalizations of Probabilistic PCA • Probabilistic PCA model: • Why do we need a probabilistic interpretation? • Shows that PCA fits a Gaussian with restricted covariance. – Hope is that WTW + σ2I is a good approximation of XTX. • Gives precise connection between PCA and factor analysis. Factor Analysis • Factor analysis is a method for discovering latent factors. • Historical applications are measures of intelligence and personality. • A standard tool and widely-used across science and engineering. https://new.edu/resources/big-5-personality-traits PCA vs. Factor Analysis • PCA and FA both write the matrix ‘X’ as • PCA and FA are both based on a Gaussian assumption. • Are PCA and FA the same? – Both are more than 100 years old. – People are still arguing about whether they are the same: • Doesn’t help that some packages run PCA when you call their FA method. PCA vs. Factor Analysis • In probabilistic PCA we assume: • In FA we assume for a diagonal matrix D that: • The posterior in this case is: • The difference is you have a noise variance for each dimension. – FA has extra degrees of freedom. PCA vs. Factor Analysis • In practice there often isn’t a huge difference: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi Factor Analysis Discussion • Differences with PCA: – Unlike PCA, FA is not affected by scaling individual features. – But unlike PCA, it’s affected by rotation of the data. – No nice “SVD” approach for FA, you can get different local optima. • Similar to PCA, FA is invariant to rotation of ‘W’. – So as with PCA you can’t interpret multiple factors as being unique. Motivation for ICA • Factor analysis has found an enormous number of applications. – People really want to find the “hidden factors” that make up their data. • But PCA and FA can’t identify the factors. Motivation for ICA • Factor analysis has found an enormous number of applications. – People really want to find the “hidden factors” that make up their data. • But PCA and FA can’t identify the factors. – We can rotate W and obtain the same model. • Independent component analysis (ICA) is a more recent approach. – Around 30 years old instead of > 100. – Under certain assumptions it can identify factors. • The canonical application of ICA is blind source separation. Blind Source Separation • Input to blind source separation: – Multiple microphones recording multiple sources. • Each microphone gets different mixture of the sources. – Goal is reconstruct sources (factors) from the measurements. http://music.eecs.northwestern.edu/research.php Independent Component Analysis Applications • ICA is replacing PCA and FA in many applications: • Recent work shows that ICA can often resolve direction of causality. https://en.wikipedia.org/wiki/Independent_component_analysis#Applications Limitations of Matrix Factorization • ICA is a matrix factorization method like PCA/FA, • Let’s assume that X = ZW for a “true” W with k = d. – Different from PCA where we assume k ≤ d. • There are only 3 issues stopping us from finding “true” W. 3 Sources of Matrix Factorization Non-Uniquness • Label switching: get same model if we permute rows of W. – We can exchange row 1 and 2 of W (and same columns of Z). – Not a problem because we don’t care about order of factors. • Scaling: get same model if you scale a row. – If we mutiply row 1 of W by α, could multiply column 1 of Z by 1/α. – Can’t identify sign/scale, but might hope to identify direction. • Rotation: get same model if we rotate W. – Rotations correspond to orthogonal matrices Q, such matrices have QTQ = I. – If we rotate W with Q, then we have (QW)TQW = WTQTQW = WTW. • If we could address rotation, we could identify the “true” directions. A Unique Gaussian Property • Consider an independent prior on each latent features zc. – E.g., in PPCA and FA we use N(0,1) for each zc. • If prior p(z) is independent and rotation-invariant (p(Qz) = p(z)), then it must be Gaussian (only Gaussians have this property). • The (non-intuitive) magic behind ICA: – If the priors are all non-Gaussian, it isn’t rotationally symmetric. – In this case, we can identify factors W (up to permutations and scalings). PCA vs. ICA http://www.inf.ed.ac.uk/teaching/courses/pmr/lectures/ica.pdf Independent Component Analysis • In ICA we approximate X with ZW, assuming p(zic) are non-Gaussian. • Usually we “center” and “whiten” the data before applying ICA. • There are several penalties that encourage non-Gaussianity: – Penalize low kurtosis, since kurtosis is minimized by Gaussians. – Penalize high entropy, since entropy is maximized by Gaussians. • The fastICA is a popular method maximizing kurtosis. ICA on Retail Purchase Data • Cash flow from 5 stores over 3 years: http://www.stat.ucla.edu/~yuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf ICA on Retail Purchase Data • Factors found using ICA: http://www.stat.ucla.edu/~yuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf Motivation for Topic Models • Want a model of the “factors” making up documents. – Instead of latent-factor models, they’re called topic models. – The canonical topic model is latent Dirichlet allocation (LDA). – “Topics” could be useful for things like searching for relevant documents. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ Term Frequency – Inverse Document Frequency • In information retrieval, classic word importance measure is TF-IDF. • First part is the term frequency tf(t,d) of term ‘t’ for document ‘d’. – Number of times “word” ‘t’ occurs in document ‘d’, divided by total words. – E.g., 7% of words in document ‘d’ are “the” and 2% of the words are “Lebron”. • Second part is document frequency df(t,D). – Compute number of documents that have ‘t’ at least once. – E.g., 100% of documents contain “the” and 0.01% have “LeBron”. • TF-IDF is tf(t,d)*log(1/df(t,D)). Term Frequency – Inverse Document Frequency • The TF-IDF statistic is tf(t,d)*log(1/df(t,D)). – It’s high if word ‘t’ happens often in document ‘d’, but isn’t common. – E.g., seeing “LeBron” a lot it tells you something about “topic” of article. – E.g., seeing “the” a lot tells you nothing. • There are *many* variations on this statistic. – E.g., avoiding dividing by zero and all types of “frequencies”. • Summarizing ‘n’ documents into a matrix X: – Each row corresponds to a document. – Each column gives the TF-IDF value of a particular word in the document. Latent Semantic Indexing • TF-IDF features are very redundant. – Consider TF-IDFs of “LeBron”, “Durant”, “Harden”, and “Kobe”. – High values of these typically just indicate topic of “basketball”. • We can probably compress this information quite a bit. • Latent Semantic Indexing/Analysis: – Run latent-factor model (like PCA or NMF) on TF-IDF matrix X. – Treat the principal components as the “topics”. – Latent Dirichlet allocation is a variant that avoids weird df(t,D) heuristic.","libVersion":"0.2.1","langs":""}