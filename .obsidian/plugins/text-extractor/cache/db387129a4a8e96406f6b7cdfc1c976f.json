{"path":".obsidian/plugins/text-extractor/cache/db387129a4a8e96406f6b7cdfc1c976f.json","text":"CPSC 340: Machine Learning and Data Mining Beyond PCA Last Time: Non-Uniqueness of PCA â€¢ When k = 1, PCA has a scaling problem. â€“ Standard fix: use normalized rows Wc of â€˜Wâ€™. â€¢ When k > 1, PCA also has a label switching and rotation. â€“ Standard fix: use normalized orthogonal rows Wc of â€˜Wâ€™. Making PCA Unique â€¢ Weâ€™ve identified several reasons that optimal W is non-unique: â€“ Unnormalized: I can multiply any wc by any non-zero Î±. â€“ Rotation: I can rotate any wc almost arbitrarily within the span. â€“ Label switching: I can switch any wc with any other wcâ€™. â€¢ PCA implementations add constraints to make solution unique: â€“ Normalization: we enforce that ||wc|| = 1. â€“ Orthogonality: we enforce that wcTwcâ€™ = 0 for all c â‰  câ€™. â€“ Sequential fitting: We first fit w1 (â€œfirst principal componentâ€) giving a line. â€¢ Then fit w2 given w1 (â€œsecond principal componentâ€) giving a plane. â€¢ Then we fit w3 given w1 and w2 (â€œthird principal componentâ€) giving a space. Basis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential FittingBasis, Orthogonality, Sequential Fitting http://setosa.io/ev/principal-component-analysis PCA Computation: SVD â€¢ How do we fit with normalization/orthogonality/sequential-fitting? â€“ It can be done with the â€œsingular value decompositionâ€ (SVD). â€“ Take CPSC 302. â€¢ 4 lines of Python code: â€“ mu = np.mean(X,axis=0) â€“ X -= mu â€“ U,s,Vh = np.linalg.svd(X) â€“ W = Vh[:k] 9 â€¢ Computing Z is cheaper now: â€œSynthesisâ€ View vs. â€œAnalysisâ€ View â€¢ We said that PCA finds hyper-plane minimizing distance to data xi. â€“ This is the â€œsynthesisâ€ view of PCA (connects to k-means and least squares). â€¢ â€œAnalysisâ€ view when we have orthogonality constraints: â€“ PCA finds hyper-plane maximizing variance in zi space. â€“ You pick W to â€œexplain as much variance in the dataâ€ as possible. Colour Opponency in the Human Eye â€¢ Classic model of the eye is with 4 photoreceptors: â€“ Rods (more sensitive to brightness). â€“ L-Cones (most sensitive to red). â€“ M-Cones (most sensitive to green). â€“ S-Cones (most sensitive to blue). â€¢ Two problems with this system: â€“ Not orthogonal. â€¢ High correlation in particular between red/green. â€“ We have 4 receptors for 3 colours. http://oneminuteastronomer.com/astro-course-day-5/ https://en.wikipedia.org/wiki/Color_visio Colour Opponency in the Human Eye â€¢ Bipolar and ganglion cells seem to code using â€œopponent colorsâ€: â€“ 3-variable orthogonal basis: â€¢ This is similar to PCA (d = 4, k = 3). http://oneminuteastronomer.com/astro-course-day-5/ https://en.wikipedia.org/wiki/Color_visio http://5sensesnews.blogspot.ca/ Next Topic: Alternatives to SVD for PCA PCA Computation: other methods â€¢ With linear regression, we had the normal equations â€“ But we also could do it with gradient descent, SGD, etc. â€¢ With PCA we have the SVD â€“ But we can also do it with gradient descent, SGD, etc. â€“ These other methods typically donâ€™t enforce the uniqueness â€œconstraintsâ€. â€¢ Sensitive to initialization, donâ€™t enforce normalization, orthogonality, ordered PCs. â€“ But you can do this in post-processing if you want. â€“ Why would we want this? We can use our tricks from Part 3 of the course: â€¢ We can do things like â€œrobustâ€ PCA, â€œregularizedâ€ PCA, â€œsparseâ€ PCA, â€œbinaryâ€ PCA. â€¢ We can fit huge datasets where SVD is too expensive. 15 PCA Computation: Alternating Minimization â€¢ With centered data, the PCA objective is: â€¢ In k-means we tried to optimize this with alternating minimization: â€“ Fix â€œcluster assignmentsâ€ Z and find the optimal â€œmeansâ€ W. â€“ Fix â€œmeansâ€ W and find the optimal â€œcluster assignmentsâ€ Z. â€¢ Converges to a local optimum. â€“ But may not find a global optimum (sensitive to initialization). PCA Computation: Alternating Minimization â€¢ With centered data, the PCA objective is: â€¢ In PCA we can also use alternating minimization: â€“ Fix â€œpart weightsâ€ Z and find the optimal â€œpartsâ€ W. â€“ Fix â€œpartsâ€ W and find the optimal â€œpart weightsâ€ Z. â€¢ Repeat until you converge to a local optimum. PCA Computation: Alternating Minimization â€¢ With centered data, the PCA objective is: â€¢ Alternating minimization steps: â€“ If we fix Z, this is a quadratic function of W (least squares column-wise): â€“ If we fix W, this is a quadratic function of Z (transpose due to dimensions): PCA Computation: Alternating Minimization â€¢ With centered data, the PCA objective is: â€¢ This objective is not jointly convex in W and Z. â€“ You will find different W and Z depending on the initialization. â€¢ For example, if you initialize with all wc = 0, then they will stay at zero. â€“ But itâ€™s possible to show that all â€œstableâ€ local optima are global optima. â€¢ You will converge to a global optimum in practice if you initialize randomly. â€“ Randomization means you donâ€™t start on one of the unstable non-global critical points. â€¢ E.g., sample each initial zij from a normal distribution. http://www.offconvex.org/2018/11/07/optimization-beyond-landscape/ PCA Computation: Stochastic Gradient Descent â€¢ For big X matrices, you can also use stochastic gradient descent: â€¢ Other variables stay the same, cost per iteration is only O(k). Next Topic: Variations on PCA Beyond Squared Error â€¢ Our objective for latent-factor models (LFM): â€¢ As before, there are alternatives to squared error. â€¢ If X has of +1 and -1 values, we could use the logistic loss: â€“ And predict xij with sign(oij), which would be a binary PCA model. Beyond Squared Error â€¢ Our objective for latent-factor models (LFM): â€¢ As before, there are alternatives to squared error. â€¢ If X has many outliers, we could use the absolute loss: â€“ Which will robust to outliers and is called robust PCA model. Regularized Matrix Factorization â€¢ Recently people have also considered L2-regularized PCA: â€¢ Replaces normalization/orthogonality/sequential-fitting. â€“ Often gives lower reconstruction error on test data. â€“ But requires regularization parameters Î»1 and Î»2. â€¢ Need to regularize W and Z because of scaling problem. â€“ If you only regularize â€˜Wâ€™ it does not do anything. â€¢ I could take unregularized solution, replace W by Î±W for a tiny Î± to shrink ||W||F as much as I want, then multiply Z by (1/Î±) to get same solution. â€“ Similarly, if you only regularize â€˜Zâ€™ it does not do anything. Non-Orthogonal and Sparse Eigenfaces http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Sparse Matrix Factorization and NMF â€¢ Alternately, many works consider L1-regularization: â€“ Called sparse coding (L1 on â€˜Zâ€™) or sparse dictionary learning (L1 on â€˜Wâ€™). â€¢ Encourage values of â€˜Zâ€™ or â€˜Wâ€™ to be exactly zero as we increase ğœ†1 or ğœ†2. â€¢ A related older method is non-negative matrix factorization (NMF): â€“ Optimizes the PCA objective but forces â€˜Zâ€™ and â€˜Wâ€™ to be non-negative. â€¢ In some applications negative quantities do not make sense. â€“ The non-negative constraint also leads to sparsity (many values set to 0). â€¢ But unlike L1-regularization, you cannot control the degree of sparsity. â€“ Details on NMF in bonus (including â€œcancer signaturesâ€ and â€œNBA shot chartsâ€). Application: Image Restoration http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Latent-Factor Models for Image Patches â€¢ Consider building latent-factors for general image patches: Latent-Factor Models for Image Patches â€¢ Consider building latent-factors for general image patches: Typical pre-processing: 1. Usual variable centering 2. â€œWhitenâ€ patches. (remove correlations - bonus) Latent-Factor Models for Image Patches http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf http://stackoverflow.com/questions/16059462/comparing-textures-with-opencv-and-gabor-filters Orthogonal bases donâ€™t seem right: â€¢ Few PCs do almost everything. â€¢ Most PCs do almost nothing. We believe â€œsimple cellsâ€ in visual cortex use: â€˜Gaborâ€™ filters Latent-Factor Models for Image Patches â€¢ Results from a non-orthogonal latent factor with L1-regularization: http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Latent-Factor Models for Image Patches â€¢ Results from a non-orthogonal latent factor with L1-regularization: http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Recent Work: Structured Sparsity â€¢ Basis learned with a â€œstructured sparsityâ€ regularizer: â€“ â€œTotal variationâ€ regularization: penalizes adjacent PCs to be similar. http://lear.inrialpes.fr/people/mairal/resources/pdf/review_sparse_arxiv.pdf Next Topic: Recommender Systems Recommender System Motivation: Netflix Prize â€¢ Netflix Prize: â€“ 100M ratings from 0.5M users on 18k movies. â€“ Grand prize was $1M for first team to reduce squared error by 10%. â€“ Started on October 2nd, 2006. â€“ Netflixâ€™s system was first beat October 8th. â€“ 1% error reduction achieved on October 15th. â€“ Steady improvement after that. â€¢ ML methods soon dominated. Motivation: Other Recommender Systems â€¢ Recommender systems are now everywhere: â€“ Music, news, books, jokes, experts, restaurants, friends, dates, etc. â€¢ Main types of approaches: 1. Content-based filtering. â€¢ Supervised learning: â€“ Extract features xi of users and items, building model to predict rating yi given xi. â€“ Apply model to prediction for new users/items. â€¢ Example: G-mailâ€™s â€œimportant messagesâ€ (personalization with â€œlocalâ€ features). 2. Collaborative filtering. â€¢ â€œUnsupervisedâ€ learning (have label matrix â€˜Yâ€™ but no features): â€“ We only have labels yij (rating of user â€˜iâ€™ for movie â€˜jâ€™). â€¢ Example: Amazon recommendation algorithm. Lessons Learned from Netflix Prize â€¢ Netflix prize awarded in 2009: â€“ Ensemble method that averaged 107 models. â€“ Increasing diversity of models more important than improving models. â€¢ Winning entry (and most entries) used collaborative filtering: â€“ Methods that only looks at ratings, not features of movies/users. â€¢ A simple collaborative filtering method that does really well (7%): â€“ â€œRegularized matrix factorizationâ€. Now adopted by many companies. http://bits.blogs.nytimes.com/2009/09/21/netflix-awards-1-million-prize-and-starts-a-new-contest/?_r=0 Collaborative Filtering Problem â€¢ Collaborative filtering is â€˜filling inâ€™ the user-item matrix: â€¢ We have some ratings available with values {1,2,3,4,5}. â€¢ We want to predict ratings â€œ?â€ by looking at available ratings. Collaborative Filtering Problem â€¢ Collaborative filtering is â€˜filling inâ€™ the user-item matrix: â€¢ What rating would â€œRyan Reynoldsâ€ give to â€œGreen Lanternâ€? â€“ Why is this not completely crazy? We may have similar users and movies. Summary â€¢ Recommender systems try to recommend products. â€¢ Orthogonal basis and sequential fitting of PCs (via SVD): â€“ Leads to non-redundant PCs with unique directions. â€¢ Biological motivation for orthogonal and/or sparse latent factors. â€¢ Alternating minimization and stochastic gradient descent: â€“ Iterative algorithms for minimizing PCA objective. â€¢ Many of our regression tricks can be used with LFMs: â€“ Robust PCA uses absolute error to be roboust to outliers. â€“ Regularized PCA can improve generalization or give sparse factors. â€¢ Next time: should we make a scatterplot with gradient descent? Proof: â€œSynthesisâ€ View = â€œAnalysisâ€ View (WWT = I) â€¢ The variance of the zij (maximized in â€œanalysisâ€ view): â€¢ The distance to the hyper-plane (minimized in â€œsynthesisâ€ view): Background Subtraction with Robust PCA â€¢ Robust PCA methods use the absolute error: â€¢ Will be robust to outliers in the matrix â€˜Xâ€™. â€¢ Encourages â€œresidualsâ€ rij to be exactly zero. â€“ Non-zero rij are where the â€œoutliersâ€ are. http://statweb.stanford.edu/~candes/papers/RobustPCA.pdf Digression: â€œWhiteningâ€ â€¢ With image data, features will be very redundant. â€“ Neighbouring pixels tend to have similar values. â€¢ A standard transformation in these settings is â€œwhiteningâ€: â€“ Rotate the data so features are uncorrelated. â€“ Re-scale the rotated features so they have a variance of 1. â€¢ Using SVD approach to PCA, we can do this with: â€“ Get â€˜Wâ€™ from SVD (usually with k=d). â€“ Z = XWT (rotate to give uncorrelated features). â€“ Divide columns of â€˜Zâ€™ by corresponding singular values (unit variance). â€¢ Details/discussion here. Kernel PCA â€¢ From the â€œanalysisâ€ view (with orthogonal PCs) PCA maximizes: â€¢ It can be shown that the solution has the form (see here): â€¢ Re-parameterizing in terms of â€˜Uâ€™ gives a kernelized PCA: â€¢ Itâ€™s hard to initially center data in â€˜Zâ€™ space, but you can form the centered kernel matrix (see here). VQ vs. PCA vs. NMF â€¢ How should we represent faces? â€“ Vector quantization (k-means). â€¢ Replace face by the average face in a cluster. â€¢ â€˜Grandmother cellâ€™: one neuron = one face. â€¢ Canâ€™t distinguish between people in the same cluster (only â€˜kâ€™ possible faces). â€¢ Almost certainly not true: too few neurons. VQ vs. PCA vs. NMF â€¢ How should we represent faces? â€“ Vector quantization (k-means). â€“ PCA (orthogonal basis). â€¢ Global average plus linear combination of â€œeigenfacesâ€. â€¢ â€œDistributed representationâ€. â€“ Coded by pattern of group of neurons: can represent infinite number of faces by changing zi. â€¢ But â€œeigenfacesâ€ are not intuitive ingredients for faces. â€“ PCA tends to use positive/negative cancelling bases. VQ vs. PCA vs. NMF â€¢ How should we represent faces? â€“ Vector quantization (k-means). â€“ PCA (orthogonal basis). â€“ NMF (non-negative matrix factorization): â€¢ Instead of orthogonality/ordering in W, require W and Z to be non-negativity. â€¢ Example of â€œsparse codingâ€: â€“ The zi are sparse so each face is coded by a small number of neurons. â€“ The wc are sparse so neurons tend to be â€œpartsâ€ of the object. Representing Faces â€¢ Why sparse coding? â€“ â€œPartsâ€ are intuitive, and brains seem to use sparse representation. â€“ Energy efficiency if using sparse code. â€“ Increase number of concepts you can memorize? â€¢ Some evidence in fruit fly olfactory system. http://www.columbia.edu/~jwp2128/Teaching/W4721/papers/nmf_nature.pdf Warm-up to NMF: Non-Negative Least Squares â€¢ Consider our usual least squares problem: â€¢ But assume yi and elements of xi are non-negative: â€“ Could be sizes (â€˜heightâ€™, â€˜milkâ€™, â€˜kmâ€™) or counts (â€˜vicodinâ€™, â€˜likesâ€™, â€˜retweetsâ€™). â€¢ Assume we want elements of â€˜wâ€™ to be non-negative, too: â€“ No physical interpretation to negative weights. â€“ If xij is amount of product you produce, what does wj < 0 mean? â€¢ Non-negativity leads to sparsity... Sparsity and Non-Negative Least Squares â€¢ Consider 1D non-negative least squares objective: â€¢ Plotting the (constrained) objective function: â€¢ In this case, non-negative solution is least squares solution. Sparsity and Non-Negative Least Squares â€¢ Consider 1D non-negative least squares objective: â€¢ Plotting the (constrained) objective function: â€¢ In this case, non-negative solution is w = 0. Sparsity and Non-Negativity â€¢ Similar to L1-regularization, non-negativity leads to sparsity. â€“ Also regularizes: wj are smaller since canâ€™t â€œcancelâ€ negative values. â€“ Sparsity leads to cheaper predictions and often to more interpretability. â€¢ Non-negative weights are often also more interpretable. â€¢ How can we minimize f(w) with non-negative constraints? â€“ Naive approach: solve least squares problem, set negative wj to 0. â€“ This is correct when d = 1. â€“ Can be worse than setting w = 0 when d â‰¥ 2. Sparsity and Non-Negativity â€¢ Similar to L1-regularization, non-negativity leads to sparsity. â€“ Also regularizes: wj are smaller since canâ€™t â€œcancelâ€ out negative values. â€¢ How can we minimize f(w) with non-negative constraints? â€“ A correct approach is projected gradient algorithm: â€¢ Run a gradient descent iteration: â€¢ After each step, set negative values to 0. â€¢ Repeat. Sparsity and Non-Negativity â€¢ Projected gradient algorithm: â€“ Similar properties to gradient descent: â€¢ Guaranteed decrease of â€˜fâ€™ if Î±t is small enough. â€¢ Reaches local minimum under weak assumptions (global minimum for convex â€˜fâ€™). â€“ Least squares objective is still convex when restricted to non-negative variables. â€¢ Solution is a â€œfixed pointâ€: w* = max{0, w* - ğ›¼t ğ›»f(w*)}. â€“ Use this to decide when to stop. â€“ A generalization is â€œproximal-gradientâ€: â€¢ Instead of constraints, allows non-smooth terms (â€œfindMinL1â€). Projected-Gradient for NMF â€¢ Back to the non-negative matrix factorization (NMF) objective: â€“ Different ways to use projected gradient: â€¢ Alternate between projected gradient steps on â€˜Wâ€™ and on â€˜Zâ€™. â€¢ Or run projected gradient on both at once. â€¢ Or sample a random â€˜iâ€™ and â€˜jâ€™ and do stochastic projected gradient. â€“ Non-convex and (unlike PCA) is sensitive to initialization. â€¢ Hard to find the global optimum. â€¢ Typically use random initialization. â€¢ Also, we usually donâ€™t center the data with NMF. Application: Sports Analytics â€¢ NBA shot charts: â€¢ NMF (using â€œKL divergenceâ€ loss with k=10 and smoothed data). â€“ Negative values would not make sense here. http://jmlr.org/proceedings/papers/v32/miller14.pdf Application: Cancer â€œSignaturesâ€ â€¢ What are common sets of mutations in different cancers? â€“ May lead to new treatment options. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3588146/ Sparse Eigenfaces http://www.jmlr.org/papers/volume11/mairal10a/mairal10a.pdf Recent Work: Structured Sparsity â€¢ â€œStructured sparsityâ€ considers dependencies in sparsity patterns. â€“ Can enforce that â€œpartsâ€ are convex regions. http://jmlr.org/proceedings/papers/v9/jenatton10a/jenatton10a.pdf Beyond Matrix Factorization: Topic Models â€¢ For modeling categorical data, â€œtopic modelsâ€ are replacing NMF. â€“ A â€œfully-Bayesianâ€ model where sparsity arises naturally. â€“ Most popular example is called â€œlatent Dirichlet allocationâ€ (CPSC 440). http://menome.com/wp/wp-content/uploads/2014/12/Blei2011.pdf Sparse Matrix Factorization â€¢ Instead of non-negativity, we could use L1-regularization: â€“ Called sparse coding (L1 on â€˜Zâ€™) or sparse dictionary learning (L1 on â€˜Wâ€™). â€¢ Disadvantage of using L1-regularization over non-negativity: â€“ Sparsity controlled by Î»1 and Î»2 so you need to set these. â€¢ Advantage of using L1-regularization: â€“ Sparsity controlled by Î»1 and Î»2, so you can control amount of sparsity. â€“ Negative coefficients often do make sense. Sparse Matrix Factorization â€¢ Instead of non-negativity, we could use L1-regularization: â€“ Called sparse coding (L1 on â€˜Zâ€™) or sparse dictionary learning (L1 on â€˜Wâ€™). â€¢ Many variations exist: â€“ Mixing L2-regularization and L1-regularization. â€¢ Or normalizing â€˜Wâ€™ (in L2-norm or L1-norm) and regularizing â€˜Zâ€™. â€“ K-SVD constrains each zi to have at most â€˜kâ€™ non-zeroes: â€¢ K-means is special case where k = 1. â€¢ PCA is special case where k = d. Canonical Correlation Analysis (CCA) â€¢ Suppose we have two matrices, â€˜Xâ€™ and â€˜Yâ€™. â€¢ Want to find matrices WX and WY that maximize correlation. â€“ â€œWhat are the latent factors in common between these datasets?â€ â€¢ Define the correlation matrices: â€¢ Canonical correlation analysis (CCA) maximizes â€“ Subject to WX and WY having orthogonal rows. â€¢ Computationally, equivalent to PCA with a different matrix. â€“ Using the â€œanalysisâ€ view that PCA maximizes Tr(WTWXTX). Probabilistic PCA â€¢ With zero-mean (â€œcenteredâ€) data, in PCA we assume that â€¢ In probabilistic PCA we assume that â€¢ Integrating over â€˜Zâ€™ the marginal likelihood given â€˜Wâ€™ is Gaussian, â€¢ Regular PCA is obtained as the limit of Ïƒ2 going to 0. Generalizations of Probabilistic PCA â€¢ Probabilistic PCA model: â€¢ Why do we need a probabilistic interpretation? â€¢ Shows that PCA fits a Gaussian with restricted covariance. â€“ Hope is that WTW + Ïƒ2I is a good approximation of XTX. â€¢ Gives precise connection between PCA and factor analysis. Factor Analysis â€¢ Factor analysis is a method for discovering latent factors. â€¢ Historical applications are measures of intelligence and personality. â€¢ A standard tool and widely-used across science and engineering. https://new.edu/resources/big-5-personality-traits PCA vs. Factor Analysis â€¢ PCA and FA both write the matrix â€˜Xâ€™ as â€¢ PCA and FA are both based on a Gaussian assumption. â€¢ Are PCA and FA the same? â€“ Both are more than 100 years old. â€“ People are still arguing about whether they are the same: â€¢ Doesnâ€™t help that some packages run PCA when you call their FA method. PCA vs. Factor Analysis â€¢ In probabilistic PCA we assume: â€¢ In FA we assume for a diagonal matrix D that: â€¢ The posterior in this case is: â€¢ The difference is you have a noise variance for each dimension. â€“ FA has extra degrees of freedom. PCA vs. Factor Analysis â€¢ In practice there often isnâ€™t a huge difference: http://stats.stackexchange.com/questions/1576/what-are-the-differences-between-factor-analysis-and-principal-component-analysi Factor Analysis Discussion â€¢ Differences with PCA: â€“ Unlike PCA, FA is not affected by scaling individual features. â€“ But unlike PCA, itâ€™s affected by rotation of the data. â€“ No nice â€œSVDâ€ approach for FA, you can get different local optima. â€¢ Similar to PCA, FA is invariant to rotation of â€˜Wâ€™. â€“ So as with PCA you canâ€™t interpret multiple factors as being unique. Motivation for ICA â€¢ Factor analysis has found an enormous number of applications. â€“ People really want to find the â€œhidden factorsâ€ that make up their data. â€¢ But PCA and FA canâ€™t identify the factors. Motivation for ICA â€¢ Factor analysis has found an enormous number of applications. â€“ People really want to find the â€œhidden factorsâ€ that make up their data. â€¢ But PCA and FA canâ€™t identify the factors. â€“ We can rotate W and obtain the same model. â€¢ Independent component analysis (ICA) is a more recent approach. â€“ Around 30 years old instead of > 100. â€“ Under certain assumptions it can identify factors. â€¢ The canonical application of ICA is blind source separation. Blind Source Separation â€¢ Input to blind source separation: â€“ Multiple microphones recording multiple sources. â€¢ Each microphone gets different mixture of the sources. â€“ Goal is reconstruct sources (factors) from the measurements. http://music.eecs.northwestern.edu/research.php Independent Component Analysis Applications â€¢ ICA is replacing PCA and FA in many applications: â€¢ Recent work shows that ICA can often resolve direction of causality. https://en.wikipedia.org/wiki/Independent_component_analysis#Applications Limitations of Matrix Factorization â€¢ ICA is a matrix factorization method like PCA/FA, â€¢ Letâ€™s assume that X = ZW for a â€œtrueâ€ W with k = d. â€“ Different from PCA where we assume k â‰¤ d. â€¢ There are only 3 issues stopping us from finding â€œtrueâ€ W. 3 Sources of Matrix Factorization Non-Uniquness â€¢ Label switching: get same model if we permute rows of W. â€“ We can exchange row 1 and 2 of W (and same columns of Z). â€“ Not a problem because we donâ€™t care about order of factors. â€¢ Scaling: get same model if you scale a row. â€“ If we mutiply row 1 of W by Î±, could multiply column 1 of Z by 1/Î±. â€“ Canâ€™t identify sign/scale, but might hope to identify direction. â€¢ Rotation: get same model if we rotate W. â€“ Rotations correspond to orthogonal matrices Q, such matrices have QTQ = I. â€“ If we rotate W with Q, then we have (QW)TQW = WTQTQW = WTW. â€¢ If we could address rotation, we could identify the â€œtrueâ€ directions. A Unique Gaussian Property â€¢ Consider an independent prior on each latent features zc. â€“ E.g., in PPCA and FA we use N(0,1) for each zc. â€¢ If prior p(z) is independent and rotation-invariant (p(Qz) = p(z)), then it must be Gaussian (only Gaussians have this property). â€¢ The (non-intuitive) magic behind ICA: â€“ If the priors are all non-Gaussian, it isnâ€™t rotationally symmetric. â€“ In this case, we can identify factors W (up to permutations and scalings). PCA vs. ICA http://www.inf.ed.ac.uk/teaching/courses/pmr/lectures/ica.pdf Independent Component Analysis â€¢ In ICA we approximate X with ZW, assuming p(zic) are non-Gaussian. â€¢ Usually we â€œcenterâ€ and â€œwhitenâ€ the data before applying ICA. â€¢ There are several penalties that encourage non-Gaussianity: â€“ Penalize low kurtosis, since kurtosis is minimized by Gaussians. â€“ Penalize high entropy, since entropy is maximized by Gaussians. â€¢ The fastICA is a popular method maximizing kurtosis. ICA on Retail Purchase Data â€¢ Cash flow from 5 stores over 3 years: http://www.stat.ucla.edu/~yuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf ICA on Retail Purchase Data â€¢ Factors found using ICA: http://www.stat.ucla.edu/~yuille/courses/Stat161-261-Spring14/HyvO00-icatut.pdf Motivation for Topic Models â€¢ Want a model of the â€œfactorsâ€ making up documents. â€“ Instead of latent-factor models, theyâ€™re called topic models. â€“ The canonical topic model is latent Dirichlet allocation (LDA). â€“ â€œTopicsâ€ could be useful for things like searching for relevant documents. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/ Term Frequency â€“ Inverse Document Frequency â€¢ In information retrieval, classic word importance measure is TF-IDF. â€¢ First part is the term frequency tf(t,d) of term â€˜tâ€™ for document â€˜dâ€™. â€“ Number of times â€œwordâ€ â€˜tâ€™ occurs in document â€˜dâ€™, divided by total words. â€“ E.g., 7% of words in document â€˜dâ€™ are â€œtheâ€ and 2% of the words are â€œLebronâ€. â€¢ Second part is document frequency df(t,D). â€“ Compute number of documents that have â€˜tâ€™ at least once. â€“ E.g., 100% of documents contain â€œtheâ€ and 0.01% have â€œLeBronâ€. â€¢ TF-IDF is tf(t,d)*log(1/df(t,D)). Term Frequency â€“ Inverse Document Frequency â€¢ The TF-IDF statistic is tf(t,d)*log(1/df(t,D)). â€“ Itâ€™s high if word â€˜tâ€™ happens often in document â€˜dâ€™, but isnâ€™t common. â€“ E.g., seeing â€œLeBronâ€ a lot it tells you something about â€œtopicâ€ of article. â€“ E.g., seeing â€œtheâ€ a lot tells you nothing. â€¢ There are *many* variations on this statistic. â€“ E.g., avoiding dividing by zero and all types of â€œfrequenciesâ€. â€¢ Summarizing â€˜nâ€™ documents into a matrix X: â€“ Each row corresponds to a document. â€“ Each column gives the TF-IDF value of a particular word in the document. Latent Semantic Indexing â€¢ TF-IDF features are very redundant. â€“ Consider TF-IDFs of â€œLeBronâ€, â€œDurantâ€, â€œHardenâ€, and â€œKobeâ€. â€“ High values of these typically just indicate topic of â€œbasketballâ€. â€¢ We can probably compress this information quite a bit. â€¢ Latent Semantic Indexing/Analysis: â€“ Run latent-factor model (like PCA or NMF) on TF-IDF matrix X. â€“ Treat the principal components as the â€œtopicsâ€. â€“ Latent Dirichlet allocation is a variant that avoids weird df(t,D) heuristic.","libVersion":"0.2.1","langs":""}