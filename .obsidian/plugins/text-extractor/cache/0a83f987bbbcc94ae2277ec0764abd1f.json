{"path":".obsidian/plugins/text-extractor/cache/0a83f987bbbcc94ae2277ec0764abd1f.json","text":"Final Exam for Math 318 Professor Geoﬀrey Schiebinger April 29, 2020 Instructions: The exam is open notes and open book. You have 24 hours to complete the exam. Please upload your answers to Canvas before noon on Thursday April 16. Late exams will not be accepted. There are 8 questions worth a total of 72 marks. Explain your reasoning in complete sentences. If the statement of a question is unclear, you are encouraged to post clariﬁcation questions to piazza. 1. (6 points) A lumber mill produces boards that are 2, 4, 6, 8, or 10 meters long. They are given trees with lengths uniformly distributed between 0 and 12 meters long and make one board from each tree, using as much wood as possible from each tree. Determine, approximately, the probability that after 1000 trees are harvested, the length of wasted wood exceeds x meters. Simplify your answer. Solution: The amount of wood wasted for each tree is a random variable Xi ∼ Unif(0, 2) for i = 1, . . . , 1000. Note that µ = EXi = 1 and σ2 = Var(X) = 1/3. By the central limit theorem 1 √1000 1000∑ i=1 √3(Xi − 1) D → Z ∼ N (0, 1). Therefore P ( 1000∑ i=1 Xi > x) = P ( √ 1000 3 Z + 1000 > x) = P (Z > 0.055(x − 1000)). 2. (10 points). Consider random variables X, Y, Z with Z ∼ Bernoulli(1/2), X ∼ Unif(0, 1), and Y ∼ Unif(0, 1). Prove or disprove the following statement: Cov(X, Y ) > 0 ⇐⇒ E(Cov(X, Y |Z)) > 0. Note: the conditional covariance is deﬁned in terms of the conditional expectation in the same way that the covariance is deﬁned in terms of the expectation: Cov(X, Y |Z) = E[(X − E(X|Z))(Y − E(Y |Z)) |Z]. Solution: This is not true. It is called Simpson’s paradox. For a counter example, let X|Z and Y |Z be as in the following picture. 1 XY0011(X, Y )|Z = 0(X, Y )|Z = 1 3. (10 points) You arrive at the airport and need to get through passport control quickly to make your ﬂight. There are two options: the ﬁrst line has 5 people in it and two agents checking passports at the end. People exit this line when either agent has examined their passport. The amount of time it takes for these agents to check a passport is Exp(λ1). The second line has 4 people in it and one agent checking passports at the end. The amount of time it takes for this agent to check a passport is Exp(λ2). Compute the expected amount of time you would have to wait in both lines. Express your answer in terms of λ1 and λ2. Solution: Let Xn denote the time it takes for the ﬁrst line to go from n to n − 1 people. For n > 1, Xn is the minimum of two exponential λ1 random variables, so Xn ∼ Exp(2λ1) for n > 1 and for n = 1, X1 ∼ Exp(λ1). Let Sn denote the time it takes to go from n people down to 0 people in the ﬁrst line. Sn = X1 + X2 + . . . + Xn. We are interested in S6 (because we include ourselves). ES6 = EX1 + 5EX2 = 5 2λ1 + 1 λ 1 The second line is easier because there is only one agent. Let Yn denote the time it takes for the second line to go from n to n − 1 people. Yn ∼ Exp(λ2). The expected time it takes to get down to zero people is E(Y1 + Y2 + Y3 + Y4 + Y5) = 5E(Y1) = 5 λ2 . 4. (8 points) Let X is a random variable with density f (x) = ax−4 on the interval [b, ∞) and f (x) = 0 for x < b. (a) (2 points) If b = 1, what is a? (b) (4 points) If EX = 1, what are a and b? (c) (2 points) Let a, b be the ones you found in part (b). What is Var(X)? Solution: (a) (2 points) a = 3 since 1 = ∫ ∞ 1 f (x)dx = ∫ ∞ 1 ax−4dx = −a x−3 3 \f \f \f ∞ 1 = a 3 . 2 (b) (4 points) a = 8 9 and b = 2 3 since ∫ ∞ b ax−4dx = a b−3 3 = 1 =⇒ a = 3b3 and ∫ ∞ b ax−3dx = 1 =⇒ a = 2b2. Combining these two, we get 2b2 = 3b 3 and b = 0 is not an option since this would imply a = 0. (c) (2 points) Note EX = 1, so VarX = E(X 2) − 1. E(X 2) = ∫ ∞ b ax2dx = a b =⇒ VarX = 8/9 2/3 − 1 = 1 3 . 5. (15 points) There is a pandemic on a small world with two cities called A and B. At any given month, each city is either experiencing an outbreak (state 1) or not experiencing an outbreak (state 0). The following events can happen independently: • A city in state 0 can spontaneously experience outbreaks with probability 1/8. This causes the city to be in state 1 next month. • A city in state 1 can infect the other city with probability 1/2. This causes the other city to be state 1 next month. • A city in state 1 can resolve their outbreak with probability 1/2. This causes that city to be in state 0 next month, unless the other city also infects it. (a) (1 point) Explain how to represent the state of the world as a Markov chain with state space {0, 1}2. Explain why it is a Markov chain. (b) (4 points) Write down the transition probability matrix for this Markov chain. Arrange the rows and columns in the following order: 00, 01, 10, 11. (c) (2 points) Which states, if any, are absorbing states? Which states if any are periodic? Is the chain ergodic? (d) (6 points) Suppose the world starts in state 01. What is the expected number of months before returning to this state? Explain your reasoning. (e) (2 points) Suppose the world starts in state 01. Let Hn = − ∑ s πn(s) log πn(s) denote the entropy of the world state after n months (here πn(s) is the probability of ﬁnding the world in state s after n months). Does Hn increase monotonically with n? Explain your reasoning. Solution: (a) (1 point) If city i is in state bi ∈ {0, 1}, then the state of the world is B = (b1, b2). At month n, the state of the world is a random variable Bn taking values in {0, 1} 2. The sequence of random variables B1, B2, . . . satisﬁes the Markov property because all of the rules above determine Bn+1 solely as a function of Bn. (b) (4 points) The transition probability matrix is P =         49 64 7 64 7 64 1 64 7 32 7 32 9 32 9 32 7 32 9 32 7 32 9 32 1 16 3 16 3 16 9 16         3 (c) (2 points) No absorbing states, no periodic states, and the chain is ergodic. (d) (6 points) The stationary distribution is π = [0.39862543, 0.17869416, 0.17869416, 0.24398625] as can be veriﬁed by matrix multiplication: πP = π. Therefore the expected number of months is 1 0.17869416 = 5.6 months. (e) (2 points) The entropy does not increase monotonically with n because π is not uniform. 6. (3 points) Let A, B, C be independent events. Prove that A and B ∩ C are independent. Solution: P (A ∩ (B ∩ C)) = P (A ∩ B ∩ C) = P (A)P (B)P (C) = P (A)P (B ∩ C). 7. (14 points) The Queen has a treasure vault with N gems in two varieties: rubies and saphires. Every day, a mischievious thief enters the Queen’s treasury, selects one of her N gems at random and replaces it with one of his N gems, also selected at random. In total, two collectively have N rubies and N saphires. The queen doesn’t mind because she likes both rubys and and saphires and she is interested in probability, so she asks her court scholar the following questions: (a) (1 point) Let Rn denote the number of rubies in the vault on day n. Explain why {Rn}n≥0 is a Markov chain. (b) (3 points) Find the state space S and the transition probabilities Pi,j for all i, j ∈ S. (c) (3 points) Is the Markov chain reversible? Explain why or why not. (d) (5 points) Find the stationary distribution π of the Markov chain and verify your answer. (e) (2 points) Suppose that the Queen’s treasury has 1 ruby and N − 1 saphires. How many days on average does it take until she again has exactly 1 ruby? Solution: (a) (1 point) Each day, the thief’s random choice depends only on the current state. (b) (3 points) The state space is {0, 1, 2, 3, . . . , N }. The transition probabilities are Pi,i−1 = i 2 N 2 , Pi,i = 2i(N − i) N 2 , Pi,i+1 = (N − i) 2 N 2 for i = 1, . . . , N − 1, and P0,1 = 1, PN,N −1 = 1. (c) (3 points) Yes, the chain is reversible because the number of transitions from i to state i + 1 will be essential equal (i.e. ±1) to the number of transitions from i + 1 to i. (d) (5 points) The stationary distribution πi satisﬁes the detailed balance conditions πiPi,i+1 = πi+1Pi+1,i. This gives πi+1 πi = (N −i) 2 i2 , which implies π1 π0 = N 2 π2 π0 = (N 2 )2 4 and in general πi π0 = (N i )2. Therefore πi = (N i )2 (N 0 )2 + (N 1 )2 + . . . + (N N)2 = (N i )2 (2N N ) . (e) (2 points) The expected number of days before she again has exactly 1 ruby is 1 π1 = (2N N ) (N 1 )2 . 8. (6 points) An internet router makes 10 6 attempts to connect to a server every hour. The attempts are independent and each attempt is successful with probability 2 × 10−6. Let T be the time to ﬁrst success in seconds. (a) (4 points) Find, approximately, as a function of t, the probability that T ≤ t. (b) (2 points) Compute P (T > t + s|T > t). Solution: (a) Let X denote the number of trials until the ﬁrst success. X ∼ Geom(p) with p = 2 × 10 −6. Note that 10−6X converges in distribution to Exp( 1 2 ). P (10 −6X < a) ≈ ∫ a 0 1 λ e− x λ dx. Now T = 10 −6Xhours × 3600sec hour . Plugging this in gives P (T < t) ≈ ∫ t 3600 0 1 2 e −x 2 dx = 1 − exp(− t 7200 ). (b) By the memoryless property of the exponential distribution, P (T > t + s|T > t) = P (T > s) = 1 − P (T < s) = exp(− s 7200 ). 5","libVersion":"0.2.1","langs":""}