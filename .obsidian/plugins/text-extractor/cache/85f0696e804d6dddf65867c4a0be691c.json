{"path":".obsidian/plugins/text-extractor/cache/85f0696e804d6dddf65867c4a0be691c.json","text":"UNIVERSITY OF CALIFORNIA SAN DIEGO Formalizing the Beginnings of Bayesian Probability Theory in the Lean Theorem Prover A thesis submitted in partial satisfaction of the requirements for the degree Master of Science in Computer Science by Rishikesh Vaishnav Committee in charge: Professor Sicun Gao, Chair Professor Sorin Lerner Professor Lawrence Saul 2022 Copyright Rishikesh Vaishnav, 2022 All rights reserved. The thesis of Rishikesh Vaishnav is approved, and it is ac- ceptable in quality and form for publication on microﬁlm and electronically. University of California San Diego 2022 iii TABLE OF CONTENTS Thesis Approval Page . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii Table of Contents . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iv List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vi Acknowledgements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . vii Abstract of the Thesis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . viii Chapter 1 Lean Theorem Prover: An Introduction . . . . . . . . . . . . . . . . . . 1 1.1 What is Lean? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1.2 The Basics of Theorem Proving in Lean . . . . . . . . . . . . . . . 3 1.2.1 A Hierarchy of Types . . . . . . . . . . . . . . . . . . . . . 3 1.2.2 Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2.3 Inductive Types . . . . . . . . . . . . . . . . . . . . . . . . 5 1.2.4 Pi Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 1.2.5 Propositions as Types . . . . . . . . . . . . . . . . . . . . . 9 1.2.6 Predicates and Sets . . . . . . . . . . . . . . . . . . . . . . 11 1.2.7 Example: Proving Commutativity of or . . . . . . . . . . . 12 Chapter 2 Measure-Theoretic Foundations . . . . . . . . . . . . . . . . . . . . . . 18 2.1 The Event Space . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.2 The Outer Measure . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.2.1 Outer Measure Transformations . . . . . . . . . . . . . . . 21 2.3 Measurable Spaces . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.3.1 Transforming Measurable Spaces . . . . . . . . . . . . . . 26 2.3.2 A Galois Connection . . . . . . . . . . . . . . . . . . . . . 28 2.3.3 The Trimmed Outer Measure . . . . . . . . . . . . . . . . . 30 2.3.4 Measurable Spaces on Pi Types . . . . . . . . . . . . . . . 32 2.4 Deﬁning a Measure . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2.5 Measure Transformations . . . . . . . . . . . . . . . . . . . . . . . 36 2.5.1 From a Partial Function . . . . . . . . . . . . . . . . . . . . 36 2.5.2 The Carathéodory Criterion . . . . . . . . . . . . . . . . . 38 2.5.3 Lifting an Outer Measure Transformation . . . . . . . . . . 39 2.5.4 Restricting a Measure . . . . . . . . . . . . . . . . . . . . 41 2.5.5 Mapping a Measure . . . . . . . . . . . . . . . . . . . . . . 42 iv Chapter 3 Probability Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.1 Independence . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.1.1 Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.1.2 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.2 Conditional Probability . . . . . . . . . . . . . . . . . . . . . . . . 52 3.2.1 Properties . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 3.2.2 Deﬁning Conditional Independence . . . . . . . . . . . . . 56 3.3 Random Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . 59 3.3.1 The Joint Distribution . . . . . . . . . . . . . . . . . . . . 60 3.3.2 Restricting a Pi . . . . . . . . . . . . . . . . . . . . . . . . 61 3.3.3 The Marginal Distribution . . . . . . . . . . . . . . . . . . 64 3.3.4 Marginalization . . . . . . . . . . . . . . . . . . . . . . . . 65 3.3.5 Independence on Random Variables . . . . . . . . . . . . . 67 3.3.6 Conditional Independence on Random Variables . . . . . . 68 Bibliography . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71 v LIST OF FIGURES Figure 2.1: Inducing an outer measure from a partial function. . . . . . . . . . . . . . 32 Figure 2.2: Generating a Pi measurable space. . . . . . . . . . . . . . . . . . . . . . . 34 Figure 3.1: Unrestricting a restricted Pi set. . . . . . . . . . . . . . . . . . . . . . . . 62 vi ACKNOWLEDGEMENTS I would ﬁrstly like to thank my advisor, Professor Sean Gao, for getting me interested in formal mathematics in the ﬁrst place, and for his great support and enthusiasm for my research interests. I would like to thank the hundreds of volunteer open-source contributors to the Neovim editor (and its plugins) for developing an inﬁnitely extensible tool that allows me to transcribe my thoughts into code as quickly as physically possible. Lastly, I would like to thank all of the members of the Lean community for their help- fulness in answering my questions and reviewing my pull requests, and for their hard work in developing, maintaining, and promoting a software that heralds a brilliant future in mathematics. vii ABSTRACT OF THE THESIS Formalizing the Beginnings of Bayesian Probability Theory in the Lean Theorem Prover by Rishikesh Vaishnav Master of Science in Computer Science University of California San Diego, 2022 Professor Sicun Gao, Chair In this master’s thesis, we present the beginnings of a formalization of Bayesian Probability Theory in the Lean Theorem Prover, a rapidly developing software for mathematical proof checking principally developed by Leonardo de Moura at Microsoft Research, that has enabled the rapid growth of a substantial digitized mathematics library (known as “mathlib”) over the past few years. We start with a short introduction to Lean, describing the very basics of its theory and how deﬁnitions and proofs are written in its type-theoretic syntax. We then move on to a mathematical description a particular part of Lean’s mathematics library that provides all of the theoretical foundation we need for our formalization. This includes a description of the measure-theoretic foundations of probability theory, including the concepts viii of outer measures, measurable spaces, the Carathéodory criterion, measures, measure maps, and measure restrictions. We then relate these foundations to probability theory as we know it, deﬁning independence, conditional probability, random variables, joint distributions, marginal distributions, and conditional distributions and proving interesting facts about these constructions. We reveal that many of the deﬁnitions and properties that may be taken for granted (as “axioms”) in probability theory actually have a fundamentally formal deﬁnition from the very basics of measure theory. This work sets up myself and other members of the global Lean community for many more exciting developments and ﬁrst formalizations in probability theory in the coming months. ix Chapter 1 Lean Theorem Prover: An Introduction 1.1 What is Lean? Mathematics as many know it is “informal”, that is, it consists of stating and proving results whose correctness and importance is veriﬁed by a group of established researchers in that ﬁeld. In many cases, deﬁnitions in subﬁelds of mathematics are introduced as convenient “axioms” rather than deriving from more general theories, with equivalences to deﬁnitions in such theories (which may be a bit less palatable initially) being shown later as theorems. Topics in mathematics (especially recent ones) are often dispersed throughout multiple papers, textbooks, and other sources. Formal theorem proving is a rapidly growing ﬁeld that has great potential to improve the rigor, quality, and accessibility of mathematical research in the coming years. It relates to the use of software to write mathematical deﬁnitions, prove facts about these deﬁnitions (that are formally veriﬁed by the theorem proving software), and ultimately use these deﬁnitions write programs with veriﬁable properties. Theorem proving has its origin in the simply-typed λ-calculus introduced by Church [1] which is the basis of many functional programming languages. A correspondence between this calculus and simple forms of logical reasoning was discovered, and 1 is known as the “Curry-Howard isomorphism” [2]. This was extended to dependent types by Löf [3], and evolved into Coquand’s “Calculus of Constructions” [4], which led to the development of the well-known “Coq” theorem prover. Coq also integrated the theory of inductive types from the “Calculus of Inductive Constructions” [5], which turned it into a full-ﬂedged theorem prover. At this point, it was clear to mathematicians that essentially all of mathematics could be formalized under such a system. However, making such formalizations required substantially more effort than doing things the informal way. When working with Coq, you have to have some knowledge of the type theory to prove things at the lower levels, and you need to provide parts of the proof that would seem “obvious” to any person reading the code, as this was necessary for the proof to type-check. In short, theorem proving in Coq provided completeness, but not much convenience. Many theorem provers have been developed since “Coq” (e.g. Isabelle, Agda, Idris), though their focus has mainly been on being functional programming languages that just happen to support theorem proving. The Lean Theorem Prover [6] has been rapidly developing since 2013, principally developed by Microsoft researcher Leonardo De Moura, and is more geared towards being usable by mathematicians. It uses a similar type theory to that of Coq, and offers a signiﬁcant edge in the area of usability, largely because of two main factors: • It introduces what it calls “elaboration” [7], which uses a general method to infer types and implicit arguments to functions. It also has type class inference, which enables it to build these arguments itself from user-deﬁned construction rules if they aren’t already in the local context. This allows many terms that are required for type-correctness of a proof to be left out entirely, making proofs that are done on this type-theoretic level (known as “term mode”) much more readable by ordinary mathematicians. • It allows for extensive metaprogramming [8] in the form of “tactics”. This allows mathe- maticians to abstract away from Lean’s type theory entirely and only have to consider the raw “proof state”, consisting of hypotheses (type instances) and goals (types). Essentially, 2 this is an automation layer between high-level reasoning and the underlying type theory. Lean allows you to build tactics from the ground up, as they can be implemented in Lean itself (starting from a few “fundamental” state-altering tactics) using monadic binding (the inductive type of these monads is deﬁned just like anything else in Lean’s standard library!). Lean has garnered attention from many prominent mathematicians worldwide, and many expect formalized mathematics to supplant informal mathematics entirely in the years to come as such tools become more usable. The Lean community’s mathlib repository [9] is rapidly growing as the main place where mathematicians are formalizing their results, and in particular Kevin Buzzard’s Xena Project [10] aims to cover all of undergraduate mathematics. While most existing formalizations have been done in Lean 3, a new version, Lean 4 [11], boasts several signiﬁcant usability improvements. Work is underway to port all existing Lean 3 code, which will likely be completed before the end of the year. 1.2 The Basics of Theorem Proving in Lean 1.2.1 A Hierarchy of Types Lean consists of a hierarchy of type collections known as \"type universes\". There are inﬁnitely many such universes, starting with Type 0. This universe consists of all of the familiar mathematical types, including the natural numbers (nat/N), the integers (int/Z), the rational numbers (rat/Q), the real numbers (real/R), etc., and types that are created for many other speciﬁc applications (vectors, matrices, complex numbers, etc.). Type 0 is itself considered a type in its parent universe, Type 1, which is a type in its parent universe, Type 2, and so on. For the purposes of this report, all you really need to know about are Type 0 and Type 1 (more on Type 1 in later sections). Higher order types only really exist for theoretical reasons (since everything has to have some type; the origins of this 3 stratiﬁcation come from a classic paradox in set theory known as “Russell’s Paradox”). 1.2.2 Functions Naturally, the next thing we want to do is deﬁne functions mapping from one type to another. In Lean, we use the arrow (→) symbol between two types to indicate a mapping between them. For example, here is the deﬁnition of addition on natural numbers: def add : nat → nat → nat | a nat.zero := a | a (nat.succ b) := nat.succ (add a b) -- add : N → N → N #check add -- N → N → N : Type #check N → N → N (the deﬁnition itself should become clearer in later sections). The #check commands tell Lean to give us the type of a given object. As you can see, functions are treated as their own types right alongside the types they map from/to. Strictly speaking, the type of the function is the largest type universe index of all of its arguments and output type (the exception to this is when the output type is a proposition (described below), in which case the type of the function is Sort 0). For example: -- N → Type : Type 1 -- because N : Type 0 and Type 0 : Type 1 #check N → Type -- Type 1 → Type 5 → Type 2 : Type 6 -- because (Type 5 : Type 6) → (Type 2 : Type 3) : Type 6 #check Type 1 → Type 5 → Type 2 4 The construction of add above is based on an induction principle that is automatically generated by Lean based on the deﬁnition of its inductive type (see the next section). We can build arbitrary functions using what’s called “lambda abstraction”. The following example, which creates a function that adds 5 to a natural number, is an example of this: -- λ (x : N), x + 5 : N → N #check λ x : N, x + 5 Functions can take multiple arguments as well: -- λ (x y : N) (z : R), (↑x + 5) * (z + ↑y) : N → N → R → R #check λ (x y: N) (z : R), ↑(x + 5) * (z + y) This is really just syntactic sugar for multiple lambda abstractions (one for each argument). 1.2.3 Inductive Types In Lean, new types are created as “inductive types”, in which various “constructor” functions are deﬁned that can be applied to produce an instance of that type. One simple example of this is the deﬁnition of “bool”, which is a binary enumerated type: inductive bool : Type | ff : bool | tt : bool This produces the functions bool.ff : bool and bool.tt : bool to instantiate a bool, which don’t take any arguments. Constructors, and the type itself, can take arguments, as in inductive prod (α : Type u) (β : Type v) | mk : α → β → prod Here, a product (ordered pair) type prod is deﬁned for any two types – for example prod nat real and prod bool bool are different types. An instance of prod bool bool would be 5 produced, for example, by calling prod.mk bool.tt bool.ff. This deﬁnition generates the induction principle prod.rec that lets you compute based on the deﬁnition of a product: def prod_example (p : prod bool N) : N := prod.rec (λ b n, cond b (2 * n) (2 * n + 1)) p The ﬁrst argument is a function from the elements that were used to construct the product to the output type (which is inferred). The last argument is the product itself. Here, cond is based on the induction principle bool.rec: def cond {α : Type u} (b : bool) (x : α) (y : α) : α := bool.rec y x b which maps to x or y depending on whether b is true or false, respectively. Because the con- structors of bool take no arguments, the arguments to bool.rec are not lambda abstractions, but rather just the output type itself. Speciﬁcally, we can check the types of bool.rec and prod.rec. For some implicit “motive” type C, -- similar to: C → C → bool → C #check bool.rec -- similar to: (α → β → C) → α × β → C #check prod.rec The types have been somewhat simpliﬁed here, in reality they also allow the output type to depend on whether the boolean is true or false, or what the particular product is – see the next section on dependent types. We deﬁne the natural numbers inductively as: 6 inductive nat | zero : nat | succ (n : nat) : nat Here, the function nat.succ produces a new natural number from an existing one. This gives us the induction principle: -- similar to: C → (N → C → C) → N → C #check nat.rec (again, this is a simpliﬁcation of the actual (dependent) type). The ﬁrst argument is the output for the zero case, and the second argument (N → C → C) is a rule for building the output based on a previous number and the value of the output on that number (hence the “inductive” in inductive type). We can deﬁne addition as: protected def add : nat → nat → nat | a zero := a | a (succ b) := succ (add a b) which is fundamentally deﬁned in terms of the recursor nat.rec but uses what’s known as the “equation compiler” to allow us to instead write the easy-to-read form above. What this function is saying is, in the case that the second number is zero, return the ﬁrst number, and in the case that it is nonzero – that is, it is succ b for some b : nat, return the number after the addition with b (whose value we will have from the induction principle). So, we can see that this function is applying nat.rec to the second argument. We can deﬁne the equivalent: protected def add (a : nat) (b : nat) : nat := nat.rec a (λ prev add_prev, nat.succ add_prev) b where add_prev takes the place of the sugared add a b in the previous deﬁnition. 7 1.2.4 Pi Types One of the key aspects of Lean is that functions can map to different types depending on the input. This is otherwise known as “dependent types”. The syntax for such types in Lean uses the “Π” symbol. For example, if we have a function f : bool → Type, we can deﬁne the dependent type Π (b : bool), f b, Where each b maps to the type f b (whatever that is). In fact, the notation α → β is really just a shorthand for Π (a : α), β, i.e. a dependent type that isn’t actually dependent (ﬁxed as β). We can now explain the types of the recursor function examples above in full. First, we have the boolean recursor: -- Π {motive : bool → Type*}, motive ff → motive tt -- → Π (n : bool), motive n #check bool.rec This recursor really has a dependent type which takes a “motive” argument that describes what type each bool (true or false) should map to. The curly braces make this an implicit argument. So, given a motive, our function is fully deﬁned as long as we can produce the correct type in the false case (the ﬁrst (explicit) argument) and in the true case (the second argument). The product recursor has the dependent type: -- Π {α : Type u} {β : Type v} {motive : prod α β → Type*}, -- (Π (fst : α) (snd : β), motive (prod.mk fst snd)) -- → Π (n : prod α β), motive n #check prod.rec Here, given a motive that speciﬁes a type for every product, we have to provide a function that maps the elements of the product to the required type. Lastly, we have the natural number recursor: 8 -- Π {motive : N → Type*}, motive 0 → (Π (n : N), motive n → motive n.succ) → Π (n : N), motive n #check nat.rec where, given a motive that speciﬁes a type for every natural number, we have to provide the motive type on zero and a function that maps from one motive type to the next. 1.2.5 Propositions as Types The topics we’ve discussed so far make Lean usable as a functional programming language, however the concept that makes it useful as a theorem prover is what’s known as the “propositions- as-types” correspondence. We have a special type in Type 0 called Prop which is the type of logical propositions. Everything in Prop is itself its own “sort”, which is a generalization of a “type” to include Prop in the hierarchy. Sort 0 is Prop, Sort 1 is Type 0, Sort 3 is Type 1, and so on. Proving some p : Prop involves producing an element of p. We can deﬁne the logical connectives “and” and “or” on this proposition space using inductive types: inductive or (a b : Prop) : Prop | inl (ha : a) : or | inr (hb : b) : or inductive and (a b : Prop) : Prop | intro (ha : a) (hb : b) : and So, a sort (more speciﬁcally, a proposition) or a b exists for every proposition a b : Prop. And we produce a “proof” (i.e. instance) of or a b by either producing an instance of a or b and applying or.inl or or.inr correspondingly. Similarly, a proposition and a b exists for every proposition a b : Prop. And we produce a proof of and a b by producing both an instance of a and b and then applying and.intro. 9 In this sense, a function in this propositional space exactly corresponds to an implication: for propositions a b : Prop, the function a → b represents the fact that a implies b. We prove this fact by producing an instance of this function, that is, a mapping from an instance of a to an instance of b. So, we can formulate the if-and-only-if equivalence relation as: inductive iff (a b : Prop) : Prop | (hmp : a → b) (hmpr : b → a) : iff that is, we need to provide the implication in both directions. In this representation, we have the base propositions true and false: inductive true : Prop | trivial : true inductive false : Prop in which we can easily produce a proof of true with the rule true.trivial, and producing an instance of false is impossible. If we have some arbitrary type p, we create the negation of p with the implication p → false, written ¬p. This type-theoretic correspondence has its limits. For example, consider the statement p ∨ ¬p. We know from classical logic that this statement is true for any p. However, it’s impossible to make use of this fact constructively (i.e. via type theory) without bending the rules a bit. That is, we can’t construct and instance p ∨ ¬p for any proposition p. To see this, suppose p is the statement that a program halts on a certain input. While it is possible to decide (i.e. construct a proof of this proposition or its negation) for some programs and some inputs, it is literally impossible to write a program to decide this for all programs on all inputs, as this is the famous “halting problem”. So, Lean provides a “classical logic” library that provides this fact by building on some axioms that aren’t exactly type-correct themselves (the details of this are somewhat out of scope for this report). 10 1.2.6 Predicates and Sets For some type α, we call a function α → Prop a “predicate”. It can be thought of as an indicator of a property on each element of α in the sense that each maps to a proposition that is either true (inhabited) or false (uninhabited). You can deﬁne predicates on an arbitrary number of arguments, in particular binary predicates on a single type is known as a relation. In fact, the inductive types and and or, and iff we have already deﬁned can be interpreted as propositional relations (binary propositional predicates) when interpreted as functions each having the type Prop → Prop → Prop. In Lean, single-argument predicates exactly correspond to a notion of a set. This is a very convenient abstraction, because rather than having to enumerate all of the elements of a set (which is often impossible), you can simply map an element to a proposition that, if you can prove it, provides evidence that the element is in the set. As such, in Lean, set α is just a synonym for α → Prop: def set (α : Type*) := α → Prop The Existential and Universal Quantiﬁers Now, suppose we have some predicate C : α → Prop, and want to show that there exists an element of α that satisﬁes C. In Lean, we use the inductive type: inductive Exists {α : Sort u} (C : α → Prop) : Prop | intro (w : α) (h : C w) : Exists meaning in order to prove this fact, we need to provide both an element of α and a proof of C w. In Lean we use the special notation ∃ (a : α), C a to indicate this type. Deﬁning the universal (∀) quantiﬁer in Lean is quite simple – nothing even needs to be deﬁned! It is simply a Pi type from a : α to C a, speciﬁcally ∀ a : α, C a, where ∀ is simply syntactic sugar for Π in the context of propositions. So, in order to prove this proposition, you 11 must map from every element a : α to a proof of C a. Subtypes Given a type α we can deﬁne a new type, called a “subtype”, based on α, by providing a set α. The subtype “restricts” the type α to that set in that in order to produce an element of subtype α, you must have an element of α that is in the provided set. In Lean, this is very similar to the existential qualiﬁer, but instead of being in Prop, it is in whatever type α originally had: inductive subtype {α : Sort u} (p : α → Prop) : Sort u | intro (val : α) (property : p val) : subtype 1.2.7 Example: Proving Commutativity of or Let’s prove a very simple fact to get a feel for how theorem proving is done in Lean. We will show that propositional “or” is commutative. We start with the proof statement: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := sorry -- prove p ∨ q ↔ q ∨ p (sorry fakes closing a goal in Lean and leaves a warning). Placing variables left of the colon is syntactic sugar that allows you to avoid having to write out the full dependent type in the statement and make a lambda for those types in the proof itself: theorem or_comm_iff : Π {p : Prop} {q : Prop}, p ∨ q ↔ q ∨ p := λ {p : Prop} {q : Prop}, sorry -- prove p ∨ q ↔ q ∨ p A Term-Mode Proof Let’s ﬁrst prove this in “term-mode” which is just the fundamental type-theoretic language we’ve been using so far. In the proof we have two directions to show, which we introduce with 12 the introduction rule for iff, which is iff.intro theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro sorry -- prove p ∨ q → q ∨ p sorry -- prove q ∨ p → p ∨ q In the ﬁrst direction we use a lambda to introduce the hypothesis p ∨ q: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro (λ hpq, sorry) -- prove q ∨ p sorry -- prove q ∨ p → p ∨ q This gives us a hypothesis hpq : p ∨ q. We can deconstruct this by cases using the or.rec elimination rule: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro (λ hpq, or.rec sorry sorry hpq) -- prove p → q ∨ p and q → q ∨ p sorry -- prove q ∨ p → p ∨ q We make a lambda for both cases to introduce the hypotheses: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro (λ hpq, or.rec (λ hp, sorry) -- prove q ∨ p with hp : p (λ hq, sorry) hpq) -- prove q ∨ p with hq : q sorry -- prove q ∨ p → p ∨ q Now, we can use the introduction rules for or, or.inr and or.inl, to close both of the goals using our exact hypotheses: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro (λ hpq, or.rec (λ hp, or.inr hp) (λ hq, or.inl hq) hpq) -- done! sorry -- prove q ∨ p → p ∨ q The other direction is essentially the same, giving us the full proof: 13 theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := iff.intro (λ hpq, or.rec (λ hp, or.inr hp) (λ hq, or.inl hq) hpq) -- done! (λ hqp, or.rec (λ hq, or.inr hq) (λ hp, or.inl hp) hqp) -- done! A Tactic Mode Proof Lean also offers what is known as “tactic mode”. This lets theorem proving feel a bit more like imperative programming by offering a set of “tactics” that alter the proof state (represented in the comments in the previous example). Tactic mode is enclosed in a begin ... end block: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin sorry, -- prove p ∨ q ↔ q ∨ p end We start with the split tactic, which applies the iff.intro constructor theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split, sorry, -- prove p ∨ q → q ∨ p sorry, -- prove q ∨ p → p ∨ q end In the ﬁrst direction, we can introduce the hypothesis p ∨ q using the intro tactic: 14 theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split, intro hpq, sorry, -- prove q ∨ p with hpq : p ∨ q sorry, -- prove q ∨ p → p ∨ q end Now, we can consider both cases p and q of the hypothesis p ∨ q using the cases tactic: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split, intro hpq, cases hpq with hp hq, sorry, -- prove q ∨ p with hp : p sorry, -- prove q ∨ p with hq : q sorry, -- prove q ∨ p → p ∨ q end What we want to do now is apply the or.inr and or.inl tactics to create an instance of the goal. We do so using the apply tactic: 15 theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split, intro hpq, cases hpq with hp hq, apply or.inr, sorry, -- prove q ∨ p with hp : p apply or.inl, sorry, -- prove q ∨ p with hq : q sorry, -- prove q ∨ p → p ∨ q end Now, we can close the goals with our exact hypotheses using the exact tactic: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split, intro hpq, cases hpq with hp hq, apply or.inr, exact hp, -- done! apply or.inl, exact hq, -- done! sorry, -- prove q ∨ p → p ∨ q end We can notice something about the other direction: the exact same tactic proof will work for it (although the hypothesis naming will be off)! Rather than copying it over, however, we can use the “;” operator, which, if used instead of a comma, causes the next tactic to apply to all of the 16 goals produced by this tactic. To make our list of tactics look like a single one to Lean, we can “group” it by putting curly braces around it. Lastly, we can replace the exact ... with the tactic assumption, which will ﬁnd an assumption matching the goal type and apply it for us. This lets us avoid having to name the hypotheses produced by cases, for the sleek proof: theorem or_comm_iff {p : Prop} {q : Prop} : p ∨ q ↔ q ∨ p := begin split; { intro h, cases h, apply or.inr, assumption, apply or.inl, assumption } end This chapter was meant to only cover the very basics of Lean (to give a “feel” for how theorem proving is done) and leaves many important topics undiscussed (e.g. type classes, exten- sionality, quotients, computability, proof irrelevance, coercion, tactic mode, metaprogramming, and others). The rest of this report is a mathematical discussion that includes Lean snippets (mostly deﬁnitions) and references to mathlib code. Understanding this code isn’t necessary for the math, it’s just there if you would like to go a bit deeper into the formalization. If so, you should certainly learn more about Lean on your own (see the book “Theorem Proving in Lean” [12]) and try out theorem proving for yourself (see the “Natural Number Game” [13]). 17 Chapter 2 Measure-Theoretic Foundations The roots of our formalization lie in measure theory, which relates to the question of how “sets of things” should be measured. It begins with the deﬁnition of an “outer measure” that establishes some basic, universal rules of how such a function should behave, which is extended to the concept of a “measure” that allows us to impose more speciﬁc rules on a “measurable space” of sets. 2.1 The Event Space We start with a deﬁnition of a type, whose sets we want to assign measure to. We will refer to this type as α. In the language of probability theory, one can think of this as the fundamental “event space”, where each “trial” yields a particular value from this space. We care to deﬁne some kind of “measure” on this event space, where we can get a particular probability value given any set from α. In this context, we can immediately formulate the concept of a random variable. It can be thought of simply as a transformation on the event space. That is, for a second type β, we can deﬁne the random variable X : α → β. Now, if we have some kind of measure function on α, we can think of deﬁning a new measure function on β corresponding to X such that for any set s : β, 18 the measure of s is the measure under α of the preimage of s from X (in notation, the measure of X ′−1(s)). We will formalize this idea in the next chapter. 2.2 The Outer Measure The most fundamental restrictions on a measure function come from the requirements of an “outer measure”, which state that, for a function m : set α → R+∞ to be an outer measure (where R+∞ is the non-negative real numbers including inﬁnity), we have the empty set property, the monotonicity property, and the countable union property: m( /0) = 0, (2.1) ∀ A, B : set α, A ⊆ B =⇒ m(A) ≤ m(B), (2.2) ∀ f : N → set α, m ( ∞⋃ i=0 f (i) ) ≤ ∞ ∑ i=0 m( f (i)). (2.3) In Lean, this deﬁnition is: structure outer_measure (α : Type*) := (measure_of : set α → R≥0∞) (empty : measure_of /0 = 0) (mono : ∀{s1 s2}, s1 ⊆ s2 → measure_of s1 ≤ measure_of s2) (Union_nat : ∀(s:N → set α), measure_of ⋃ (i, s i) ≤ Σ’i, measure_of (s i)) This reﬂects some of our intuition on how sets should be measured. The empty set property represents the notion that an empty set should have no mass, the monotonicity property guarantees that a set has no less mass than any subset of itself, and the countable union property “places a limit” on the monotonicity property in that a set cannot acquire any more mass than that which is present from any covering set of subsets. 19 Why Countable? One subtle but important aspect of the countable union property is that f is a function from the natural numbers to sets. In other words, it only applies to lists of sets, as opposed to the strictly more general notion of sets of sets. In this case, the indexed union would be replaced by a set union (supremum), and the indexed sum would be replaced by a set sum: ∀ S : set (set α), m ( ⋃ s ∈ S s ) ≤ ∑ s ∈ S m(s). The reason that we instead restrict our requirement to countable collections of sets comes down to the distinction between continuous and discrete event spaces. Consider, for example, the case where α = [0, 1] ⊆ R (that is, α is the subtype of R equal to the unit interval). In the context of probabilities, we can consider deﬁning an outer measure m corresponding to a uniform distribution on this real interval, where ∀x, y, x ≤ y =⇒ m([x, y]) = m((x, y]) = m([x, y)) = m((x, y)) = y − x. With this outer measure, we have, as one would expect in probability, that the measure of any singleton set m({x}) = 0, with this extending to any countable union of such singletons. We can now see an issue if we were to instead use the more general set union property above: this reasonable deﬁnition of a uniform measure would not be an outer measure! For example, suppose we have x, y such that x < y. Then, if we let S = {{a} | a ∈ [x, y]}, we have m ( ⋃ s ∈ S s ) = m ([x, y]) = y − x ̸≤ ∑ s ∈ S m(s) = 0, violating our alternate union property. With the original countable union property, however, this would be an outer measure, because there is no list f : N → set [0, 1] consisting of all singletons that unions to a set of non-zero measure (because of the fact that real intervals are not countable). 20 2.2.1 Outer Measure Transformations From Functions Given an arbitrary function m0 : set α → R+∞ such that m0( /0) = 0 (satisfying (2.1)), we may want to create an outer measure that is “based on” it in some sense. One notion would be to ﬁnd the maximum outer measure m such that ∀ s : set α, m(s) ≤ m0(s). (2.4) Clearly, the outer measure that is identically zero on all sets satisﬁes this requirement, However, does a maximum exist at all, and if so, how can we construct it? It turns out that if we let fromfunc(m0)(s) = inf f :N→set α s ⊆ ∞⋃ i=0 f (i) =⇒ ∞ ∑ i=0 m0( f (i)) (where the antecedent in the inﬁmum effectively makes the value ∞ when it is not satisﬁed for some f ), then not only is fromfunc(m0) a valid outer measure, but it is also the unique maximum outer measure satisfying (2.4), as we can show that no other such outer measure can exceed it on any set. In Lean, we have: def of_function (m : set α → R≥0∞) (m_empty : m /0 = 0) : outer_measure α := let µ := λs, ⊓ {f : N → set α} (h : s ⊆ ⋃ i, f i), Σ’i, m (f i) in empty := le_antisymm (infi_le_of_le (λ_, /0) (infi_le_of_le (empty_subset _) (by simp [m_empty]))) (zero_le _), mono := assume s1 s2 hs, infi_le_infi $ assume f, infi_le_infi2 $ assume hb, ⟨subset.trans hs hb, le_refl _⟩, Union_nat := . . . } 21 where the local µ is deﬁned as above. From Partial Functions If we’re instead given a partial set function on the subtype S : set (set α), where the function m0 : S → R+∞ is only deﬁned on S, we can ﬁrst “extend” this function to the full set type by deﬁning extend(m0)(s) =  || || m0(s) if s ∈ S, ∞ otherwise, and then use fromfunc(extend(m0)) to get an outer measure. In Lean: def extend {S : set α → Prop} (m : Π (s : set α), S s → R≥0∞) (s : α) : R≥0∞ := ⊓ (h : S s), m s h Here, using the proposition S s as an index for the indexed inﬁmum makes the value of the inﬁmum inﬁnity when S s is not true (because it will be an “empty” index), so this is equivalent to the above deﬁnition. Mapping/Co-Mapping an Outer Measure Given a second type β, a function f : α → β, and an outer measure m0 on α, we can construct a function map f (m0) such that, for all s : set β, map f (m0)(s) = m0( f ′−1(s)). That is, the measure of s under the map is the measure under m0 of the preimage of s on f . Similarly, given an outer measure m1 on β, we can construct a function comap f (m1) such that, for all s : set α, comap f (m1)(s) = m1( f ′′(s)). 22 That is, the measure of s under the co-map is the measure under m1 of the image of s on f . Both map f (m0) and comap f (m1) can be shown to be outer measures. In Lean we have the deﬁnitions: def map (f : α → β) (m : outer_measure α) : outer_measure β := { measure_of := λs, m (f −1’ s), empty := m.empty, mono := λ s t h, m.mono (preimage_mono h), Union_nat := λ s, by rw [preimage_Union]; exact m.Union_nat (λ i, f −1’ s i) } def comap (f : α → β) (m : outer_measure β) : outer_measure α := { measure_of := λ s, m (f ’’ s), empty := by simp, mono := λ s t h, m.mono (image_subset f h), Union_nat := λ s, by { rw [image_Union], apply m.Union_nat } Restricting an Outer Measure Given an outer measure m0 on α, we may want to “restrict” it to a t : set α such that for all s : set α, we have restrictt(m0)(s) = m0(s ∩ t), (2.5) that is, we measure of the part of s that intersects with t. The function restrictt(m0) can also be shown to be an outer measure. In Lean however, we can cleverly avoid the need to explicitly prove this fact. Consider the coercion function ct : t → α (from the subtype t to the full type α). We can replace the above deﬁnition with restrictt(m0) = mapct (comapct (m0)), 23 which, for any s : set α, equivalently reduces to: restrictt(m0)(s) = m0(c ′′ t (c ′−1 t (s))), where it’s easy to see that c′′ t (c ′−1 t (s)) = s ∩ t. Because map(·) and comap(·) preserve outer measures, we immediately know that restrictt(m0)(s) is an outer measure. 2.3 Measurable Spaces The outer measure requirements give us useful information about inequalities on a measure function (in addition to the trivial empty set property). However, we would also like to be able to say things about various equalities relating measures of sets on our event space. For example, for disjoint A, B : set α, we may want to be able to say for an outer measure m on α that m(A ∪ B) = m(A) + m(B), or more generally, if they’re not necessarily disjoint, that m(A ∪ B) = m(A \\ B) + m(A ∩ B) + m(B \\ A). However, imposing such equality requirements on measures of our event space can be quite taxing, especially when we have incomplete knowledge of what the measures of every single set should be. So, we would like to have some granularity of control on where such equality restrictions apply. This is where the concept of a “measurable space” (otherwise known as a “σ-algebra”) becomes useful. We deﬁne a measurable space as a set of \"measurable sets\" M : set (set α) with the properties of empty set measurability, complement closure, and countable union closure: /0 ∈ M, (2.6) ∀ s : set α, s ∈ M =⇒ s c ∈ M, (2.7) ∀ f : N → set α, (∀ i : N, f (i) ∈ M) =⇒ ∞⋃ i=0 f (i) ∈ M. (2.8) 24 In Lean we have: structure measurable_space (α : Type*) := (measurable_set’ : set α → Prop) (measurable_set_empty : measurable_set’ /0) (measurable_set_compl : ∀ s, measurable_set’ s → measurable_set’ (s)c) (measurable_set_Union : ∀ f : N → set α, (∀ i, measurable_set’ (f i)) → measurable_set’ ⋃ ( i, f i)) From these properties, we can derive other intuitive closures, including those of countable intersection: ∀ f : N → set α, ∀ i : N, f (i) ∈ M =⇒ ∞⋂ i=0 f (i) ∈ M, and difference: ∀ a, b : set α, a ∈ M =⇒ b ∈ M =⇒ a \\ b ∈ M. Why Countable (Again)? The reason for deﬁning closure only under countable unions, rather than unions over any arbitrary set of sets, again can be seen by considering the continuous case. As suggested above, the measurable sets intuitively correspond to sets that we “know” the measure of. In the case of measuring on a continuous event space (e.g. the reals), regardless of our knowledge of the mass of certain (non-trivial) intervals, we always know the mass of the singleton sets and any countable union of them – exactly zero. If we were to replace the countable union closure property with ∀ S : set (set α), (∀ s ∈ S, s ∈ M) =⇒ ⋃ s∈S s ∈ M, then for any measurable space M that contains all of the singleton sets (∀ x ∈ R, {x} ∈ M), we would immediately have that every possible set is measurable (because any set can be written as 25 the set union of the singleton sets of its constituents), and so we would lose our granularity in choosing measurable sets on the intervals. In only requiring closure on countable set unions, we “contain” the closure on these singletons, again because of the uncountability of the real intervals. 2.3.1 Transforming Measurable Spaces Generating Measurable Spaces Given any S : set (set α), we can consider its measurable space “closure”, i.e. the smallest measurable space including every set in S. In Lean, we can deﬁne this by creating an inductive type whose constructors mirror the outer measure requirements: inductive generate_measurable (s : set (set α)) : set α → Prop | basic : ∀ u ∈ s, generate_measurable u | empty : generate_measurable /0 | compl : ∀ s, generate_measurable s → generate_measurable cs | union : ∀ f : N → set α, (∀ n, generate_measurable (f n)) → generate_measurable ⋃ ( i, f i) def generate_from (s : set (set α)) : measurable_space α := { measurable_set’ := generate_measurable s, measurable_set_empty := generate_measurable.empty, measurable_set_compl := generate_measurable.compl, measurable_set_Union := generate_measurable.union } Here, we will write this generated measurable space as generate(S). Visually, we can think of the generated measurable space as originating in the “mosaic” of set intersections and differences created by S and the universal set (“univ”) on our event space. The set of measurable sets can be seen to consist of all possible countable unions of the “tiles” of this mosaic. Now, suppose we have some predicate C : set α → Prop. If we want to show that C 26 holds for all sets in generate(S), we can prove that it holds for S itself, for the empty set, under complement, and under countable union: ∀ s ∈ S, C(s), (2.9) C( /0), (2.10) ∀ s ∈ generate(S), C(s) =⇒ C(s c), (2.11) ∀ f : N → set α, (∀ i : N, f (i) ∈ generate(S) ∧C( f (i))) =⇒ C ( ∞⋃ i=0 f (i) ) . (2.12) Combining Measurable Spaces Given a set S of measurable spaces, we can now deﬁne their \"combination\" into the smallest measurable space that contains both of them (i.e. their supremum) as sup(S) = generate (⋃ s∈S s ) , the measurable space generation on the union (set supremum) of all of their sets of measurable sets. Mapping/Co-Mapping Measurable Spaces Similarly to what we did for outer measures, given a second type β, and a function f : α → β, we can deﬁne a notion of a map/comap on Mα with respect to s. Given a measurable space Mα on α, we deﬁne our map to be the set of all set β such that their preimage is a measurable set map f (Mα) = {b : set β | f ′−1(b) ∈ Mα}. 27 Given a measurable space Mβ on β, we deﬁne our comap to be the set of all set α such that they are the preimage of some measurable set: comap f (Mβ) = {a : set α | ∃ b : set β, f ′−1(b) = a} (you can also think of this as simply taking the preimages of every set in Mβ). The Lean deﬁnitions are: protected def map (f : α → β) (m : measurable_space α) : measurable_space β := { measurable_set’ := λ s, m.measurable_set’ (f −1’ s), measurable_set_empty := m.measurable_set_empty, measurable_set_compl := assume s hs, m.measurable_set_compl _ hs, measurable_set_Union := assume f hf, by { rw preimage_Union, exact m.measurable_set_Union _ hf }} protected def comap (f : α → β) (m : measurable_space β) : measurable_space α := { measurable_set’ := λ s, ∃s’, m.measurable_set’ s’ ∧ f −1’ s’ = s, measurable_set_empty := ⟨ /0, m.measurable_set_empty, rfl⟩, measurable_set_compl := assume s ⟨s’, h1, h2⟩, ⟨(s’)c, m.measurable_set_compl _ h1, h2 ▷ rfl⟩, measurable_set_Union := assume s hs, let ⟨s’, hs’⟩ := classical.axiom_of_choice hs in ⟨ ⋃ i, s’ i, m.measurable_set_Union _ (λ i, (hs’ i).left), by simp [hs’] ⟩ } 2.3.2 A Galois Connection One very useful abstraction in mathlib is what is known as the “Galois Connection”. Given types α and β and functions f : α → β and g : β → α, it can be thought of as a kind of 28 “joint monotonicity” property on f and g, requiring that ∀ a : α, b : β, f (a) ≤ b ⇐⇒ a ≤ g(b). Once you have established a Galois Connection between two functions, several useful properties arise (which include the monotonicity of both f and g). In Lean we have the deﬁnition: def galois_connection [preorder α] [preorder β] (l : α → β) (u : β → α) : Prop := ∀ a b, l a ≤ b ↔ a ≤ u b in which the preorder type classes ensure the existence of some ≤ relation on both spaces (and also bundles some standard properties along with this). In our case, we can show that, for any f : α → β, there is a Galois Connection between comap f (·) and map f (·) (where the ≤ relation on measurable spaces is the subset relation). To see this, consider an arbitrary measurable space Mb on β and Ma on α. Here, comap f (Mb) ⊆ Ma means that every set in Mb has a preimage in Ma, which is exactly the same statement as Mb ⊆ map f (Ma). In Lean we establish this fact with the lemmas: lemma comap_le_iff_le_map (m : measurable_space α) (m’ : measurable_space β) (f : α → β) : m’.comap f ≤ m ↔ m’ ≤ m.map f := ⟨assume h s hs, h _ ⟨_, hs, rfl⟩, assume h s ⟨t, ht, heq⟩, heq ▷ h _ ht⟩ lemma gc_comap_map (f : α → β) : galois_connection (measurable_space.comap f) (measurable_space.map f) := assume m m’, comap_le_iff_le_map m m’ f Among the useful properties that the Galois Connection gives us is the fact that, for any list of measurable spaces fb on β, we have that the comap of the indexed supremum of fb is the indexed supremum of the comap of each measurable space (regardless of whether the index type is countable): comap f ( sup i∈ι ( fb(i)) ) = sup i∈ι (comap f ( fb(i))). (2.13) 29 In Lean, we use the general property galois_connection.l_supr to prove this: lemma comap_supr (f : β → α) (m : ι → measurable_space α) : ⊔(i, m i).comap f = ⊔(i, (m i).comap f) := (gc_comap_map f).l_supr 2.3.3 The Trimmed Outer Measure As mentioned before, measurable sets correspond to sets which we “know” the measure of. So, if we only know the value of an potential outer measure on the measurable sets, in order to be able to actually deﬁne our outer measure completely, we want to have some “standard” way to generate values for non-measurable sets (based on our knowledge of measurable sets) so as to satisfy the requirements to be an outer measure. Suppose we have some measurable space M on α and some partial set function m0 deﬁned on M. Also assume that m0 satisﬁes the properties for an outer measure ((2.1), (2.2), (2.3)) restricted to sets from M. We can consider the outer measure given by induce(m0) = fromfunc(extend(m0)). In Lean: def induced_outer_measure {P : set α → Prop} (m : Π (s : set α), P s → R≥0∞) (P0 : P /0) (m0 : m /0 P0 = 0) : outer_measure α := outer_measure.of_function (extend m) (extend_empty P0 m0) where extend_empty shows that the extended m gives zero measure to the empty set. It turns out that with such a function, we have the property: ∀ s : set α, induce(m0)(s) = inf t:set α s ⊆ t ∧ t ∈ M =⇒ m0(t), 30 in other words, the measurable sets retain their value from the partial function (by the monotonicity property) and each non-measurable set gets the inﬁmum of the values of all of its measurable supersets (see Figure 2.1). This gives us exactly the outer measure construction principle we were seeking! As a matter of fact, a measurable superset matching exactly matching this inﬁmum is guaranteed to exist: ∀ s : set α, ∃ t : set α, s ⊆ t ∧ t ∈ M ∧ induce(m0)(s) = m0(t). The proof of this boils down to the fact that we can construct a list f : N → set α of measurable sets that “narrows down” to induce(m0)(s) in the sense that ∀ i : N, m0( f (i)) < induce(m0)(s) + i−1. Now, if we’re given an arbitrary outer measure m and a measurable set M, how can we check whether m was constructed as above? This is where the notion of a “trimmed” outer measure becomes useful. Let subM(m) be m restricted to the domain M. Deﬁning trimM(m) = induce(subM(m)), we can now check whether trimM(m) = m, that is, whether trimM assigns the same values to non-measurable sets that we originally had in m. In Lean we deﬁne: def trim [M : measurable_space α] (m : outer_measure) : outer_measure α := induced_outer_measure (λ s (hs : M.measurable_set’ s), m s) measurable_set.empty m.empty where (λ s (hs : M.measurable_set’ s), m s) is exactly our subM(m). 31 A B s1 s2 s3 s4 s5 Figure 2.1: Inducing an outer measure from a partial function. Suppose we have the measurable space M = generate({A, B}) represented above, with a partial function f : M → R+∞ satisfying (2.1), (2.2), (2.3) on its domain. Let’s induce an outer measure m = induce( f ). In this case, each of the non-measurable sets shown would take the measure of the smallest measurable superset: m(s1) = f (A∪B), m(s2) = f (A∩B), m(s3) = f (A\\B), m(s4) = f (Ac), and m(s5) = f ((A∪B)c). 2.3.4 Measurable Spaces on Pi Types Suppose we have an index of types β : ι → Type, with some M : Π (i : ι), set (set β(i)), a mapping from type indices to a measurable space for that type. Now, consider the function space Π (i : ι), β(i), i.e. the type of dependent functions from indices to their corresponding types (as we will see later, this will represent indexed random variables, which can be deﬁned over different types). How could we deﬁne a measurable space on this function space “corresponding” to M in some sense? Let’s start by deﬁning a “function evaluation” function. Given any f : Π (i : ι), β(i), let eval(i)( f ) = f (i) – so, this simply applies the function at the given index, which initially may not seem very useful. However, now consider the preimage of some s : set β(i), eval(i)′−1(s). This consists of all of the functions whose i-value is within s. So, if we take comapeval(i)(M(i)), we end up with the set of all such preimages. If we do this for all i, we have a measurable space on our function space corresponding to each M(i), and we can combine them by using the supremum operation on measurable spaces that we had 32 previously deﬁned, giving us the following deﬁnition of a “pi measurable space”: pi_MS(M) = sup i:ι ( comapeval(i)(M(i)) ) . (2.14) And in Lean: instance measurable_space.pi {δ : Type*} {π : δ → Type*} [m : Π a, measurable_space (π a)] : measurable_space (Π a, π a) := ⊔ (a : δ), (m a).comap (λ (b : Π (i : δ), π i), b a) (see Figure 2.2). Here, the lambda abstraction (λ (b : Π (i : δ), π i), b a) is exactly our eval(i) function. 2.4 Deﬁning a Measure We now have all the deﬁnitions we need to deﬁne an outer measure that corresponds to the concept of probability, allowing us to derive useful equalities based on a measurable space. For any f : N → set α, let PD( f ) indicate that f is pairwise disjoint, that is, the sets f maps to with any distinct pair of indices are disjoint. Given a measurable space M we deﬁne a “measure” on M as an outer measure m that additionally satisﬁes the properties of disjoint measurable set countable union and trim invariance: ∀ f : N → set α, (∀ i : N, f (i) ∈ M) ∧ (PD( f )) =⇒ m ( ∞⋃ i=0 f (i) ) = ∞ ∑ i=0 m( f (i)), (2.15) trimM(m) = m. (2.16) The disjoint measurable set countable union property makes this measure correspond to probability as we know it in that if you have any list of disjoint measurable sets, you can write the measure of their union as the sum of their individual measures. This lets us derive many different useful 33 γ Mγ f (0) : γ f (1) : δ comapeval(0)(Mγ) δ Mδ f (0) : γ f (1) : δ comapeval(1)(Mδ) f (0) : γ f (1) : δ sup ( comapeval(0)(Mγ), comapeval(1)(Mδ) ) Figure 2.2: Generating a Pi measurable space. Suppose our index type is the binary subtype of the natural numbers, ι = {0, 1}, with β(0) = γ and β(1) = δ. Pictured ﬁrst are Mγ and Mδ, measurable spaces on the types γ and δ. With the transformation comapeval(·)(·), these are transformed into measurable spaces on Π (i : ι), β(i). This is essentially the same measurable space in Pi form, where the other indices are allowed to take all other values (hence the gray bars). Then, we take the supremum of these measurable spaces to get a Pi measurable space. The diagrams should be read like this: select a measurable set for each of the indices (just 0 and 1 in this case), and consider all of the functions f : Π (i : ι), β(i) whose values fall into the corresponding set at each index – these make up a Pi measurable set. equalities from the mosaic of our measurable space. The trim invariance property “normalizes” the outer measure by ﬁxing the measures on non-measurable sets based on measurable ones as explained earlier. It’s not necessary for our results (the ﬁrst property is already enough), but including it greatly simpliﬁes proofs regarding measure transformations, which will become clear later. In Lean this deﬁnition looks like: 34 structure measure (α : Type*) [measurable_space α] extends outer_measure α := (m_Union {f : N → set α} : (∀ i, measurable_set (f i)) → pairwise (disjoint on f) → measure_of ⋃ ( i, f i) = Σ’ i, measure_of (f i)) (trimmed : to_outer_measure.trim = to_outer_measure) (here, the measure_of ﬁeld is derived from the parent structure outer_measure). Why Countable (Again)? We again ﬁnd a countability criterion in this deﬁnition, and, indeed here as well it has to do with allowing for certain situations in the continuous case. As with the outer measure, on real distributions, for example, we would like to assign zero measure to the singletons, while being able to have non-zero measure on the non-trivial real intervals. Using instead the more general set union, however, would make such a measure impossible as it would violate the altered criterion for a set of singletons that union to a non-trivial real interval with positive measure. In Lean: class is_probability_measure (µ : measure α) : Prop := (measure_univ : µ univ = 1) The Probability Measure The ﬁnal criterion an outer measure needs to satisfy to correspond to a notion of probability is simply the requirement that it gives measure 1 to the whole event space. We refer to a measure µ on event space α such that µ(univ : set α) = 1 as a “probability measure”. More generally, probability measures are a speciﬁc type of “ﬁnite measure” satisfying µ(univ) < ∞. 35 2.5 Measure Transformations Now that we’ve deﬁned measures, we would like to talk about ways to transform one measure into another. Because measures are outer measures that satisfy speciﬁc additional criteria, transforming measures fundamentally relates to transforming outer measures. For this we can consider “lifting” outer measure transformations into the context of measures. We will be particularly interested in doing this for the restrict(·) and map(·) transforma- tions deﬁned above. Given a measure, simply applying an outer measure transformation may not always result in an outer measure also satisfying the requirements to be considered a measure. So, we will need some additional transformations that, when composed with the ones above, allow us to preserve the behavior of the original transformation at least on the measurable sets while also ensuring that the resulting outer measure is also a measure. 2.5.1 From a Partial Function Suppose we have some measurable space M on α and some partial set function m0 deﬁned on M which satisﬁes (2.1) and (2.15). Then, we can deﬁne a measure µ = induce(m0) which can easily be shown to both preserve the measure of m0 on M (satisfying (2.15)) and satisfy (2.16). In Lean this is the function of_measurable: def of_measurable (m : Π (s : set α), measurable_set s → R≥0∞) (m0 : m /0 measurable_set.empty = 0) (mU : ∀ {{f : N → set α}} (h : ∀ i, measurable_set (f i)), pairwise (disjoint on f) → m ⋃ ( i, f i) (measurable_set.Union h) = Σ’ i, m (f i) (h i)) : measure α := 36 { m_Union := λ f hf hd, show induced_outer_measure m _ m0 (Union f) = Σ’ i, induced_outer_measure m _ m0 (f i), begin rw [induced_outer_measure_eq m0 mU, mU hf hd], congr, funext n, rw induced_outer_measure_eq m0 mU end, trimmed := show (induced_outer_measure m _ m0).trim = induced_outer_measure m _ m0, begin unfold outer_measure.trim, congr, funext s hs, exact induced_outer_measure_eq m0 mU hs end, ..induced_outer_measure m _ m0 } From an Outer Measure? Given an outer measure m, we may want to deﬁne a measure from it. In light of the above, the obvious thing to do is µ = induce(subM(m)), (where, again, subM(m) creates a partial set function from m deﬁned only on M). However, we must keep in mind that in order for this to produce an actual measure, subM(m) must satisfy both (2.1) and (2.15). (2.1) will always be satisﬁed, because this is a property of an outer measure. So we would be left to show (2.15). This can of course be done for appropriate outer measures, but can we save ourselves some work in the proof by using the fact that we started with with an outer measure to simplify the requirement? It turns out that we can, using what’s known as the Carathéodory Criterion. 37 2.5.2 The Carathéodory Criterion Given a set function f : set α → R+∞ and a set s, s is referred to as “Carathéodory” (written cara f (s)) if ∀ t : set α, f (t) = f (t ∩ s) + f (t \\ s), that is, the measure of any set t can be decomposed as the measure of its intersection with s plus the measure of its difference with s. Now, consider an outer measure m and measurable space M. It can actually be shown that the set of all Carathéodory sets forms a measurable space. In Lean, we write this as outer_measure.caratheodory, or, for an outer measure m, we can use the dot notation m.caratheodory. More importantly, it can also be shown that ∀ s ∈ M, caram(s) (2.17) is enough to satisfy (2.15). In Lean, this is represented with the function from outer measures to measures: def outer_measure.to_measure (ms : measurable_space α) (m : outer_measure α) (h : ms ≤ m.caratheodory) : measure α := measure.of_measurable (λ s _, m s) m.empty (λ f hf hd, m.Union_eq_of_caratheodory (λ i, h _ (hf i)) hd) where the hypothesis (h : ms ≤ m.caratheodory) is exactly (2.17) above. Deriving The Disjointedness Condition To get a feel for this fact, consider an s : set α such that caram(s) and t such that s and t are disjoint. Now, consider the measure of their union m(s ∪ t). Because (s ∪ t) \\ s = t (by disjointedness) and (s ∪ t) ∩ s = s, we know from caram(s) that m(s ∪ t) = m(s) + m(t). 38 Now, we can extend this idea to arbitrary unions on any ﬁnite list of elements. Speciﬁcally, for any list S : N → set α such that PD(S), if we have that ∀ i : N, caram(S(i)), we can show that for all n : N, m (⋃ i<n S(i) ) = ∑ i<n m(S(i)) using the above and induction on n. However, we’re still short of (2.15), because it applies to inﬁnite lists. But thanks to the fact that m is an outer measure, we have (2.2) and (2.3), which we can show allows us to extend to this case as well (in Lean, this is outer_measure.Union_eq_of_caratheodory). We now have an easier way to show that induce(subM(m)) is a measure, as we can prove (2.17) instead of (2.15). A Measure’s Measurable Sets are Carathéodory An interesting fact arises from (2.16): we have, for any measure µ, ∀ s ∈ M, caraµ(s). (2.18) In Lean, we have the lemma lemma le_to_outer_measure_caratheodory (ms : measurable_space α) (µ : measure α) : ms ≤ µ.to_outer_measure.caratheodory := . . . (where µ implicitly uses ms as its measurable space). The usefulness of this property will become clear when we discuss transformations on measures. 2.5.3 Lifting an Outer Measure Transformation Suppose we have types α and β with measurable spaces Mα and Mβ, and a function f from outer measures on α to outer measures on β. The question now is whether f is also a map between measures. Well, if it isn’t already is a map between measures, we could use the function 39 mentioned earlier in the deﬁnition mtrans f (µ) = induce(subMβ( f (µ))) in an attempt to make it so (if f is already a map between measures, then we can still use this function, because induce(subMβ(·)) will have no effect on a measure via (2.16), since this is exactly trimMβ(·)). However, we need to be sure that we satisfy (2.15) for every f (µ). Luckily, since we now have (2.17), we can instead prove the simpler criterion: ∀ µ, ∀ s ∈ Mβ, cara f (µ)(s), (2.19) i.e. all measures map via f to outer measures that include all of Mb as Carathéodory sets. In Lean, our mtrans is referred to as lift_linear. This is because it really is deﬁned as a bundled “linear map” (a function with some additional properties), but for our purposes we can take the deﬁnition to be: def lift_linear {ma : measurable_space α} {mb : measurable_space β} (f : outer_measure α → outer_measure β) (hf : ∀ µ : measure α, mb ≤ (f µ).caratheodory) : measure α → measure β := λ µ, (f µ).to_measure (hf µ) Just being able to map from a measure to another is not useful by itself, however. The important point about this transformation (assuming (2.19) is satisﬁed) is that all measurable sets in Mβ are guaranteed to preserve their values from the underlying outer measure transformation: ∀ s : set β, s ∈ Mβ =⇒ mtrans f (µ)(s) = f (µ)(s). (2.20) 40 2.5.4 Restricting a Measure We now come to our ﬁrst important measure transformation: restriction. For any measure µ and set s : set α, we can simply deﬁne this transformation as mrestricts(µ) = mtransrestricts(µ). For this to be a mapping between measures, we’re now left to show (2.19). Suppose we have some measure µ and some t ∈ Mα. We want to show cararestricts(µ)(t), i.e. for all u : set α, restricts(µ)(u) = restricts(µ)(u ∩ t) + restricts(µ)(u \\ t) = µ((u ∩ t) ∩ s) + µ((u \\ t) ∩ s) = µ((u ∩ s) ∩ t) + µ((u ∩ s) \\ t), which we know is true because (2.18) implies that t is Carathéodory on µ. So, this is a valid mapping between measures and we immediately have via (2.20) the property that, for all t ∈ Mα: mrestricts(µ)(t) = restricts(µ)(t) = µ(t ∩ s). (2.21) In Lean: def restrict {m0 : measurable_space α} (s : set α) : measure α → measure α := lift_linear (outer_measure.restrict s) (λ µ s’ hs’ t, begin suffices : µ (s ∩ t) = µ (s ∩ t ∩ s’) + µ (s ∩ t \\ s’), { simpa [← set.inter_assoc, set.inter_comm _ s, ← inter_diff_assoc] }, exact le_to_outer_measure_caratheodory _ _ hs’ _, end) 41 Alternative Application Principles The equality (2.21) can in fact be extended beyond the measurable sets in certain cases. The ﬁrst are the “null-measurable sets”. To understand this deﬁnition, we ﬁrst need to deﬁne the concept of “almost-everywhere equal” sets. We deﬁne, for a measure µ and two sets s and t, s aeµ = t ⇐⇒ µ ((s \\ t) ∪ (t \\ s)) = 0, that is, they differ on a set of measure 0. We deﬁne null-measurability of t as: nullmµ(t) ⇐⇒ ∃ t′ ∈ Mα, t aeµ = t′, that is to say, there exists some measurable set that is almost-everywhere equal to s. It can be show that null-measurability with respect to mrestricts(µ) is also sufﬁcient to show (2.21). The second alternative case where (2.21) applies is when the restricting set s itself is measurable. In this case, we can in fact show that (2.21) applies to every possible set. The proof of this boils down to the fact that in this case, trimming the restricted outer measure has no effect: mrestricts(µ) = induce(subM(restricts(µ))) = restricts(µ). 2.5.5 Mapping a Measure For types α and β with measurable spaces Mα and Mβ, and a function f : α → β, we would now like to consider lifting our second transformation, map f (·) (from outer measures on α to outer measures on β). However, here it’s not as simple as doing mtransmap f (µ). As we will show, in this case, whether or not this is a valid measure map depends on the additional criteria of f being a “measurable function”. 42 Measurable Functions We deﬁne measurability on functions as the requirement that every measurable set in the output space has a preimage that is a measurable set in the input space: measurable( f ) ⇐⇒ ∀ s ∈ Mβ, f ′−1(s) ∈ Mα. In Lean, we have the proposition def measurable [measurable_space α] [measurable_space β] (f : α → β) : Prop := ∀ {t : set β}, measurable_set t → measurable_set (f −1’ t) We will show that this condition is sufﬁcient for mtransmap f (µ) to be a measure map, that is, to satisfy (2.19). Suppose we have some measure µ and some s ∈ Mβ. We want to show caramap f (µ)(s), i.e. for all t : set α, map f (µ)(u) = map f (µ)(u ∩ s) + map f (µ)(u \\ s), which becomes µ( f ′−1(u)) = µ( f ′−1(u ∩ s)) + µ( f ′−1(u \\ s)) = µ( f ′−1(u) ∩ f ′−1(s)) + µ( f ′−1(u) \\ f ′−1(s)). Since f is measurable, we know that f ′−1(u) is a measurable set, which implies via (2.18) that it is also Carathéodory on µ, so the above is true. 43 Deﬁning the Measure Map But we’re still left with the question: what if f is not a measurable function? In that case, we aren’t guaranteed that mtransmap f (µ) is a measure, so in order to be sure that we have a map between measures, we need to handle this case specially. So, we use the deﬁnition mmap f (µ) =  || || mtransmap f (µ) if measurable( f ), 0 otherwise, where 0 here denotes the measure that is zero on all sets. In Lean, we deﬁne this measure map as: def map [measurable_space α] [measurable_space β] (f : α → β) : measure α → measure β := if hf : measurable f then lift_linear (outer_measure.map f) (λ µ s hs t, le_to_outer_measure_caratheodory µ _ (hf hs) (f −1’ t)) else 0 This deﬁnition means that we need to additionally qualify our application rule with measurable( f ) – for any measurable function f and s ∈ Mβ, mmap f (µ)(s) = µ( f ′−1(s)): ∀ f : α → β, measurable( f ) =⇒ ∀ s ∈ Mβ, mmap f (µ)(s) = µ( f ′−1(s)) (2.22) (again via (2.20)). Measurable Functions to Pi Types Suppose we have an index of types β : ι → Type, with some M : Π (i : ι), set (set β(i)), a mapping from type indices to a measurable space for that type. Also suppose we have a type α with measurable space Mα and a list of functions F : Π (i : ι), α → β(i). Now, consider the 44 “index evaluation” function Feval(F) : α → Π (i : ι), β(i) such that: ∀ a : α, i : ι, Feval(F)(a)(i) = F(i)(a). that is, Feval(F) maps from α to the Pi type from i : ι to β(i) that gets the value of F at that index evaluated at a (as we will see in the next chapter, F corresponds to a list of random variables and Feval(F) “evaluates” that list on a certain event space outcome). Since we know via (2.14) that we have a measurable space on Π (i : ι), β(i) induced by M, we can ask the question: is Feval(F) measurable? There is in fact a simple criteria for determining this: measurable(Feval(F)) ⇐⇒ ∀ i : ι, measurable(F(i)) (2.23) (in terms of random variables, the index evaluator is measurable if and only if each of the individual random variables are measurable themselves). This can be proven with the help of (2.13), and we have the Lean lemma: lemma measurable_pi_iff {g : α → Π a, π a} : measurable g ↔ ∀ a, measurable (λ x, g x a) := by simp_rw [measurable_iff_comap_le, measurable_space.pi, measurable_space.comap_supr, measurable_space.comap_comp, function.comp, supr_le_iff] This function takes in g, which you can see is already the type we get from Feval. However, because it is implicit (the curly braces), this lambda can be inferred from F and we don’t need to convert it directly (using the abstraction (λ a i, F i a)). 45 Chapter 3 Probability Theory We now have all of the foundational tooling we need to deﬁne several standard concepts in probability theory. We begin with different kinds of deﬁnitions of independence of events and their properties, which leads us to an equivalent, but more powerful deﬁnition of the standard concept of event independence. We then move on to a deﬁnition of what it means to “condition” a measure on an event in terms of a scaled restriction transformation, which allows us to derive many standard Bayesian results, including Bayes’ Theorem. This also allows us to come up with a ﬁrst deﬁnition of conditional independence. After this, we deﬁne joint and marginal distributions on random variables, and deﬁne what it means for random variables to be independent and conditionally independent, extending our previous theorems into this new context. 3.1 Independence Given a measure µ on α and two events from our event space a, b : set α, we can deﬁne a primitive notion of independence by simply requiring that the measure of their intersection is the product of their measures: µ(a ∩ b) = µ(a)µ(b). (3.1) 46 While useful, this deﬁnition is limiting, because we would like to speak more generally about independence between two sets of events. We will see that such a deﬁnition allows us to come up with a deﬁnition of independence between events that is equivalent, but more informative, than our original one. 3.1.1 Deﬁnitions Independence of Sets of Sets Given two sets of events A, B : set (set α), we deﬁne independence of A and B under µ as: indep_setsµ(A, B) ⇐⇒ ∀ a ∈ A, b ∈ B, µ(a ∩ b) = µ(a)µ(b), that is, for any pair of events from A and B, the measure of their intersection is the product of their measures. In Lean, this deﬁnition looks almost the same: def indep_sets {α} [measurable_space α] (s1 s2 : set (set α)) (µ : measure α) : Prop := ∀ t1 t2 : set α, t1 ∈ s1 → t2 ∈ s2 → µ (t1 ∩ t2) = µ t1 * µ t2 This is the most general formulation of independence. For some a, b : set α, we can immediately see that (3.1) is equivalent to indep_setsµ({a}, {b}). However, as we will soon see, we won’t actually be using this deﬁnition directly. Independence of Measurable Spaces Given two measurable spaces MA, MB : set (set α), we deﬁne independence of MA and MB under µ as: indepµ(MA, MB) ⇐⇒ indep_setsµ(MA, MB), 47 simply the independence of their sets as previously deﬁned. This will be the most frequent deﬁnition that we use, since the sets of events whose independence we care about are almost always also measurable spaces. Independence of Sets Given two sets a, b : set α, we deﬁne independence between them under µ as: indep_setµ(a, b) ⇐⇒ indepµ(generate({a}), generate({b})), independence of their generated measurable spaces. Again in Lean we have a very similar-looking deﬁnition: def indep_set {α} [measurable_space α] (s t : set α) (µ : measure α) : Prop := indep (generate_from {s}) (generate_from {t}) µ Note that for any set s : set α, generate({s}) = { /0, s, sc, univ} exactly. It’s perhaps immediately obvious by cases that this deﬁnition is equivalent to (3.1), but we will soon show that this fact arises from a more general result regarding independence on the measurable spaces generated by a pair of independent “π-systems”. 3.1.2 Properties Dynkin Systems A “Dynkin system”, also known as a “λ-system”, is a generalization of a measurable spaces that enables a powerful induction principle in certain cases. We deﬁne a Dynkin system as a set of sets M : set (set α) with the properties of empty set measurability, complement closure, 48 and disjoint countable union closure: /0 ∈ M, (3.2) ∀ s : set α, s ∈ M =⇒ sc ∈ M, (3.3) ∀ f : N → set α, ∀ i : N, f (i) ∈ M ∧ PD( f ) =⇒ ∞⋃ i=0 f (i) ∈ M. (3.4) The disjointedness condition in (3.4) is the only difference from the deﬁnition of measurable spaces (see (2.7)). Given any S : set (set α), we can consider its Dynkin system “closure”, i.e. the smallest Dynkin system including every set in S. We deﬁne this closure similarly to how we did for measurable spaces with generate(S), modifying the last construction rule to instead union only over disjoint sets. We will denote this generator with generateD(S). Now, suppose we have some predicate C : set α → Prop. If we want to show that C holds for all sets in generateD(S), we have to prove that it holds for S itself, for the empty set, under complement, and under disjoint countable union: ∀ s ∈ S, C(s), (3.5) C( /0), (3.6) ∀ s ∈ generateD(S), C(s) =⇒ C(sc), (3.7) ∀ f : N → set α, (∀ i : N, f (i) ∈ generateD(S) ∧C( f (i))) ∧ PD( f ) =⇒ C ( ∞⋃ i=0 f (i) ) . (3.8) One important fact about Dynkin systems is that a Dynkin system that is closed under intersection must also be a measurable space. The gist of the proof is that the intersection closure allows us to derive difference closure, which in turn allows us to show that any list of sets in the Dynkin system can be decomposed into a disjoint list of sets also in the Dynkin system whose union is 49 the same. Another important fact is that, for some Dynkin system D, and some s ∈ D, the set {t | t ∩ s ∈ D} is also a Dynkin system. Clearly, the extra disjointedness condition could be useful in proving certain properties on measurable spaces. However, we need to know under which circumstances it can apply – that is, when can we equate a generated measurable space with a generated Dynkin system so that we can make use of it? This is where we ﬁnd a use for what are known as “π-systems”. π-systems and the π-λ Theorem We deﬁne, for a set S : set (set α), the “π-system” criterion as: PI(S) ⇐⇒ ∀ s, t ∈ S, s ∩ t ̸= /0 =⇒ s ∩ t ∈ S, simply the requirement that S is closed under non-empty intersection. The non-empty requirement is not really necessary for our results, but including it allows us to be a bit more general. One immediately obvious fact is that singleton sets are π-systems: ∀ s : set α, PI({s}). (3.9) At this point, we can in fact show that, if PI(S), the generated Dynkin system generateD(S) is closed under intersection, and hence is also a measurable space. Additionally, we have generateD(S) = generate(S), 50 that is, the generated measurable space and Dynkin systems are identical. This result is known as Dynkin’s π-λ theorem. We now have the criterion we need for a new induction principle on generated measurable spaces: If they are generated by a π-system, then we can replace (2.12) with (∀ i : N, f (i) ∈ generate(S) ∧C( f (i))) ∧ PD( f ) =⇒ C ( ∞⋃ i=0 f (i) ) , (3.10) where we now have the new hypothesis PD( f ) to help us in our proof. In Lean this induction principle looks like: theorem induction_on_inter {C : set α → Prop} {s : set (set α)} [m : measurable_space α] (h_eq : m = generate_from s) (h_inter : is_pi_system s) (h_empty : C /0) (h_basic : ∀ t ∈ s, C t) (h_compl : ∀ t, measurable_set t → C t → C (t)c) (h_union : ∀ f : N → set α, pairwise (disjoint on f) → (∀ i, measurable_set (f i)) → (∀ i, C (f i)) → C ( ⋃ i, f i)) : ∀ {t}, measurable_set t → C t := . . . where the h_inter hypothesis and the pairwise (disjoint on f) condition in h_union is essentially the only difference between this and the auto-generated generate_measurable.rec. The Axiomatic Deﬁnition of Independence With the help of (3.10) we can prove that, ∀ S, T : set (set α), PI(S) ∧ PI(T ) =⇒ indep_setsµ(S, T ) ⇐⇒ indepµ(generate(S), generate(T )), (3.11) in English, the independence of π-systems implies the independence of the measurable spaces they generate. We can now simplify our criterion for independence of sets: from (3.11), and the 51 fact that singletons are π-systems, we have ∀ s,t : set α, indep_setµ(s,t) ⇐⇒ indep_setsµ({s}, {t}) ⇐⇒ µ(s ∩ t) = µ(s)µ(t), (3.12) So, we now have license to use the “bundled” indep_setµ(s,t) in place of our classical deﬁnition of independence (again, the equivalence can easily be seen by cases, but it’s good to know the general result). 3.2 Conditional Probability Given a measure µ representing a probability measure on a measurable space M and a “conditioning” set c : set α, we would like to formalize the deﬁnition of a conditional probability measure of µ given c. Conceptually, given some set s, we would like to get the measure of s restricted to c, that is, µ(s ∩ c), additionally scaled by µ(c)−1 to give s itself the full measure of 1. We have exactly the transformations we need for this, and can use the deﬁnition: µ|c = µ(c) −1 · mrestrictc(µ), (where · is the measure transformation of scalar multiplication). We will use the notation µ|c to indicate a measure that has been transformed by condition- ing on c. For application we will use the notation µ(s | c) = µ|c(s), which is more familiar in the context of probability theory. We immediately have the property 52 that we want via (2.21): for all s ∈ M, we have µ(s | c) = µ(c) −1 × µ(s ∩ c). (3.13) In Lean, the deﬁnition and application rule look like: def cond (µ : measure α) (s : set α) : measure α := (µ s)−1 · µ.restrict s lemma cond_apply (µ : measure α) (s : set α) (hms : measurable_set s) (t : set α) : µ[t|s] = (µ s)−1 * µ (s ∩ t) := by { rw [cond, measure.smul_apply, measure.restrict_apply’ hms, set.inter_comm], refl } where we use the notation µ[|s] for cond µ s and µ[t|s] for the application cond µ s t. 3.2.1 Properties Probability Measure One property that we can quickly observe is that, as long as µ(c) > 0 and ﬁnite, µ|c is a probability measure. Because the universal set is always measurable, we have by (3.13) that µ(univ | c) = µ(c) −1 × µ(c ∩ univ) = µ(c) −1 × µ(c) = 1. Intersection An additional property we can derive is that conditioning on two measurable sets is the same as conditioning on their intersection: ∀ s,t ∈ M, µ|s|t = µ|s∩t. 53 Note that this is an equality between the measures themselves, not just between their applications on certain sets. Such an equality can be proven in this case using the “extensionality” principle, which states that, for measures µa and µb deﬁned on measurable space M, µa = µb ⇐⇒ ∀s ∈ M, µa(s) = µb(s). In Lean we have the statement: lemma cond_cond_eq_cond_inter (µ : measure α) {s : set α} (hms : measurable_set s) {t : set α} (hmt : measurable_set t) (hci : µ (s ∩ t) ̸= 0) : µ[|s][|t] = µ[|s ∩ t] := . . . Bayes’ Theorem Bayes’ Theorem now can be formalized fairly easily as the result: ∀ s,t ∈ M, µ(t | s) = µ(s) −1 × µ(s | t) × µ(t). The proof in Lean is easy, using cond_apply: lemma cond_mul_eq_inter {s : set α} (hms : measurable_set s) (hcs : µ s ̸= 0) (t : set α) : µ[t|s] * µ s = µ (s ∩ t) := by rw [cond_apply µ hms t, mul_comm, ←mul_assoc, ennreal.mul_inv_cancel hcs (measure_ne_top _ s), one_mul] 54 /-- **Bayes’ Theorem** -/ theorem cond_eq_inv_mul_cond_mul {s : set α} (hms : measurable_set s) {t : set α} (hmt : measurable_set t) (ht : µ t ̸= 0) : µ[t|s] = (µ s)−1 * µ[s|t] * (µ t) := by rw [mul_assoc, cond_mul_eq_inter µ hmt ht s, set.inter_comm, cond_apply _ hms] Independence and Conditional Irrelevance Using the deﬁnition of conditional probability, we can now derive a more intuitive equivalent criterion for independence of events: ∀ s,t ∈ M, indep_setµ(s,t) ⇐⇒ µ(t) ̸= 0 =⇒ µ(s | t) = µ(s), (3.14) that is, independence is equivalent to the irrelevance of conditioning on one of the sets when it is nonzero. In the ﬁrst direction, the result follows by (3.12) and (3.13). In the second direction, we have to explicitly consider the cases of µ(t) = 0 and µ(t) ̸= 0. If µ(t) = 0, then we trivially have that s and t are independent by multiplication with zero and the monotonicity property. Otherwise, we can prove they are independent again using (3.13). In Lean, this proof takes the form: theorem indep_set_iff_cond_irrel {s : set α} (hms : measurable_set s) (hmt : measurable_set t) (µ : measure α) [is_probability_measure µ] : indep_set s t µ ↔ µ t ̸= 0 → µ[s|t] = µ s := 55 begin split; intro h, -- intro hypotheses in both directions { intro hct, -- first direction, intro ‘µ t ̸= 0‘ simp [*, (indep_set_iff_measure_inter_eq_mul hmt hms µ).mp h.symm, ← mul_assoc, ennreal.inv_mul_cancel hct (measure_ne_top _ _)] }, by_cases hct : µ t = 0, -- second direction { rw indep_set_iff_measure_inter_eq_mul hms hmt µ, -- trivial independence simp [measure_inter_null_of_null_right, hct] }, { have hcond := h hct, -- use the fact that ‘µ[t|s] * µ s = µ (s ∩ t)‘ refine (indep_set_iff_measure_inter_eq_mul hms hmt µ).mpr _, rwa [ ← mul_comm, ennreal.inv_mul_eq_iff_eq_mul hct (measure_ne_top _ _), ← measure.restrict_apply’ hmt] }, end 3.2.2 Deﬁning Conditional Independence Given three sets of events A, B,C : set (set α), we deﬁne conditional independence of A and B given C as the independence of A and B when µ is conditioned on any c ∈ C: cindep_setsµ(A, B,C) ⇐⇒ ∀ c ∈ C, indep_setsµ|c(A, B). In Lean: def cond_indep_sets {α} [measurable_space α] (s1 s2 : set (set α)) (C : set (set α)) (µ : measure α) : Prop := ∀ (c ∈ C), indep_sets s1 s2 µ[|c] 56 Given three measurable spaces MA, MB, MC : set (set α), we deﬁne conditional independence of MA and MB given MC as cindepµ(MA, MB, MC) ⇐⇒ ∀ c ∈ MC, indepµ|c(MA, MB). Given events a, b : set α and a set of events c : set α, we deﬁne we deﬁne conditional independence of a and b given c as cindep_setµ(a, b, c) ⇐⇒ indep_setµ|c(a, b). In Lean: def cond_indep_set {α} [measurable_space α] (s t : set α) (c : set α) (µ : measure α) : Prop := cond_indep_sets (generate_from {s}) (generate_from {t}) {c} µ Notice that we don’t use generate({c}) for this last deﬁnition. The reason for this is simple – doing so would not result in an equivalent deﬁnition. This is because of the fact that conditional independence does not imply independence (nor does independence imply conditional indepen- dence), so generating a set of conditions that includes the universal set – conditioning on which would result in an unconditioned measure – would not necessarily maintain any conditional independence. Conditional Independence and Intersection Irrelevance We can now generalize (3.14) to conditional independence with the theorem: ∀ s,t, c ∈ M, cindep_setµ(s,t, c) ⇐⇒ µ(c ∩ t) ̸= 0 =⇒ µ(s | c ∩ t) = µ(s | c). (3.15) 57 To see this, ﬁrst observe that we can rewrite this as: ∀ s,t, c ∈ M, indep_setµ|c(s,t) ⇐⇒ µ|c(t) ̸= 0 =⇒ µ|c(s | t) = µ|c(s). Now, there are two cases to consider: either µ(c) = 0 or µ(c) > 0. If µ(c) = 0, then both sides can be shown to be true quite easily. If µ(c) > 0, then we know that µ|c is a probability measure, so we can directly apply (3.14) to get our result. In Lean: theorem cond_indep_set_iff_cond_inter_irrel (hms : measurable_set s) (hmt : measurable_set t) {c : set α} (hmc : measurable_set c) (µ : measure α . volume_tac) [is_probability_measure µ] : cond_indep_set s t c µ ↔ µ (c ∩ t) ̸= 0 → µ[s|c ∩ t] = µ[s|c] := begin have : µ (c ∩ t) ̸= 0 → (µ[s|c ∩ t] = µ[s|c] ↔ µ[|c][|t] s = µ[s|c]), { intro h, rw ←cond_cond_eq_cond_inter µ hmc hmt _ h, exact (outer_measure.pos_of_subset_ne_zero _ (set.inter_subset_left _ _) h).ne’ }, rw [cond_indep_set_def, forall_congr this], -- rewrite RHS by_cases h : µ c = 0, { simp [*, indep_set_of_cond_null_measure, measure_inter_null_of_null_left] }, { haveI := probability_theory.cond_is_probability_measure µ h, simp only [*, inter_ne_zero_iff_cond_ne_zero, indep_set_iff_cond_irrel] } end Observe that (3.15) is in fact a generalization of (3.14). This is because we can show that conditioning on the universal set (c = univ) lets us simplify down to it. In English, this theorem states that two sets s and t are conditionally independent given a third set c if and only if applying the additional restriction of t on top of c in the conditional measure has no effect on the 58 probability of s (when this restrictor c ∩ t has non-zero measure). 3.3 Random Variables Let’s now move on to the formalization of distributions of random variables. As mentioned earlier, a random variable can simply be seen as a transformation on our event space: for some type β, we can have a random variable f : α → β. Suppose we have measurable spaces Mα and Mβ for α and β, respectively. If we have some measure µα on Mα, we would like to deﬁne a new measure on Mβ that exactly corresponds to the random variable f in the sense that every measurable set in β gets exactly the measure from µα of its preimage under f . As long as f is measurable, we have exactly the tool we need for this via (2.22): we can use mmap f (µα) as our distribution. If we’re starting with f and don’t have a deﬁnition of a measurable space on β, we can easily deﬁne a space such that f is measurable by letting Mβ = map f (Mα). Now, let’s make things more interesting. Suppose instead of a single random variable, we have multiple, in fact possibly inﬁnitely many, in what we will refer to as an “index” of random variables with some index type ι. For our results, the index type ι can be anything. It doesn’t even have to be countable! That is, we now have β : ι → Type as an index of types, with our variable index being the Pi type: F : Π (i : ι), α → β(i). It’s an index of random variables where the output type can depend on the index. 59 3.3.1 The Joint Distribution We can now imagine deﬁning a “random variable” representing the entirety of this index with the previously deﬁned Feval(F) : α → Π (i : ι), β(i), where (again) ∀ a : α, i : ι, Feval(F)(a)(i) = F(i)(a). That is, Feval(F) can itself be considered a random variable on α that maps each a : α to the function in Π (i : ι), β(i) that is the indexed “evaluation” of every random variable in F at a. Naturally, now, we will have to discuss a measurable space on this space consisting of sets of pi types, i.e. set (Π (i : ι), β(i)). Luckily, as long as we have a measurable space for every β(i) in the form of a map Mβ : Π (i : ι), set (set β(i)), we have exactly the construction we need via (2.14). And measurability on Feval(F) is guaranteed as long as each of the individual random variables are measurable via (2.23). We will implicitly maintain that this is the case for the rest of the discussion, so in particular all of the measure transformation application properties mentioned earlier will hold. As expected, we now deﬁne our joint distribution on F as jointµ(F) = mmapFeval(F)(µ), the µ-mapped evaluator function. This makes for a very concise Lean deﬁnition: def joint (µ : measure α) (F : Π i : ι, α → (β i)) : measure (Π i : ι, β i) := map (λ a i, F i a) µ Here, the (λ a i, F i a) lambda abstraction is exactly our Feval(F). If any of the F(i) are not measurable, we know by the deﬁnition of mmap(·) that this measure is the zero measure, so clearly the only interesting results will arise when all the random variables are measurable functions. Again, this is a measure on the space of Π (i : ι), β(i), so it measures sets of assignments to every 60 indexed random variable. 3.3.2 Restricting a Pi In many cases, however, we don’t care about measures on the whole space of variables that we’ve deﬁned. It’s common to want to know the probability of a set of assignments to a speciﬁc subset of the random variables, rather than all of them. This is where we come across the notion of a “Pi restriction function”. For some set of indices I : ι, this is simply a function that converts a function from the whole index space to one only deﬁned over I: for all f : Π (i : ι), β(i), we deﬁne this restriction function restrI( f ) : Π (i : I), β(i) such that, for all i : I, restrI( f )(i) = f (i). Indeed, the only difference between f and restrI( f ) is that restrI( f ) is only deﬁned on the restricted subtype domain I. In Lean, this has the deﬁnition: def set.restrict {α : Type*} {π : α → Type*} (s : set α) (f : Π a : α, π a) : Π a : s, π a := λ a : s, f a Restricting a Pi Set We now have a tool that we can easily use to convert from Pi sets to restricted Pi sets and vice versa, simply using the standard set image and preimage transformations. In the ﬁrst direction we deﬁne the “I-restriction image” of a f : set (Π (i : ι), β(i)) as <[I] f = restr ′′ I ( f ), with type set (Π (i : I), β(i)), where the “<[] ” notation mirrors that used in the Lean formalization. This function in particular, however, won’t be that useful except within certain deﬁnitions and 61 proofs. Unrestricting a Restricted Pi Set The other direction is more interesting. We deﬁne the “I-restriction preimage” of a f : set (Π (i : I), β(i)) as >[I] f = restr ′−1 I ( f ), which gives us the set of all Pis in Π (i : ι), β(i) (with the full index type ι) that map to f . If we consider any i /∈ I, this set’s functions will fully span β(i) at that index for every assignment originally in f (and for every possible combination of the other excluded indices; see Figure 3.1). It’s easy to show that Π (i : I), β(i) is a measurable function (under the measurable spaces constructed by (2.14)). So, we can now think about converting from a “marginal assignment” set to a complete assignment set that we can take the measure of with the joint distribution. β(0) β(1) β(2) β(3) >[I] f : set (Π (i : ι), β(i)) β(1) β(2) f : set (Π (i : I), β(i)) Figure 3.1: Unrestricting a restricted Pi set. Suppose our index type is the subtype of the ﬁrst four natural numbers ι = {0, 1, 2, 3}, Pictured on the right is a set f : Π (i : I), β(i) of Pi types restricted to the indices I = {1, 2}. Pictured on the left is a set of unrestricted Pi types >[I] f : Π (i : ι), β(i) from taking the I-restriction preimage of f . In both diagrams, the set of Pi types is the set of all functions where each index maps to within one of the colored regions. From one Restricted Pi Set to Another Something else we may want to do is convert from one restricted pi set to another. For example, if we have two sets I, I′ : set ι, we may want to go from set (Π (i : I), β(i)) to 62 set (Π (i : I′), β(i)). How can we deﬁne such a function? To start, let’s consider a tool that will help us in this deﬁnition. Let “subcastI(I′) : set I” indicate casting a set of a subtype, which essentially indicates the set of all things in both I′ and I (for all intents and purposes, this is I′ ∩ I, just represented as a set of the subtype I). Let’s use the notation (I′ : set I) = subcastI(I′) to represent this (taking inspiration from cast-via- coercion notation in Lean). Given an assignment f0 : Π (i : I′), β(i), we can go to an assignment Π (i : (I′ : set I)), β(i) using a modiﬁed version of the restriction function we deﬁned earlier, prestrI′,I( f0). We deﬁne prestrI′,I : Π (i : I′), β(i) → Π (i : (I′ : set I)), β(i) as prestrI′,I( f0)(i : (I′ : set I)) = f0(i), so, in essence, prestrI′,I restricts a Pi to only the indices that are in both I′ and I. Dealing with nested subtypes like this is obviously annoying, but fortunately we don’t have to think about it for too long, as prestrI′,I is simply a means to an end. Given some f : set (Π (i : I′), β(i)), we can go to set (Π (i : (I′ : set I)), β(i)) with the image prestr′′ I′,I( f ). And now, we can take the preimage of this subtype-subtype pi set on I′ : set I to the subtype pi set on I by itself using the previously deﬁned (I′ : set I)-restriction preimage (where the “full index” type is now actually the subtype of I). This gives us the deﬁnition >>[I] f = >[I′ : set I] (prestr ′′ I′,I( f )), which achieves our goal of going from set (Π (i : I′), β(i)) to set (Π (i : I), β(i)). Generality aside, however, let’s look at how we’re going to practically use this. In most situations, we will have that I′ ⊆ I. If this is the case, then we can see a correspondence between >[I] and >>[I] in that, just like how >[I] extended to all possible combinations of values for the β(i) on the indices outside of I, >>[I] extends to all possible combinations of values for the β(i) on the indices outside of I′ but in I, i.e. I \\ I′. In particular, we will soon see that the way that we 63 are going to use this will be for sets A, B : set ι, where I = A ∪ B and I′ = A or I′ = B. 3.3.3 The Marginal Distribution For a set of indices I : set ι, the marginal distribution is easy to deﬁne: marginalµ(I, F) = jointµ(restrI(F)), in other words, we simply imagine that our variable index is deﬁned on the subtype I rather than the full index type ι, and make a joint distribution out of that, giving us a measure on Π (i : I), β(i). In Lean: def marginal (µ : measure α) (F : Π i : ι, α → (β i)) (mv : set ι) : measure (Π i : mv, β i) := joint µ (mv.restrict F) (here the dot notation mv.restrict F is a shorthand for set.restrict mv F). The Conditional Distribution Let’s quickly deﬁne the notation FevalrX (F) = Feval(restrX (F)). We can see FevalrX (F) : α → Π (i : X), β(i) as the transformation of F into a random variable that maps from a : α to the evaluation of F at a, restricted to indices X. For some I,C : set ι and c : set (Π (i : C), β(i)), we deﬁne for our convenience a \"condi- tional random variable measure\" as condµ(I,C, F | c) = marginalµ |FevalrC(F)′−1(c)(I, F). This is simply the marginal distribution of I on F, where instead of the normal measure µ, we use µ conditioned on c, after pre-imaging c into the space of α with FevalrC(F) ′−1. In Lean: 64 def cond (µ : measure α) (F : Π i : ι, α → (β i)) (I C : set ι) (c : set (Π i : C, β i)) : measure (Π i : I, β i) := marginal (cond_measure µ ((λ a (i : C), F i a) −1’ c)) F I where the lambda (λ a (i : C), F i a) is our FevalrC(F). 3.3.4 Marginalization While the deﬁnition of a marginal distribution makes sense, we would still like some insight into how it relates to the joint distribution (on the unrestricted F). For this, we ﬁrst deﬁne a generic “marginalization” on a measure µι on Π (i : ι), β(i) and set of indices I : set α as marginalize(µι, I) = mmaprestrI (µι). In Lean: def marginalize (µ : measure (Π i : ι, β i)) (mv : set ι) : measure (Π i : mv, β i) := map (set.restrict mv) µ Note that there is no mention of F in this deﬁnition, as we are marginalizing purely based on a measure that already is on Π (i : ι), β(i). Via (2.22), the effect of this deﬁnition is to give any measurable s : set (Π (i : I), β(i)) the measure of its preimage under µι: marginalize(µι, I)(s) = µι(>[I] s). (3.16) The Marginalization Principle We can also show without too much difﬁculty that marginalµ(I, F) = marginalize(jointµ(F), I), (3.17) 65 that is to say, the marginal distribution is the marginalized joint distribution. (3.16) and (3.17) together give us an interesting application rule: for any measurable s : set (Π (i : I), β(i)), marginalµ(I, F)(s) = jointµ(F)(>[I] s), which says that the marginal probability of s is the joint probability of that same set, extended to allow the unmarginalized variables to take any value. In the discrete case, we can see how this corresponds to the concept of “summing up” over the probabilities of all of the combinations of values of the unmarginalized variables. In Lean, we have the three short lemmas: variables {α : Type*} {m : measurable_space α} (µ : measure α) {ι : Type*} {β : ι → Type*} (F : Π i : ι, α → (β i)) [Π i : ι, measurable_space (β i)] lemma marginal_eq_marginalize_joint (hm : ∀ i : ι, measurable (F i)) (mv : set ι) : marginal µ F mv = marginalize (joint µ F) mv := by { rw [marginalize, joint, map_map, function.comp], refl, apply measurable_pi_restrict, exact measurable_pi_iff.mpr hm } lemma marginalize_apply (µ : measure (Π i : ι, β i)) (mv : set ι) {s : set (Π i : mv, β i)} (hms : measurable_set s) : marginalize µ mv s = µ (>[] s) := by { rw [marginalize, map_apply _ hms], apply measurable_pi_restrict } theorem marginal_apply (hm : ∀ i : ι, measurable (F i)) (mv : set ι) {s : set (Π i : mv, β i)} (hms : measurable_set s) : marginal µ F mv s = joint µ F (>[] s) := by rw [marginal_eq_marginalize_joint _ _ hm, marginalize_apply _ _ hms] 66 3.3.5 Independence on Random Variables We have an understanding of the independence of events, but what does it mean for random variables to be independent? Suppose we have a measure µ on α and two random variables X : α → β and Y : α → γ, with measurable spaces Mβ and Mγ. For some measurable s : set β, and measurable t : set γ, we can look at the preimages X ′−1(s) and Y ′−1(t), and check whether they are independent under µ. Now, if this is true for all possible pairs of measurable sets on β and γ, we say that X and Y are independent. We have exactly the functions we need to formalize this deﬁnition with: indepvµ(X,Y ) ⇐⇒ indepµ(comapX (Mβ), comapY (Mγ)). This deﬁnition is useful, though it’s still not as general as we would like. We want to also be able to discuss independence on pairs of sets of random variables, not just pairs of individual random variables. Suppose we have A, B : set ι. We have measurable spaces on Π (i : A), β(i) and Π (i : B), β(i) via (2.14). Let’s refer to these as MA and MB, respectively. We want to formalize the idea that, for any measurable combination of assignments to the variables indexed by A and B, let’s call them a : set (Π (i : A), β(i)) and b : set (Π (i : B), β(i)), they have independent α-preimages FevalrA(F)′−1(a) and FevalrB(F)′−1(b) under µ. We formalize this more general deﬁnition of random variable independence as: indepvsµ(F, A, B) ⇐⇒ indepµ ( comapFevalrA(F)(MA), comapFevalrB(F)(MB) ) . And in Lean: def independent (µ : measure α) (F : Π (i : ι), α → β i) (A B : set ι) : Prop := indep ((measurable_space.pi).comap (λ a i, (pi_subtype A F) i a)) ((measurable_space.pi).comap (λ a i, (pi_subtype B F) i a)) µ 67 where we are using dot notation for comap and measurable_space.pi (that is, (2.14)) infers the correct Pi measurable space on Π (i : A), β(i) and Π (i : B), β(i) in the ﬁrst and second arguments, respectively. A useful fact we can prove about this deﬁnition is that indepvsµ(F, A, B) ⇐⇒ indepjointµ(F) ( comaprestrA(MA), comaprestrB(MB) ) . (3.18) This means that independence on random variables is the same as independence on the joint distribution of the measurable space preimages of the restricted measurable spaces. 3.3.6 Conditional Independence on Random Variables Taking inspiration from our deﬁnition of independence on random variables, let’s now deﬁne the conditional independence of random variables. Given A, B,C : set ι with measurable spaces MA, MB, and MC on the space Π (i : A), β(i), the space Π (i : B), β(i), and the space Π (i : C), β(i), respectively via (2.14), we deﬁne the conditional independence of the variables A and B given the variables C as: cindepvsµ(F, A, B,C) ⇐⇒ cindepµ ( comapFevalrA(F)(MA), comapFevalrB(F)(MB), comapFevalrC(F)(MC) ) . In words, this is the criterion that, for any measurable combination of assignments to the variables indexed by A and B, let’s call them a : set (Π (i : A), β(i)) and b : set (Π (i : B), β(i)), they have independent α-preimages Feval(restrA(F))′−1(a) and Feval(restrB(F))′−1(c) under µ|c for all c ∈ comapFeval(restrC(F))(MC). Deﬁning this in Lean, we have: def cond_independent (µ : measure α) (F : Π (i : ι), α → β i) (A B C : set ι) : Prop := cond_indep ((measurable_space.pi).comap (λ a i, (pi_subtype A F) i a)) ((measurable_space.pi).comap (λ a i, (pi_subtype B F) i a)) ((measurable_space.pi).comap (λ a i, (pi_subtype C F) i a)) µ 68 As this suggests, we can see the following equivalence by deﬁnition: cindepvsµ(F, A, B,C) ⇐⇒ ∀ c ∈ comapFevalrC(F)(MC), indepvsµ|c(F, A, B). (3.19) With the help of (3.19) and (3.18), we can show that cindepvsµ(F, A, B,C) ⇐⇒ cindepjointµ(F) ( comaprestrA(MA), comaprestrB(MB), comaprestrC (MC) ) , (3.20) the conditional version of (3.18) stating that conditional independence on random variables is the same as conditional independence on the joint distribution of the measurable space preimages of Π (i : A), β(i) and Π (i : B), β(i) given the measurable space preimage of Π (i : C), β(i). Conditional Independence and Intersection Irrelevance This brings us to the latest results I have formalized in this area as of writing this report. Using (3.20), we can “lift” the result (3.15) to the context of random variables with the theorem: cindepvsµ(F, A, B,C) ⇐⇒ ∀ b ∈ MB, c ∈ MC, marginalµ(B ∪C, F)(b ∩ c) ̸= 0 =⇒ condµ(A, B ∪C, F | b ∩ c) = condµ(A,C, F | c). (3.21) Here, you should read the uses of b ∩ c as >>[B ∪C] b ∩ >>[B ∪C] c – this has been left implicit for brevity. Note again that the conclusion on the righthand-side is an equality on measures themselves, not just their applications. Verbally, this theorem states that the criterion of A and B being conditionally independent given C is the same thing as requiring that, for any measurable b : Π (i : B), β(i) and c : Π (i : C), β(i) with non-zero probability of happening together, additionally conditioning on b (on top of c) has no effect on the conditional distribution on A. In Lean this is the statement: 69 theorem cond_independent_iff_cond_inter_irrel (µ : measure α) (F : Π (i : ι), α → β i) [is_probability_measure µ] (hm : ∀ i : ι, measurable (F i)) (A B C : set ι) : cond_independent µ F A B C ↔ ∀ (b : set (Π i : B, β i)) (hmb : measurable_set b) (c : set (Π i : C, β i)) (hmc : measurable_set c), marginal µ F (B ∪ C) (>>[] b ∩ >>[] c) ̸= 0 → cond µ F A (B ∪ C) (>>[] b ∩ >>[] c) = cond µ F A C c := . . . Independence and Conditional Irrelevance From (3.21) we can also derive an equivalent criterion for (unconditional) independence: indepvsµ(F, A, B) ⇐⇒ ∀ b ∈ MB, marginalµ(B, F)(b) ̸= 0 =⇒ condµ(A, B, F | b) = marginalµ(A, F). (3.22) which states that the criterion of A and B being independent is the same thing as requiring that, for any measurable b with positive probability, conditioning on b has no effect on the marginal distribution on A. This results simpliﬁes directly from (3.21) when we set C = /0. In Lean: theorem independent_iff_cond_irrel (µ : measure α) (F : Π (i : ι), α → β i) [is_probability_measure µ] (hm : ∀ i : ι, measurable (F i)) (A B : set ι) : independent µ F A B ↔ ∀ (b : set (Π i : B, β i)) (hmb : measurable_set b), marginal µ F B b ̸= 0 → cond µ F A B b = marginal µ F A := . . . 70 Bibliography [1] James K. Rilling and Thomas R. Insel. A Formulation of the Simple Theory of Types. The Journal of Symbolic Logic, 5(2):56–68, 1940. [2] William A. Howard. The formulae-as-types notion of construction. To H.B. Curry: Essays on Combinatory Logic, Lambda Calculus and Formalism, pages 479–490, 1980. [3] Per Martin-Löf. An Intuitionistic Theory of Types: Predicative Part. Studies in Logic and the Foundations of Mathematics, 80:73–118, 1975. [4] Thierry Coquand. The Calculus of Constructions. PhD thesis, INRIA, 1986. [5] Christine Paulin-Mohring. Introduction to the Calculus of Inductive Constructions. All about Proofs, Proofs for All, 55, 2015. [6] Leonardo de Moura, Soonho Kong, Jeremy Avigad, Floris Van Doorn, , and Jakob von Raumer. The Lean Theorem Prover (system description). International Conference on Automated Deduction, pages 378–388, 2015. [7] Leonardo de Moura, Jeremy Avigad, Soonho Kong, and Cody Roux. Elaboration in Dependent Type Theory. 2015. [8] Gabriel Ebner, Sebastian Ullrich, Jared Roesch, Jeremy Avigad, and Leonardo de Moura. A metaprogramming framework for formal veriﬁcation. Proceedings of the ACM on Programming Languages, 1(34):1–29, 2017. [9] The mathlib Community. The Lean Mathematical Library. Proceedings of the 9th ACM SIGPLAN International Conference on Certiﬁed Programs and Proofs, page 367–381, 2020. [10] Kevin Buzzard. The Xena Project. https://www.ma.imperial.ac.uk/~buzzard/ xena/. [11] Leonardo de Moura and Sebastian Ullrich. The Lean 4 Theorem Prover and Programming Language. Platzer A., Sutcliffe G. (eds) Automated Deduction – CADE 28. CADE 2021. Lecture Notes in Computer Science., 12699, 2021. 71 [12] The Lean Community. Theorem Proving in Lean. https://leanprover.github.io/ theorem_proving_in_lean/. [13] The Lean Community. The Natural Number Game. https://www.ma.imperial.ac.uk/ ~buzzard/xena/natural_number_game/. 72","libVersion":"0.2.1","langs":""}