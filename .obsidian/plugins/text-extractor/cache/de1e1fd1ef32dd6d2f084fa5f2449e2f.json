{"path":".obsidian/plugins/text-extractor/cache/de1e1fd1ef32dd6d2f084fa5f2449e2f.json","text":"Math 318 Solutions to Assignment #6 March 17, 2020 Total marks = [30]. 1. (a) The times between bad events are independent Exp(2.5) random variables X1, X2, . . ., which each have mean µ = EXi = 1 2.5 = 0.4 and standard deviation σ = 0.4. The item is discarded at time S200 = ∑200 i=1 Xi. By the CLT, P (S200 ≤ 72) = P ( S200 − (200)(0.4) (0.4)√200 ≤ 72 − (200)(0.4) (0.4)√200 ) ≈ P (Z ≤ −1.41) = 1 − Φ(1.41) = 1 − 0.9207 = 0.0793 [3] (b) P (S200 ≥ 92) = P ( S200 − (200)(0.4) (0.4)√200 ≥ 92 − (200)(0.4) (0.4) √200 ) ≈ P (Z ≥ 2.12) = 1 − Φ(2.12) = 1 − 0.9830 = 0.0170 [2] 2. Let X1, . . . , X300 be the weights of the containers and let S = ∑300 i=1 Xi be the total weight. X1 has mean µ = (8 + 2)/2 = 5 and variance σ2 = ∫ 8 2 (x − 5)2 1 6 dx = 3. Writing c for the capacity, and applying the Central Limit Theorem, we require that 0.99 = P (S < c) = P ( S − 300µ σ√300 < c − 300µ σ√300 ) ≈ (Z < c − 1500 30 ) . From tables this gives c−1500 30 ≈ 2.33, so c ≈ 1500 + 30 × 2.33 ≈ 1570. [6] 3. (a) i. This follows from the fact that if X ∼ Poisson(λ1) and Y ∼ Poisson(λ1) are independent, then X + Y ∼ Poisson(λ1 + λ2). Each Ai has µ = σ = 1. [1] ii. By the CLT, (∑100 i=1 Ai − 100)/( √100) has an approximately standard normal distribution, and hence the distribution of Y , which is that of ∑100 i=1 Ai, is approx- imately that of 100 + √100Z with Z ∼ N (0, 1). [1] iii. The p.d.f. of 100 + 10Z is a bell curve shifted to have mean 100 and with width scaled by √100 = 10. Since Z lies between −3 and +3 with high probability, Y lies between 70 and 130 with high probability. [1] (b) i. Let µ = ENi = P (Y = 100) = 0.040 and σ2 = VarNi = µ(1 − µ). The CLT says that (M (i) − iµ)/(σ√i) is approximately standard normal. Equivalently, M (i) has approximately the same distribution as iµ + σ√iZ. [2] ii. A typical size for |i−1M (i)−µ| is σ/ √i. For i = 10000 this gives typical ﬂuctuation size of σ/100 = 0.002. The plot is consistent with this. [1] 4. (a) If U ∼ Uniform[0, 1] then f (U ) is a random variable with mean µ = ∫ 1 0 f (x)dx = I and variance σ2 = ∫ 1 0 f (x) 2dx − I 2. Let SN = f (U1) + · · · + f (UN ), so that IN = N −1SN . By the central limit theorem, lim N →∞ P ( a ≤ SN − N µ σ√N ≤ b ) = P (a ≤ Z ≤ b). Taking a = −x, b = x, this gives limN →∞ P (|IN − I| ≤ σxN −1/2) = P (|Z| ≤ x). [3] (b) We want N such that P (|IN − I| ≤ σxN −1/2 = 0.01) ≈ P (|Z| ≤ x) = 0.95. From the table, this means x = 1.96, so N = (σx/0.01) 2 = (196σ)2. In the worst case, σ = 1, so we should take N = (196) 2 = 38, 416. [3] 1 5. (a) Here is a script that will perform the approximation. import numpy as np # Approximate \\Phi(1) - \\Phi(0) via Monte Carlo integration # Let N be the number of terms we will try. N = 40000; # Build a vector of N uniform random numbers. U_n = np.random.uniform(size=N); # For each entry x, compute f(x): f_U_n = np.exp(-(U_n**2)/2)/np.sqrt(2*np.pi); # Sum the entries: numerator = sum(f_U_n); # Compute the average: approximation_1 = numerator/N approximation_1 Your output is going to be random, of course, so results will vary. Running the above code three times gives output 0.34114, 0.34127, and 0.34147. [3] (b) The probability that a uniform random variable on [0, 1] × [0, 1] lands in a subset A ⊂ [0, 1] × [0, 1] is simply the area of A. Let A be the region {(x, y) : x ∈ [0, 1], y ≤ f (x)}, and notice that f (x) ≤ 1, so A is entirely contained inside [0, 1] × [0, 1]. Thus we can estimate this probability using the proportion of points that fall in A. [1] (c) A script to perform this approximation: # Approximate \\Phi(1) - \\Phi(0) by generating random points in a unit # square and counting the proportion that fall under the curve y = f(x) # Let N be the number of terms we will try. N = 40000; # Build N uniform random points in the square: do this by # generating the coordinates as uniforms. Make a 2xN matrix whose # first row is the X coords and second row is the Ys. coords = np.random.uniform(size=(2,N)); coords.shape # For each point, compute whether or not the point falls in the # region y <= f(x) good_point = (coords[1,] <= np.exp(-(coords[0,:]**2)/2)/np.sqrt(2*np.pi)); # Compute the proportion: approximation_2 = sum(good_point)/N approximation_2 As in (a) your results will be random. Running this code three times produces the results 0.33950, 0.34335, and 0.34250. [3] 2","libVersion":"0.2.1","langs":""}