{"path":".obsidian/plugins/text-extractor/cache/ee7d8ffc541014849fa3eb4e4baae935.json","text":"3 Naive Bayes [35 points]| If you run main.py mnist-naive-nb, it will load training and test data for MNIST, discretized into bi- nary values. It will then fit a “naive’ naive Bayes model, which is a generative classifier that assumes (@1, s, ,@a,y) is product of Bernoullis. This model has a very high test error (88.6%), since the features do not affect the predictions. [8.1] [3 points] The regular naive Bayes model discussed in class writes the joint probablity of a dataset (X,y) as n d pX,y) = [T |p) [T p” 1) =t i a o) 1y LIy ) 1@ =TT o= 00 T 650 - 00 —=) | =t j=1 using Bernoullis to parameterize p(y’) and cach conditionals p(a; | y). Show how to derive the MLE for any particular parameter 0y, You can assume here that the log-likelihood is concave (so that any stationary point is a global optimum). Answer: TODO","libVersion":"0.2.1","langs":"eng"}