{"path":".obsidian/plugins/text-extractor/cache/b561c55bc3541e5c0dfdf6062ee0524e.json","text":"CPSC 540: Machine Learning Bayesian Statistics Mark Schmidt University of British Columbia Winter 2020 Motivation: Controlling Complexity For many structured prediction tasks, we need very complicated models. We require multiple forms of regularization to prevent overﬁtting. In 340 we saw two ways to reduce complexity of a model: Model averaging (ensemble methods). Regularization (linear models). Bayesian methods combine both of these. Average over models, weighted by posterior (which includes regularizer). Allows you to ﬁt extremely-complicated models without overﬁtting. Current Hot Topics in Machine Learning Bayesian learning includes: Gaussian processes. Approximate inference. Bayesian nonparametrics. Why Bayesian Learning? Standard L2-regularized logistic regression steup: Given ﬁnite dataset containing IID samples. E.g., samples (x i, yi) with xi ∈ Rd and yi ∈ {−1, 1}. Find “best” w by minimizing NLL with a regularizer to “prevent overﬁtting”. ˆw ∈ argmin w − n∑ i=1 log p(yi | xi, w) + λ 2 ∥w∥ 2. Predict labels of new example ˜x using single weights ˆw, ˆy = sgn( ˆwT ˜x). But data was random, so weight ˆw is a random variables. This might put our trust in a ˆw where posterior p( ˆw | X, y) is tiny. Bayesian approach: “all parameters are nuissance parameters”. Treat w as random and predict based on rules of probability. Problems with MAP Estimation Does MAP make the right decision? Consider three hypothesese H = {“lands ′′, “crashes ′′, “explodes ′′} with posteriors: p(“lands ′′ | D) = 0.4, p(“crashes ′′ | D) = 0.3, p(“explodes ′′ | D) = 0.3. The MAP estimate is “plane lands”, with posterior probability 0.4. But probability of dying is 0.6. If we want to live, MAP estimate doesn’t give us what we should do. Bayesian approach considers all models: says don’t take plane. Bayesian decision theory: accounts for costs of diﬀerent errors. MAP vs. Bayes MAP (regularized optimization) approach maximizes over w: ˆw ∈ argmax w p(w | X, y) ≡ argmax w p(y | X, w)p(w) (Bayes’ rule, w ⊥ X) ˆy ∈ argmax y p(y | ˜x, ˆw). Bayesian approach predicts by integrating over possible w: p(˜y | ˜x, X, y) = ∫ w p(˜y, w | ˜x, X, y)dw marginalization rule = ∫ w p(˜y | w, ˜x, X, y)p(w | ˜x, X, y)dw product rule = ∫ w p(˜y | w, ˜x)p(w | X, y)dw ˜y ⊥ X, y | ˜x, w Considers all possible w, and weights prediction by posterior for w. Motivation for Bayesian Learning Motivation for studying Bayesian learning: 1 Optimal decisions using rules of probability (and possibly error costs). 2 Gives estimates of variability/conﬁdence. E.g., this gene has a 70% chance of being relevant. 3 Elegant approaches for model selection and model averaging. E.g., optimize λ or optimize grouping of w elements. 4 Easy to relax IID assumption. E.g., hierarchical Bayesian models for data from diﬀerent sources. 5 Bayesian optimization: fastest rates for some non-convex problems. 6 Allows models with unknown/inﬁnite number of parameters. E.g., number of clusters or number of states in hidden Markov model. Why isn’t everyone using this? Philosophical: Some people don’t like “subjective” prior. Computational: Typically leads to nasty integration problems. Coin Flipping Example: MAP Approach MAP vs. Bayesian for a simple coin ﬂipping scenario: 1 Our likelihood is a Bernoulli, p(H | θ) = θ. 2 Our prior assumes that we are in one of two scenarios: The coin has a 50% chance of being fair (θ = 0.5). The coin has a 50% chance of being rigged (θ = 1). 3 Our data consists of three consecutive heads: ‘HHH’. What is the probability that the next toss is a head? MAP estimate is ˆθ = 1, since p(θ = 1 | HHH) > p(θ = 0.5 | HHH). So MAP says the probability is 1. But MAP overﬁts: we believed there was a 50% chance the coin is fair. Coin Flipping Example: Posterior Distribution Bayesian method needs posterior probability over θ, p(θ = 1 | HHH) = p(HHH | θ = 1)p(θ = 1) p(HHH) (Bayes rule) (marg and prod rule) = p(HHH | θ = 1)p(θ = 1) p(HHH | θ = 0.5)p(θ = 0.5) + p(HHH | θ = 1)p(θ = 1) = (1)(0.5) (1/8)(0.5) + (1)(0.5) = 8 9 , and similarly we have p(θ = 0.5 | HHH) = 1 9 . So given the data, we should believe with probability 8 9 that coin is rigged. There is still a 1 9 probability that it is fair that MAP is ignoring. Coin Flipping Example: Posterior Predictive Posterior predictive gives probability of head given data and prior, p(H | HHH) = p(H, θ = 1 | HHH) + p(H, θ = 0.5 | HHH) = p(H | θ = 1, HHH)p(θ = 1 | HHH) + p(H | θ = 0.5, HHH)p(θ = 0.5 | HHH) = (1)(8/9) + (0.5)(1/9) = 0.94. So the correct probability given our assumptions/data is 0.94, and not 1. Though with a diﬀerent prior we would get a diﬀerent answer. Notice that there was no optimization of the parameter θ: In Bayesian stats we condition on data and integrate over unknowns. In Bayesian stats/ML: “all parameters are nuissance parameters”. Coin Flipping Example: Discussion Comments on coin ﬂipping example: Bayesian prediction uses that HHH could come from fair coin. As we see more heads, posterior converges to 1. MLE/MAP/Bayes usually agree as data size increases. If we ever see a tail, posterior of θ = 1 becomes 0. If the prior is correct, then Bayesian estimate is optimal: Bayesian decision theory gives optimal action incorporating costs. If the prior is incorrect, Bayesian estimate may be worse. This is where people get uncomfortable about “subjective” priors. But MLE/MAP are also based on “subjective” assumptions. Bayesian Model Averaging In 340 we saw that model averaging can improve performance. E.g., random forests average over random trees that overﬁt. But should all models get equal weight? What if we ﬁnd a random decision stump that ﬁts the data perfectly? Should this get the same weight as deep random trees that likely overﬁt? In science, research may be fraudulent or not based on evidence. Should “vaccines cause autism” or “climate change denial” models get equal weight? In these cases, naive averaging may do worse. Bayesian Model Averaging Suppose we have a set of m probabilistic classiﬁers wj Previously our ensemble method gave all models equal weights, p(˜y | ˜x) = 1 m p(˜y | ˜x, w1) + 1 m p(˜y | ˜x, w2) + · · · + 1 m p(˜y | ˜x, wm). Bayesian model averaging (following rules of probability) weights by posterior, p(˜y | ˜x) = p(w1 | X, y)p(˜y | ˜x, w1) + p(w2 | X, y)(˜y | ˆx, w2)+ · · · + p(wm | X, y)p(˜y | ˜x, wm). So we should weight by probability that wj is the correct model. Equal weights assume all models are equally probable and ﬁt data equally well. Bayesian Model Averaging Weights are posterior, so proportional to likelihood times prior: p(wj | X, y) ∝ p(y | X, wj) | {z } likelihood p(wj) | {z } prior . Likelihood gives more weight to models that predict y well. Prior should gives less weight to models that are likely to overﬁt. This is how rules of probability say we should weight models. It’s annoying that it requires a “prior” belief over models. But as n → ∞, all weight goes to “correct” model[s] w∗ as long as p(w∗) > 0. Bayes for Density Estimation and Generative/Discriminative We can use Bayesian approach for density estimation: With data D and parameters θ we have: 1 Likelihood p(D | θ). 2 Prior p(θ). 3 Posterior p(θ | D). We can use Bayesian approach for supervised learning: Generative approach (naive Bayes, GDA) are density estimation on X and y: 1 Likelihood p(y, X | w). 2 Prior p(w). 3 Posterior p(w | X, y). Discriminative approach (logistic regression, neural nets) just conditions on X: 1 Likelihood p(y | X, w). 2 Prior p(w). 3 Posterior p(w | X, y). 7 Ingredients of Bayesian Inference (MEMORIZE) 1 Likelihood p(y | X, w). Probability of seeing data given parameters. 2 Prior p(w | λ). Belief that parameters are correct before we’ve seen data. 3 Posterior p(w | X, y, λ). Probability that parameters are correct after we’ve seen data. We won’t use the MAP “point estimate”, we want the whole distribution. 4 Predictive p(˜y | ˜x, w). Probability of test label ˜y given parameters w and test features ˜x. For example, sigmoid function for logistic regression. 7 Ingredients of Bayesian Inference (MEMORIZE) 5 Posterior predictive p(˜y | ˜x, X, y, λ). Probability of new data given old, integrating over parameters. This tells us which prediction is most likely given data and prior. 6 Marginal likelihood p(y | X, λ) (also called “evidence”). Probability of seeing data given hyper-parameters (integrating over parameters). We’ll use this later for hypothesis testing and setting hyper-parameters. 7 Cost C(ˆy | ˜y). The penalty you pay for predicting ˆy when it was really was ˜y. Leads to Bayesian decision theory: predict to minimize expected cost. Review: Decision Theory Are we equally concerned about “spam” vs. “not spam”. Consider a scenario where diﬀerent predictions have diﬀerent costs: Predict / True True “spam” True “not spam” Predict “spam” 0 100 Predict “not spam” 10 0 In 340 we discussed predictin ˆy given ˆw by minimizing expected cost: E[Cost(ˆy = “spam”)] = p(˜y = “spam” | ˜x, ˆw)C(ˆy = “spam” | ˜y = “spam”) + p(˜y = “not spam” | ˜x, ˆw)C(ˆy = “spam” | ˜y = “not spam”). Consider a case where p(˜y = “spam” | ˜x, ˆw) > p(˜y = “not spam” | ˜x, ˆw). We might still predict “not spam” if expected cost is lower. Bayesian Decision Theory Bayesian decision theory: Instead of using a MAP estimate ˆw, we should use posterior predictive, E[Cost(ˆy = “spam”)] = p(˜y = “spam” | ˜x, X, y)C(ˆy = “spam” | ˜y = “spam”) + p(˜y = “not spam” | ˜x, X, y)C(ˆy = “spam” | ˜y = “not spam”). Minimizing this expected cost is the optimal action. Note that there is a lot going on here: Expected cost depends on cost and posterior predictive. Posterior predictive depends on predictive and posterior Posterior depends on likelihood and prior. Summary Bayesian statistics: Condition on the data, integrate (rather than maximize) over posterior. “All parameters are nuissance parameters”. Bayesian model averaging and decision theory: Model averaging and decision theory based on rules of probability. Next time: learning the prior?","libVersion":"0.2.1","langs":""}