{"path":".obsidian/plugins/text-extractor/cache/c78d9edd39e5a3e2702f34deecbcbc41.json","text":"CPSC 340: Machine Learning and Data Mining Deep Learning Deep Learning Vocabulary · “Deep learning”: Models with many hidden layers. – Usually neural networks. · “Neuron”: node in the neural network graph. – “ Visible unit ”: feature. – “ Hidden unit ”: latent factor z ic or h(z ic ). · “Activation function”: non- linear transform. · “Activation”: h(z i ). · “Backpropagation”: compute gradient of neural network. – Sometimes “backpropagation” means “ training with SGD ”. · “ Weight decay ”: L2- regularization. · “Cross entropy ”: softmax loss. · “Learning rate”: SGD step- size. · “Learning rate decay ”: using decreasing step- sizes. · “ Vanishing/Exploding gradient ”: gradient becoming real small/big for deep net Stochastic Gradient Training · Standard training method is stochastic gradient (SG): – Choose a random example ‘i ’ (more common: mini-batch of samples) – Use backpropagation to get gradient with respect to all parameters. – Take a small step in the negative gradient direction. · Challenging to make SG work: – Often doesn’t work as a “black box” learning algorithm. – But people have developed a lot of tricks/modifications to make it work. · Highly non- convex , so are the problem local minima? – Some empirical/theoretical evidence that local minima are not the problem. – If the network is “deep” and “wide” enough, we think all local minima are good. – But it can be hard to get SG to close to a local minimum in reasonable time. Ne w I s s u e: V a n i s h i n g G r a d i en t s • Co n s i d e r t h e s i gm o i d fu n c t i o n : • A w a y f r o m t h e o r i g i n , t h e gr a d i en t i s n ea r l y z er o . • Th e p r ob l e m g e ts w or s e wh e n y ou t a k e th e si g m o i d o f a si g m o i d : • I n d e e p n e t w o r k s , m a n y gr a d i en t s c a n b e n ea r l y z er o e v er y w h er e . – A n d n u m e ri c a l l y t h e y w i l l b e s e t t o 0 , s o SGD d oe s n ot m o v e . R e c t i f i e d L i n e a r Un i t s ( Re L U ) • Mo de r n ne t w o r k s o f t e n r e pl a c e si g m o i d w i t h pe r c e p t r o n l o ss ( Re L U ): • J u s t se t s ne g a t i v e v a l ue s z ic t o z e r o . – Re d u c e s va n i s h i n g g r a d i e n t p r o b l e m ( p o s i t i v e r e g i o n i s n e v e r f l a t ) . – G i v e s s p a r s e r a c t i v a t i o n s . – St i l l gi v e s a u n i v e r s a l ap p r o x im a t o r i f s i z e o f h i d d e n l a y e r s g r o w s w i t h ‘n ’ . “Swish” Activiation · Recent work searched for “best” activation: · Found that z ic /(1+exp( -z ic )) worked best (“swish” function). – A bit weird because it allows negative values and is non- monotonic. – But basically the same as ReLU when not close to 0. Sk ip Con n e c t ion s De e p L e ar n in g • S k i p c o nne c t i o ns c a n a l s o r e duc e v a ni s hi ng g r a di e n t pr o bl e m : • Ma k e s “ s ho r t c ut s ” be t w e e n l a y e r s ( s o f e w e r t r a ns f o r m a t i o ns ) . – M a n y v a r i a t i o n s e x i s t o n s k i p c o n n e c t i o n s e x i s t . Re s N e t “B l o c k s ” • Re s i d u a l n e t w o r k s ( Re s N e t s ) ar e a v ar ian t o n s k ip c o n n e c t io n s . – C o n s i s t o f r e p e a t e d “ b l o c k s ” , fi r s t m e t h o d s t h a t s u c c es s fu l l y u s ed 1 0 0 + la y e r s . • Us u a l c o m p u t a t i o n o f a ct i v a t i o n b a s e d o n p r e v i o u s 2 l a y e r s : • Re s N e t “ bl o c k ”: – Ad d s a ct i v a t i on s f r om “2 l a y e r s a g o” . • Di f f e r e n ce s f r o m u s u a l s k i p c o n n e ct i o n s : – Act i v a t i on s v e ct or s a l an d a l+ 2 mu s t h a v e th e sa m e si z e . – No w e ig h t s o n a l , s o W l an d W l+ 1 mu s t f o c u s o n “ u p d a ti n g ” a l (f i t “r e s i d u a l ”). • I f y o u us e Re L U , t he n W l = 0 i m pl i e s a l+ 2 =a l . ht t p s : / / e n . w ik ip e d ia. o r g / w ik i/ R e s id u al_ n e u r al_ n e t w o r k ht t p s : / / t o w a r d s d a t a s c i e n c e . c o m / a n - ov e r v i e w - of - re s n e t - an d - it s - va r i a n t s - 5281e 2f 56035 Parameter Initialization · Parameter initialization is crucial: – Can’t initialize weights in same layer to same value, or units will stay the same. · Architecture is symmetric, so gradient would be the same for every hidden unit in the layer, so they’d all just always stay doing the exact same thing. – Can’t initialize weights too large, it will take too long to learn. · A traditional random initialization: – Initialize bias variables to 0. – Sample from standard normal, divided by 10 5 (0.00001* randn). · w = .00001* randn(k,1) – Performing multiple initializations does not seem to be important (except maybe with very small networks) Parameter Initialization · Also common to transform data in various ways: – Subtract mean, divide by standard deviation, “whiten”, standardize y i . · More recent initializations try to standardize initial z i : – Use different initialization in each layer. – Try to make variance of z i the same across layers. · Popular approach is to sample from standard normal, divide by sqrt(2* nInputs ). – Use samples from uniform distribution on [ - b,b], where Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · One approach: manual “babysitting” of the step-size. – Run SG for a while with a fixed step- size. – Occasionally measure error and plot progress: – If error is not decreasing, decrease step- size. Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · Bias step -size multiplier : use bigger step-size for the bias variables. · Momentum (stochastic version of “heavy-ball” algorithm): – Add term that moves in previous direction: – Usually β t = 0.9. Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Good demo to check out: https://distill.pub /2017/momentum/ Setting the Step - Size · Automatic method to set step size is Bottou trick : 1. Grab a small set of training examples (maybe 5% of total). 2. Do a binary search for a step size that works well on them. 3. Use this step size for a long time (or slowly decrease it from there). · Several recent methods using a step size for each variable : – AdaGrad, RMSprop, Adam (often work better “out of the box”). – Some controversy versus plain stochastic gradient (often with momentum). · SGD can often get lower test error, even though it takes longer and requires more tuning of step- size. · Batch size (number of random examples) also influences results. – Bigger batch sizes often give faster convergence but maybe to worse solutions? · Another recent trick is batch normalization: – Try to “standardize” the hidden units within the random samples as we go. – Held as example of deep learning “alchemy ” (blog post here about deep learning claims). · Sounds science - ey and often works, but little theoretical understanding. Com m on De e p L e ar n in g T r ic k s • Da t a st a n d a r d i z a t i o n ( “ c e n t e r i n g ” a n d “ w h i t e n i n g ”) . • Pa r a m e t e r in it ializ a t io n : “ sm a l l but di f f e r e n t “. – I f w e i n i t i a l i z e a l l p a r a m e t e r s i n t h e l a y e r t o s a m e v a l u e , t h e y s t a y t h e s a m e . – Al s o c om m on t o u s e i n i t i a l i z a t i on s t h a t a r e st a n d a r d i z e d wi th i n l a y e r s . • St e p - si z e se l e c t i o n: “ ba b y si t t i ng “. – U s e b i g g e r st e p - s i z e f o r t h e b i a s v a ri a b l e s , o r d i f f e r e n t f o r e a c h l a y e r . – M e t h o d s t h a t u s e a s t e p s i z e f o r e ac h c o o r d in a t e ( Ad a Gr a d , RM S p r op , Ad a m ). • E a r l y s t op p i n g of th e op ti mi z a ti on b a s e d on v a l i d a ti on a c c u r a c y . • Mo m e n t um : ad d s w e ig h t e d s u m o f p r e vio u s S GD d ir e c t io n s . • Ba t c h n o r m a l i z a t i o n : a d a p ti v e s t a n d a r d i z i n g w i th i n l a y e r s . – O f t e n a l l o w s s i g m o i d a c t i v a t i o n s i n d e e p n e t w o rk s . Com m on De e p L e ar n in g T r ic k s • L2 - re g u l a r i z a t i o n or L1 - re g u l a r i z a t i o n ( “ w e i g h t d e c a y ” ) . – Som e t i m e s wi t h d i f f e r e n t ! fo r e a c h l a y e r . – R e c e n t w o r k s h o w s th i s c a n i n tr o d u c e b a d l o c a l o p ti m a . • Dr o p o u t : r a n d o m l y z e r o e s a c t i v a t i o n s ‘z ’ v a l u e s t o d i s c o u r a g e d e p e n d e n c e . • Re c t i f i e d l i n e a r u n i t s ( Re L U ) a s n o n - l i n e a r t r a n s f o r m a t i o n . – Ma k e s o bj e ct i v e no n - d i f f e r e n t i a b l e , b u t w e n o w k n o w S G D s t i l l c o n v e r g e s i n t h i s s e t t i n g . • R e s i d u a l / s k i p c o n n e c t i o n s : c o n n e c t l a y e r s t o m u l t i p l e p r e v i o u s l a y e r s . – We n o w k n o w t h a t s u c h c o n n e c t i o n s m a k e i t m o r e l i k e l y t o c o n v e r g e t o g o o d m i n i m a . • Ne u r a l a r c h i t e c t u r e s e a r c h : t r y t o c l e v e r l y s e a r c h sp a c e o f h y p e r - pa r a me t e r s . – Th i s g e t s e x p en s i v e! • S o m e o f t h e s e tr ic k s a r e e x p l o r e d i n b o n u s s l i d e s . Mi s s i n g T h e o r y B e h i n d T r a i n i n g D e e p N e tw o rk s • Un f o r t u n a t el y , we d o n o t u n d e r s t a n d m a n y o f t h e s e t r i c k s v e r y w e l l . – La r g e p o r ti o n o f th e o r y i s o n d e g e n e r a t e c a s e o f l i n e a r n e u r a l n e tw o r k s . • Or o t h e r w e i r d c a s e s l i k e “ 1 h i d d e n u n i t p e r l a y e r ” . – A l o t o f r e s e a r c h i s p e r f o r m e d u s i n g “ gr a d s tu d e n t d e sc e n t ”. • S e v e r a l v a r i a t i o n s a r e t r i e d , on e s th a t p e r f o r m w e l l e m p i r i c a l l y a r e ke p t ( p o s s i b l y o v e r f i t t i n g ) . • P o p u l a r E x a m p l e s : – Ba t c h n or m a l i z a t i on or i g i n a l l y p r op ose d t o fi x “ i n t e r n a l c o v a r i a t e sh i ft ” . • I n t e r n a l c o v a r i a t e s h i f t n o t d e f i n e d i n o r i g i n a l p a p e r , a n d b a t c h n o r m d o e s s e e m t o r e d u c e i t . – O f t e n s i ng l e d o ut a s a n e x a m pl e o f pr o bl e m s w i t h m a c hi ne l e a r ni ng s c ho l a r s hi p . • L i k e m a n y h e u r i s t i c s , p e o p l e u s e b a t c h n o rm b e c a u s e th e y f o u n d th a t i t o f t e n h e l p s . – M a n y pe o pl e ha v e w o r k e d o n be t t e r e x pl a na t i o ns . – Ad a m o p ti m i z e r is a n ic e c o m b in a t io n s o f id e as f r o m s e v e r al e x is t in g alg o r it h m s . • Suc h as “ m o m e n t um ” and “ Ad a G r a d ” , b o t h o f w h i c h a r e w e l l - unde r s t o o d t he o r e t ic ally . – B ut t he o r y i n t he o r i g i na l pa pe r w a s i nc o r r e c t , a nd A da m f a i l s a t s o l v i ng s o m e v e r y - s i m pl e o p t i m i z a t i o n pr o bl e m s . • Bu t i s A d a m i s o f t en u s ed b ec a u s e i t i s a m a z i n g a t t r a i n i n g s o m e n e t w o r k s . – It ha s be e n h y po t he s i z e d t ha t w e “ c o n v e r g e d” t o w a r ds ne t w o r k s t ha t a r e e a s i e r f o r c ur r e n t S G D m e t ho ds l i k e A da m . Summary · Unprecedented performance on difficult pattern recognition tasks. · Backpropagation computes neural network gradient via chain rule. · Parameter initialization is crucial to neural net performance. · Optimization and step size are crucial to neural net performance. – “Babysitting”, momentum. · ReLU avoid “vanishing gradients”. · Next: The most important idea in computer vision? Autoencoders · Autoencoders are an unsupervised deep learning model: – Use the inputs as the output of the neural network. – Middle layer could be latent features in non - linear latent-factor model. · Can do outlier detection, data compression, visualization, etc. – A non - linear generalization of PCA. · Equivalent to PCA if you don’t have non- linearities . http://inspirehep.net/record/1252540/plots Autoencoders https://www.cs.toronto.edu/~hinton/science.pdf Denoising Autoencoder · Denoising autoencoders add noise to the input: – Learns a model that can remove the noise. http://inspirehep.net/record/1252540/plots CPSC 340: Machine Learning and Data Mining Convolutions Spring 2022 (2021W2) Convolutional Neural Networks – arguably the most important idea in computer vision Motivation: Automatic Brain Tumor Segmentation · Task: labeling tumors and normal tissue in multi - modal MRI data. Input: Output: · Applications: – Radiation therapy target planning, quantifying treatment responses. – Mining growth patterns, image - guided surgery. · Challenges: – Variety of tumor appearances, similarity to normal tissue. – “You are never going to solve this problem.” Naïve Voxel - Level Classifier · We could treat classifying a voxel as supervised learning: – Standard representation of image: each pixel gets “intensity” between 0 and 255. · We can formulate predicting y i given x i as supervised learning. · But it doesn’t work at all with a linear weighting of these features. Need to Summarize Local Context · The individual pixel intensity values are almost meaningless: – The same x i could lead to different y i . · Intensities not standardized. · Non- trivial overlap in signal for different tissue types. · “Partial volume” effects at boundaries of tissue types. Need to Summarize Local Context · We need to represent the “context” of the pixel (what is around it). – Include all the values of neighbouring pixels as extra features? · Run into coupon collection problems: requires lots of data to find patterns. – Measure neighbourhood summary statistics (mean, variance, histogram)? · Variation on bag of words problem: loses spatial information present in voxels. – Standard approach uses convolutions to represent neighbourhood. Example: Measuring “brightness” of an Area - This pixel is in a “bright” area of the image, which reflects “bleeding” of tumour. - But the actual numeric intensity value of the pixel is the same as in darker “gray matter” areas. - I want a feature saying “this pixel is in a bright area of the image”. - This will us help identify that it’s a tumour pixel. - How to measure brightness in area? Easy way: take average pixel intensity in “neighbourhood”. - Applying this “averaging” to every pixel gives a new image: - We can use “pixel value in new image” as a new feature. - New feature helps identify if pixel is in a “bright” area. The annoying thing about squares · “Take the average of a square window” loses spatial information. · Example: Fixing the “square” issues · Consider instead “blurring” the image. – Gets rid of “local” noise, but better preserves spatial information. · How do you “blur”? – Take weighted average of window, putting more “weight” on “close” pixels: Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution: Big Picture · How do you use convolution to get features? – Apply several different convolutions to your image. – Each convolution gives a different “image” value at each location. – Use theses different image values to give features at each location. Convolutions: Big Picture · What can features coming from convolutions represent? – Some filters give you an average value of the neighbourhood. – Some filters detect edges (directionally) (first derivative) · “Is there a change from dark to bright?” · “If so, from which direction in space?” – Some filters detect lines (“second derivative”) · “Is there a spike or is the change speeding up?” 1D Convolution Example · Consider a 1D “signal” (maybe from sound): – We’ll come back to images later. · For each “time”: – Compute dot- product of signal at surrounding times with a “filter ” of weights. · This gives a new “signal” : – Measures a property of “neighbourhood”. – This particular filter shows a local “how spiky ” value. 1D Convolution (notation is specific to this lecture) · 1D convolution input: – Signal ‘x’ which is a vector length ‘n’. · Indexed by i = 1, 2, …, n – Filter ‘w’ which is a vector of length ‘2m+1’: · Indexed by i = - m, - m+1, …, - 2, 0, 1, 2, …, m - 1, m · Output is a vector of length ‘n’ with elements: – You can think of this as centering w at position ‘ i ’, and taking a dot product of ‘w’ with that “part” x i . 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 -1 1D Convolution Examples · Examples: – “Identity” – “Translation” 1D Convolution Examples · Examples: – “Identity” – “Local Average” Boundary Issue · What can we do about the “?” at the edges ? · Can assign values past the boundaries : · “Zero”: · “Replicate”: · “Mirror”: · Or just ignore the “?” values and return a shorter vector : Formal Convolution Definition · We’ve defined the convolution as: · In other classes you may see it defined as: · For simplicity we’re skipping the “reverse” step , and assuming ‘w’ and ‘x’ are sampled at discrete points (not functions). · But keep this mind if you read about convolutions elsewhere. 1D Convolution Examples · Translation convolution shift signal: – “What is my neighbour’s value?” 1D Convolution Examples · Averaging convolution (“general value of signal in this region?”) – Less sensitive to noise (or spikes) than raw signal. 1D Convolution Examples · Laplacian convolution approximates second derivative: – “Sum to zero” filters “respond” if input vector looks like the filter 1D Convolution Examples · Gaussian convolution (“blurring”): – Compared to averaging it’s more smooth and maintains peaks better. We stopped here Recap of Last Time Deep Learning Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel 1D Convolution Example · Consider a 1D “signal” (maybe from sound): – We’ll come back to images later. · For each “time”: – Compute dot- product of signal at surrounding times with a “filter ” of weights. · This gives a new “signal” : – Measures a property of “neighbourhood”. – This particular filter shows a local “how spiky ” value. 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 -1 1D Convolution Examples · Laplacian convolution approximates second derivative: – “Sum to zero” filters “respond” if input vector looks like the filter 1D Convolution Examples · Gaussian convolution (“blurring”): – Compared to averaging it’s more smooth and maintains peaks better. 1D Convolution Examples · Centered difference convolution approximates first derivative: – Positive means change from low to high (negative means high to low). 1D Convolution Examples · Sharpen convolution enhances peaks. – An “average” that places negative weights on the surrounding pixels. Digression: Derivatives and Integrals · Numerical derivative approximations can be viewed as filters: – Centered difference: [ - 1, 0, 1] (like check_correctness in the homework code) · Numerical integration approximations can be viewed as filters: – “Simpson’s” rule: [1/6, 4/6, 1/6] (a bit like Gaussian filter). · Derivative filters add to 0 , integration filters add to 1 – For constant function, derivative should be 0 and average = constant. 34 Laplacian of Gaussian Filter · Laplacian of Gaussian is a smoothed 2 nd -derivative approximation: Images and Higher- Order Convolution · 2D convolution: – Signal ‘x’ is the pixel intensities in an ‘n’ by ‘n’ image. – Filter ‘w’ is the pixel intensities in a ‘2m+1’ by ‘2m+1’ image. · The 2D convolution is given by: · 3D and higher-order convolutions are defined similarly. https://towardsdatascience.com/intuitively-understanding-convolutions -for-deep-learning-1f6f42faee1 Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples http://setosa.io/ev/image -kernels Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples 3D Convolution 3D Convolution 3D Convolution 3D Convolution 3D Convolution Convolutions as Features · Classic vision methods use convolutions as features : – Usually have different types/variances/orientations. – Can take maxes across locations/orientations/scales. · Notable convolutions: – Gaussian (blurring/averaging). – Laplace of Gaussian (second- derivative). – Gabor filters (directional first- or higher- derivative). Filter Banks · To characterize context, we used to use filter banks like “MR8”: – 1 Gaussian filter, 1 Laplacian of Gaussian filter. – 6 max(abs(Gabor)) filters: · 3 scales of sine/cosine (maxed over 6 orientations). · Convolutional neural networks (next time!) are replacing filter banks. http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html Summary · Convolutions are flexible class of signal/image transformations. – Can detect edges (approximate directional derivatives) at different scales – Max(convolutions) can yield features invariant to some transformations. · Filter banks : – Make features for a vision problem by taking a bunch of convolutions. · Next: – Combining this with deep learning. Global and Local Features for Domain Adaptation · Suppose you want to solve a classification task, where you have very little labeled data from your domain. · But you have access to a huge dataset with the same labels, from a different domain. · Example: – You want to label POS tags in medical articles, and pay a few $$$ to label some. – You have access the thousands of examples of Wall Street Journal POS labels. · Domain adaptation: using data from different domain to help. Global and Local Features for Domain Adaptation · “Frustratingly easy domain adaptation”: – Use “global” features across the domains, and “local” features for each domain. – “Global” features let you learn patterns that occur across domains. · Leads to sensible predictions for new domains without any data. – “Local” features let you learn patterns specific to each domain. · Improves accuracy on particular domains where you have more data. – For linear classifiers this would look like: Image Coordinates · Should we use the image coordinates? – E.g., the pixel is at location (124, 78) in the image. · Considerations: – Is the interpretation different in different areas of the image? – Are you using a linear model? · Would “distance to center” be more logical? – Do you have enough data to learn about all areas of the image? Alignment- Based Features · The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. · We aligned the images with a “template image”. Alignment- Based Features · The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. · We aligned the images with a “template image”. – Allowed “alignment- based” features: Motivation: Automatic Brain Tumor Segmentation · Final features for brain tumour segmentation: – Gaussian convolution of original/template/priors/symmetry, Laplacian of Gaussian on original. · All with 3 variances. · Max(Gabor) with sine and cosine on orginal (3 variances). Motivation: Automatic Brain Tumour Segmentation · Logistic regression and SVMs among best methods. – When using these 72 features from last slide. – If you used all features I came up with, it overfit . · Possible solutions to overfitting: – Forward selection was too slow. · Just one image gives 8 million training examples. – I did manual feature selection (“guess and check”). – L2- regularization with all features also worked. · But this is slow at test time . · L1- regularization gives best of regularization and feature selection. FFT implementation of convolution · Convolutions can be implemented using fast Fourier transform: – Take FFT of image and filter, multiply elementwise, and take inverse FFT. · It has faster asymptotic running time but there are some catches: – You need to be using periodic boundary conditions for the convolution. – Constants matter: it may not be faster in practice. · Especially compared to using GPUs to do the convolution in hardware. – The gains are largest for larger filters (compared to the image size). 74 SIFT Features · Scale-invariant feature transform (SIFT): – Features used for object detection (“is particular object in the image”?) – Designed to detect unique visual features of objects at multiple scales. – Proven useful for a variety of object detection tasks. http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html CPSC 340: Machine Learning and Data Mining Convolutional Neural Networks (and miscellaneous deep learning tricks) Spring 2022 (2021W2) 1D Convolution as Matrix Multiplication · 1D convolution: – Takes signal ‘x’ and filter ‘w’ to produces vector ‘z’: – Can be written as a matrix multiplication: Typo: the signs of -1 and 2 are wrong in the matrix 1D Convolution as Matrix Multiplication · Each element of a convolution is an inner product: · So convolution is a matrix multiplication (I’m ignoring boundaries): · The shorter ‘w’ is, the more sparse the matrix is. 2D Convolution as Matrix Multiplication · 2D convolution: – Signal ‘x’, filter ‘w’, and output ‘z’ are now all images/matrices: – Vectorized ‘z’ can be written as a matrix multiplication with vectorized ‘x’: Motivation for Convolutional Neural Networks · Consider training neural networks on 256 by 256 images. – This is 256 by 256 by 3 ≈ 200,000 inputs. · If first layer has k=10,000, then it has about 2 billion parameters . – We want to avoid this huge number (due to storage and overfitting). · Key idea: make Wx i act like several convolutions (to make it sparse): 1. Each row of W only applies to part of x i . 2. Use the same parameters between rows. · Forces most weights to be zero, reduces number of parameters. Motivation for Convolutional Neural Networks · Classic vision methods uses fixed convolutions as features: – Usually have different types/variances/orientations. – Can do subsampling or take maxes across locations/orientations/scales. Motivation for Convolutional Neural Networks · Convolutional neural networks learn the convolutions: – Learning ‘W’ and ‘v’ automatically chooses types/variances/orientations. – Don’t pick from fixed convolutions, but learn the elements of the filters. Motivation for Convolutional Neural Networks · Convolutional neural networks learn the convolutions: – Learning ‘W’ and ‘v’ automatically chooses types/variances/orientations. – Can do multiple layers of convolution to get deep hierarchical features. http://fortune.com/ai-artificial-intelligence -deep-machine -learning/ Two Main Motivations · Translation invariance (data -efficient to learn, less likely to overfit) · Hierarchy Hierarchically composed feature representations Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. – Pooling layer : combine results of convolutions. · Can add some invariance or just make the number of parameters smaller. · Often ‘max pooling ’: Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. – Pooling layer : combine results of convolutions. · Can add some invariance or just make the number of parameters smaller. · Often ‘max pooling’ or else ‘average pooling ’: Max Pooling vs Average Pooling · Both downsample the image · Max pooling: “any of these options is present” – Much more common, especially in early layers – “There’s an edge here, but I don’t really care how thick it is” · Average pooling: “all/most of these options are present” – If used, more often at the end of the network – “Most of the big patches look like a picture of a train“ LeNet for Optical Character Recognition http://blog.csdn.net/strint/article/details/44163869 Deep Hierarchies in the Visual System http://www.strokenetwork.org/newsletter/articles/vision.htm https://en.wikibooks.org/wiki/Sensory_Systems/Visual_Signal_Processing Deep Hierarchies in Optics http://www.argmin.net/2018/01/25/optics/ We stopped here","libVersion":"0.2.1","langs":""}