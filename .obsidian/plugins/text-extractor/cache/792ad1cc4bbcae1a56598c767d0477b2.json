{"path":".obsidian/plugins/text-extractor/cache/792ad1cc4bbcae1a56598c767d0477b2.json","text":"CPSC 340: Machine Learning and Data Mining Linear Classifiers Last Time: L1-Regularization â€¢ We discussed L1-regularization: â€“ Also known as â€œLASSOâ€ and â€œbasis pursuit denoisingâ€. â€“ Regularizes â€˜wâ€™ so we decrease our test error (like L2-regularization). â€“ Yields sparse â€˜wâ€™ so it selects features (like L0-regularization). â€¢ Properties: â€“ Itâ€™s convex and fast to minimize (with â€œproximal-gradientâ€ methods). â€“ Solution is not unique (sometimes people do L2- and L1-regularization). â€“ Usually includes â€œcorrectâ€ variables but tends to yield false positives. L1-loss vs. L1-regularization â€¢ Donâ€™t confuse the L1 loss with L1-regularization! â€“ L1-loss is robust to outlier data points. â€¢ You can use this instead of removing outliers. â€“ L1-regularization is robust to irrelevant features. â€¢ You can use this instead of removing features. â€¢ And note that you can be robust to outliers and irrelevant features: â€¢ Can we smooth and use â€œHuber regularizationâ€? â€“ Huber regularizer is still robust to irrelevant features. â€“ But itâ€™s the non-smoothness that sets weights to exactly 0. 3 L*-Regularization â€¢ L0-regularization (AIC, BIC, Mallowâ€™s Cp, Adjusted R2, ANOVA): â€“ Adds penalty on the number of non-zeros to select features. â€¢ L2-regularization (ridge regression): â€“ Adding penalty on the L2-norm of â€˜wâ€™ to decrease overfitting: â€¢ L1-regularization (LASSO): â€“ Adding penalty on the L1-norm decreases overfitting and selects features: L0- vs. L1- vs. L2-Regularization Sparse â€˜wâ€™ (Selects Features) Speed Unique â€˜wâ€™ Coding Effort Irrelevant Features L0-Regularization Yes Slow No Few lines Not Sensitive L1-Regularization Yes* Fast* No 1 line* Not Sensitive L2-Regularization No Fast Yes 1 line A bit sensitive â€¢ L1-Regularization isnâ€™t as sparse as L0-regularization. â€“ L1-regularization tends to give more false positives (selects too many). â€“ And itâ€™s only â€œfastâ€ and â€œ1 lineâ€ with specialized solvers. â€¢ Cost of L2-regularized least squares is O(nd2 + d3). â€“ Changes to O(ndt) for â€˜tâ€™ iterations of gradient descent (same for L1). â€¢ â€œElastic netâ€ (L1- and L2-regularization) is sparse, fast, and unique. â€¢ Using L0+L2 does not give a unique solution. Ensemble Feature Selection â€¢ We can also use ensemble methods for feature selection. â€“ Usually designed to reduce false positives or reduce false negatives. â€¢ In this case of L1-regularization, we want to reduce false positives. â€“ Unlike L0-regularization, the non-zero wj are still â€œshrunkâ€. â€¢ â€œIrrelevantâ€ variables can be included before â€œrelevantâ€ wj reach best value. â€¢ A bootstrap approach to reducing false positives: â€“ Apply the method to bootstrap samples of the training data. â€“ Only take the features selected in all bootstrap samples. Ensemble Feature Selection â€¢ Example: bootstrapping plus L1-regularization (â€œBoLASSOâ€). â€“ Reduces false positives. â€“ Itâ€™s possible to show it recovers â€œcorrectâ€ variables with weaker conditions. â€¢ Can replace â€œintersectionâ€ with â€œselected frequencyâ€ if has false negatives too. Next Topic: Linear Classifiers Motivation: Identifying Important E-mails â€¢ How can we automatically identify â€˜importantâ€™ e-mails? â€¢ A binary classification problem (â€œimportantâ€ vs. â€œnot importantâ€). â€“ Labels are approximated by whether you took an â€œactionâ€ based on mail. â€“ High-dimensional feature set (that weâ€™ll discuss later). â€¢ Gmail uses regression for this binary classification problem. Binary Classification Using Regression? â€¢ Can we apply linear models for binary classification? â€“ Set yi = +1 for one class (â€œimportantâ€). â€“ Set yi = -1 for the other class (â€œnot importantâ€). â€¢ At training time, fit a linear regression model: â€“ And try to minimize squared error between output oi and labels yi. â€¢ The model will try to make: â€“ Output oi = wTxi = +1 for â€œimportantâ€ e-mails, â€“ Output oi = wTxi = -1 for â€œnot importantâ€ e-mails. Binary Classification Using Regression? â€¢ Can we apply linear models for binary classification? â€“ Set yi = +1 for one class (â€œimportantâ€). â€“ Set yi = -1 for the other class (â€œnot importantâ€). â€¢ Linear model gives real numbers like 0.9, -1.1, and so on. â€¢ So to predict, we look at whether oi = wTxi is closer to +1 or -1. â€“ If oi = 0.9, predict à·œð‘¦i = +1. â€“ If oi = -1.1, predict à·œð‘¦i = -1. â€“ If oi = 0.1, predict à·œð‘¦i = +1. â€“ If oi = -100, predict à·œð‘¦i = -1. â€“ We write this operation (rounding to +1 or -1) as à·œð‘¦i = sign(oi). Decision Boundary in 1D â€¢ We can interpret â€˜wâ€™ as a hyperplane separating x into sets: â€“ Set where wTxi > 0 and set where wTxi < 0. Decision Boundary in 1DDecision Boundary in 2D decision tree KNN linear classifier â€¢ Linear classifier would be a oi= wTxi function coming out of screen: â€“ And the boundary is at oi=wTxi=0. Should we use least squares for classification? â€¢ Consider training by minimizing squared error with yi that are +1 or -1: â€¢ If we predict oi = +0.9 and yi = +1, error is small: (0.9 â€“ 1)2 = 0.01. â€¢ If we predict oi = -0.8 and yi = +1, error is bigger: (-0.8 â€“ 1)2 = 3.24. â€¢ If we predict oi = +100 and yi = +1, error is huge: (100 â€“ 1)2 = 9801. â€“ But it shouldnâ€™t be, the prediction is correct. â€¢ Least squares penalized for being â€œtoo rightâ€. â€“ +100 has the right sign, so the error should not be large. Should we use least squares for classification? â€¢ Least squares can behave weirdly when applied to classification: â€¢ Why? Squared error of green line is huge! â€“ Make sure you understand why the green line achieves 0 training error. â€œ0-1 Lossâ€ Function: Minimizing Classification Errors â€¢ Could we instead minimize number of classification errors? â€“ This is called the 0-1 loss function: â€¢ You either get the classification wrong (error of 1) or right (error of 0). â€“ We can write using the L0-norm as ||sign(oi)â€“ y||0. â€¢ Unlike regression, in classification itâ€™s reasonable we exactly match yi (itâ€™s +1 or -1). â€¢ Important special case: â€œlinearly separableâ€ data. â€“ Classes can be â€œseparatedâ€ by a hyper-plane. â€“ So a perfect linear classifier exists. Perceptron Algorithm for Linearly-Separable Data â€¢ One of the first â€œlearningâ€ algorithms was the â€œperceptronâ€ (1957). â€“ Searches for a â€˜wâ€™ such that sign(wTxi) = yi for all i. â€¢ Perceptron algorithm: â€“ Start with w0 = 0. â€“ Go through examples in any order until you make a mistake predicting yi. â€¢ Set wt+1 = wt + yixi. â€“ Keep going through examples until you make no errors on training data. â€¢ If a perfect classifier exists, this algorithm finds one in finite number of steps. â€¢ Intuition: â€“ Consider a case where wTxi < 0 but yi = +1. â€“ In this case the update â€œadds more of xi to wâ€ so that wTxi is larger. â€“ If yi = -1, you would be subtracting the squared norm. https://en.wikipedia.org/wiki/Perceptron Geometry of why we want the 0-1 lossThoughts on the previous (and next) slide â€¢ We are now plotting the loss vs. the output oi. â€“ â€œLoss spaceâ€, which is different than parameter space or data space. â€¢ We're plotting the individual loss for a particular training example. â€“ In the figure the label is yi = âˆ’1 (so loss is centered at -1). â€¢ It will be centered at +1 when yi = +1. â€“ The objective in least squares regression is a sum of â€˜nâ€™ of these losses: â€¢ (The next slide is the same as the previous one) Geometry of why we want the 0-1 lossGeometry of why we want the 0-1 lossGeometry of why we want the 0-1 loss0-1 Loss Function â€¢ Unfortunately the 0-1 loss is non-convex in â€˜wâ€™. â€“ It is easy to minimize if a perfect classifier exists (perceptron). â€“ Otherwise, finding the â€˜wâ€™ minimizing 0-1 loss is a hard problem. â€“ Gradient is zero everywhere: donâ€™t even know â€œwhich way to goâ€. â€“ NOT the same type of problem we had with using the squared loss. â€¢ We can minimize the squared error, but it might give a bad model for classification. â€¢ Motivates convex approximations to 0-1 loss. â€“ The two most common are the â€œhingeâ€ loss and the â€œlogisticâ€ loss. Summary â€¢ Ensemble feature selection reduces false positives or negatives. â€¢ Binary classification using regression: â€“ Encode using yi in {-1,1}. â€“ Use output oi = wTxi, and make predictions using sign(oi). â€“ â€œLinear classifierâ€ (a hyperplane splitting the space in half). â€¢ Least squares is a weird error for classification. â€¢ Perceptron algorithm: finds a perfect classifier (if one exists). â€¢ 0-1 loss is the ideal loss, but is non-smooth and non-convex. â€¢ Next time: one of the best â€œout of the boxâ€ classifiers. 26 L1-Regularization as a Feature Selection Method â€¢ Advantages: â€“ Deals with conditional independence (if linear). â€“ Sort of deals with collinearity: â€¢ Picks at least one of â€œmomâ€ and â€œmom2â€. â€“ Very fast with specialized algorithms. â€¢ Disadvantages: â€“ Tends to give false positives (selects too many variables). â€¢ Neither good nor bad: â€“ Does not take small effects. â€“ Says â€œgenderâ€ is relevant if we know â€œbabyâ€. â€“ Good for prediction if we want fast training and donâ€™t care about having some irrelevant variables included. â€œElastic Netâ€: L2- and L1-Regularization â€¢ To address non-uniqueness, some authors use L2- and L1-: â€¢ Called â€œelastic netâ€ regularization. â€“ Solution is sparse and unique. â€“ Slightly better with feature dependence: â€¢ Selects both â€œmomâ€ and â€œmom2â€. â€¢ Optimization is easier though still non-differentiable. L1-Regularization Debiasing and Filtering â€¢ To remove false positives, some authors add a debiasing step: â€“ Fit â€˜wâ€™ using L1-regularization. â€“ Grab the non-zero values of â€˜wâ€™ as the â€œrelevantâ€ variables. â€“ Re-fit relevant â€˜wâ€™ using least squares or L2-regularized least squares. â€¢ A related use of L1-regularization is as a filtering method: â€“ Fit â€˜wâ€™ using L1-regularization. â€“ Grab the non-zero values of â€˜wâ€™ as the â€œrelevantâ€ variables. â€“ Run standard (slow) variable selection restricted to relevant variables. â€¢ Forward selection, exhaustive search, stochastic local search, etc. Non-Convex Regularizers â€¢ Regularizing |wj|2 selects all features. â€¢ Regularizing |wj| selects fewer, but still has many false positives. â€¢ What if we regularize |wj|1/2 instead? â€¢ Minimizing this objective would lead to fewer false positives. â€“ Less need for debiasing, but itâ€™s not convex and hard to minimize. â€¢ There are many non-convex regularizers with similar properties. â€“ L1-regularization is (basically) the â€œmost sparseâ€ convex regularizer. Can we just use least squares?? â€¢ What went wrong? â€“ â€œGoodâ€ errors vs. â€œbadâ€ errors. Can we just use least squares?? â€¢ What went wrong? â€“ â€œGoodâ€ errors vs. â€œbadâ€ errors. Feature Selection Hierarchy â€¢ Consider a linear models with higher-order terms, â€¢ The number of higher-order terms may be too large. â€“ Canâ€™t even compute them all. â€“ We need to somehow decide which terms weâ€™ll even consider. â€¢ Consider the following hierarchical constraint: â€“ You only allow w12 â‰  0 if w1 â‰  0 and w2 â‰  0. â€“ â€œOnly consider feature interaction if you are using both features already.â€ Hierarchical Forward Selection â€¢ Hierarchical Forward Selection: â€“ Usual forward selection, but consider interaction terms obeying hierarchy. â€“ Only consider w12 â‰  0 once w1 â‰  0 and w2 â‰  0. â€“ Only allow w123 â‰  0 once w12 â‰  0 and w13 â‰  0 and w23 â‰  0. â€“ Only allow w1234 â‰  0 once all threeway interactions are present. http://arxiv.org/pdf/1109.2397v2.pdf Online Classification with Perceptron â€¢ Perceptron for online linear binary classification [Rosenblatt, 1957] â€“ Start with w0 = 0. â€“ At time â€˜tâ€™ we receive features xt. â€“ We predict à·œð‘¦t = sign(wtTxt). â€“ If à·œð‘¦t â‰  yt, then set wt+1 = wt + ytxt. â€¢ Otherwise, set wt+1 = wt. (Slides are old so above Iâ€™m using subscripts of â€˜tâ€™ instead of superscripts.) â€¢ Perceptron mistake bound [Novikoff, 1962]: â€“ Assume data is linearly-separable with a â€œmarginâ€: â€¢ There exists w* with ||w*||=1 such that sign(xt Tw*) = sign(yt) for all â€˜tâ€™ and |xTw*| â‰¥ Î³. â€“ Then the number of total mistakes is bounded. â€¢ No requirement that data is IID. Perceptron Mistake Bound â€¢ Letâ€™s normalize each xt so that ||xt|| = 1. â€“ Length doesnâ€™t change label. â€¢ Whenever we make a mistake, we have sign(yt) â‰  sign(wtTxt) and â€¢ So after â€˜kâ€™ errors we have ||wt||2 â‰¤ k. Perceptron Mistake Bound â€¢ Letâ€™s consider a solution w*, so sign(yt) = sign(xt Tw*). â€“ And letâ€™s choose a w* with ||w*|| = 1, â€¢ Whenever we make a mistake, we have: â€“ Note: wtTw* â‰¥ 0 by induction (starts at 0, then at least as big as old value plus Î³). â€¢ So after â€˜kâ€™ mistakes we have ||wt|| â‰¥ Î³k. Perceptron Mistake Bound â€¢ So our two bounds are ||wt|| â‰¤ sqrt(k) and ||wt|| â‰¥ Î³k. â€¢ This gives Î³k â‰¤ sqrt(k), or a maximum of 1/Î³2 mistakes. â€“ Note that Î³ > 0 by assumption and is upper-bounded by one by ||x|| â‰¤ 1. â€“ After this â€˜kâ€™, under our assumptions weâ€™re guaranteed to have a perfect classifier.","libVersion":"0.2.1","langs":""}