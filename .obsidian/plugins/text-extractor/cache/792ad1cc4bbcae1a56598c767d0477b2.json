{"path":".obsidian/plugins/text-extractor/cache/792ad1cc4bbcae1a56598c767d0477b2.json","text":"CPSC 340: Machine Learning and Data Mining Linear Classifiers Last Time: L1-Regularization • We discussed L1-regularization: – Also known as “LASSO” and “basis pursuit denoising”. – Regularizes ‘w’ so we decrease our test error (like L2-regularization). – Yields sparse ‘w’ so it selects features (like L0-regularization). • Properties: – It’s convex and fast to minimize (with “proximal-gradient” methods). – Solution is not unique (sometimes people do L2- and L1-regularization). – Usually includes “correct” variables but tends to yield false positives. L1-loss vs. L1-regularization • Don’t confuse the L1 loss with L1-regularization! – L1-loss is robust to outlier data points. • You can use this instead of removing outliers. – L1-regularization is robust to irrelevant features. • You can use this instead of removing features. • And note that you can be robust to outliers and irrelevant features: • Can we smooth and use “Huber regularization”? – Huber regularizer is still robust to irrelevant features. – But it’s the non-smoothness that sets weights to exactly 0. 3 L*-Regularization • L0-regularization (AIC, BIC, Mallow’s Cp, Adjusted R2, ANOVA): – Adds penalty on the number of non-zeros to select features. • L2-regularization (ridge regression): – Adding penalty on the L2-norm of ‘w’ to decrease overfitting: • L1-regularization (LASSO): – Adding penalty on the L1-norm decreases overfitting and selects features: L0- vs. L1- vs. L2-Regularization Sparse ‘w’ (Selects Features) Speed Unique ‘w’ Coding Effort Irrelevant Features L0-Regularization Yes Slow No Few lines Not Sensitive L1-Regularization Yes* Fast* No 1 line* Not Sensitive L2-Regularization No Fast Yes 1 line A bit sensitive • L1-Regularization isn’t as sparse as L0-regularization. – L1-regularization tends to give more false positives (selects too many). – And it’s only “fast” and “1 line” with specialized solvers. • Cost of L2-regularized least squares is O(nd2 + d3). – Changes to O(ndt) for ‘t’ iterations of gradient descent (same for L1). • “Elastic net” (L1- and L2-regularization) is sparse, fast, and unique. • Using L0+L2 does not give a unique solution. Ensemble Feature Selection • We can also use ensemble methods for feature selection. – Usually designed to reduce false positives or reduce false negatives. • In this case of L1-regularization, we want to reduce false positives. – Unlike L0-regularization, the non-zero wj are still “shrunk”. • “Irrelevant” variables can be included before “relevant” wj reach best value. • A bootstrap approach to reducing false positives: – Apply the method to bootstrap samples of the training data. – Only take the features selected in all bootstrap samples. Ensemble Feature Selection • Example: bootstrapping plus L1-regularization (“BoLASSO”). – Reduces false positives. – It’s possible to show it recovers “correct” variables with weaker conditions. • Can replace “intersection” with “selected frequency” if has false negatives too. Next Topic: Linear Classifiers Motivation: Identifying Important E-mails • How can we automatically identify ‘important’ e-mails? • A binary classification problem (“important” vs. “not important”). – Labels are approximated by whether you took an “action” based on mail. – High-dimensional feature set (that we’ll discuss later). • Gmail uses regression for this binary classification problem. Binary Classification Using Regression? • Can we apply linear models for binary classification? – Set yi = +1 for one class (“important”). – Set yi = -1 for the other class (“not important”). • At training time, fit a linear regression model: – And try to minimize squared error between output oi and labels yi. • The model will try to make: – Output oi = wTxi = +1 for “important” e-mails, – Output oi = wTxi = -1 for “not important” e-mails. Binary Classification Using Regression? • Can we apply linear models for binary classification? – Set yi = +1 for one class (“important”). – Set yi = -1 for the other class (“not important”). • Linear model gives real numbers like 0.9, -1.1, and so on. • So to predict, we look at whether oi = wTxi is closer to +1 or -1. – If oi = 0.9, predict ො𝑦i = +1. – If oi = -1.1, predict ො𝑦i = -1. – If oi = 0.1, predict ො𝑦i = +1. – If oi = -100, predict ො𝑦i = -1. – We write this operation (rounding to +1 or -1) as ො𝑦i = sign(oi). Decision Boundary in 1D • We can interpret ‘w’ as a hyperplane separating x into sets: – Set where wTxi > 0 and set where wTxi < 0. Decision Boundary in 1DDecision Boundary in 2D decision tree KNN linear classifier • Linear classifier would be a oi= wTxi function coming out of screen: – And the boundary is at oi=wTxi=0. Should we use least squares for classification? • Consider training by minimizing squared error with yi that are +1 or -1: • If we predict oi = +0.9 and yi = +1, error is small: (0.9 – 1)2 = 0.01. • If we predict oi = -0.8 and yi = +1, error is bigger: (-0.8 – 1)2 = 3.24. • If we predict oi = +100 and yi = +1, error is huge: (100 – 1)2 = 9801. – But it shouldn’t be, the prediction is correct. • Least squares penalized for being “too right”. – +100 has the right sign, so the error should not be large. Should we use least squares for classification? • Least squares can behave weirdly when applied to classification: • Why? Squared error of green line is huge! – Make sure you understand why the green line achieves 0 training error. “0-1 Loss” Function: Minimizing Classification Errors • Could we instead minimize number of classification errors? – This is called the 0-1 loss function: • You either get the classification wrong (error of 1) or right (error of 0). – We can write using the L0-norm as ||sign(oi)– y||0. • Unlike regression, in classification it’s reasonable we exactly match yi (it’s +1 or -1). • Important special case: “linearly separable” data. – Classes can be “separated” by a hyper-plane. – So a perfect linear classifier exists. Perceptron Algorithm for Linearly-Separable Data • One of the first “learning” algorithms was the “perceptron” (1957). – Searches for a ‘w’ such that sign(wTxi) = yi for all i. • Perceptron algorithm: – Start with w0 = 0. – Go through examples in any order until you make a mistake predicting yi. • Set wt+1 = wt + yixi. – Keep going through examples until you make no errors on training data. • If a perfect classifier exists, this algorithm finds one in finite number of steps. • Intuition: – Consider a case where wTxi < 0 but yi = +1. – In this case the update “adds more of xi to w” so that wTxi is larger. – If yi = -1, you would be subtracting the squared norm. https://en.wikipedia.org/wiki/Perceptron Geometry of why we want the 0-1 lossThoughts on the previous (and next) slide • We are now plotting the loss vs. the output oi. – “Loss space”, which is different than parameter space or data space. • We're plotting the individual loss for a particular training example. – In the figure the label is yi = −1 (so loss is centered at -1). • It will be centered at +1 when yi = +1. – The objective in least squares regression is a sum of ‘n’ of these losses: • (The next slide is the same as the previous one) Geometry of why we want the 0-1 lossGeometry of why we want the 0-1 lossGeometry of why we want the 0-1 loss0-1 Loss Function • Unfortunately the 0-1 loss is non-convex in ‘w’. – It is easy to minimize if a perfect classifier exists (perceptron). – Otherwise, finding the ‘w’ minimizing 0-1 loss is a hard problem. – Gradient is zero everywhere: don’t even know “which way to go”. – NOT the same type of problem we had with using the squared loss. • We can minimize the squared error, but it might give a bad model for classification. • Motivates convex approximations to 0-1 loss. – The two most common are the “hinge” loss and the “logistic” loss. Summary • Ensemble feature selection reduces false positives or negatives. • Binary classification using regression: – Encode using yi in {-1,1}. – Use output oi = wTxi, and make predictions using sign(oi). – “Linear classifier” (a hyperplane splitting the space in half). • Least squares is a weird error for classification. • Perceptron algorithm: finds a perfect classifier (if one exists). • 0-1 loss is the ideal loss, but is non-smooth and non-convex. • Next time: one of the best “out of the box” classifiers. 26 L1-Regularization as a Feature Selection Method • Advantages: – Deals with conditional independence (if linear). – Sort of deals with collinearity: • Picks at least one of “mom” and “mom2”. – Very fast with specialized algorithms. • Disadvantages: – Tends to give false positives (selects too many variables). • Neither good nor bad: – Does not take small effects. – Says “gender” is relevant if we know “baby”. – Good for prediction if we want fast training and don’t care about having some irrelevant variables included. “Elastic Net”: L2- and L1-Regularization • To address non-uniqueness, some authors use L2- and L1-: • Called “elastic net” regularization. – Solution is sparse and unique. – Slightly better with feature dependence: • Selects both “mom” and “mom2”. • Optimization is easier though still non-differentiable. L1-Regularization Debiasing and Filtering • To remove false positives, some authors add a debiasing step: – Fit ‘w’ using L1-regularization. – Grab the non-zero values of ‘w’ as the “relevant” variables. – Re-fit relevant ‘w’ using least squares or L2-regularized least squares. • A related use of L1-regularization is as a filtering method: – Fit ‘w’ using L1-regularization. – Grab the non-zero values of ‘w’ as the “relevant” variables. – Run standard (slow) variable selection restricted to relevant variables. • Forward selection, exhaustive search, stochastic local search, etc. Non-Convex Regularizers • Regularizing |wj|2 selects all features. • Regularizing |wj| selects fewer, but still has many false positives. • What if we regularize |wj|1/2 instead? • Minimizing this objective would lead to fewer false positives. – Less need for debiasing, but it’s not convex and hard to minimize. • There are many non-convex regularizers with similar properties. – L1-regularization is (basically) the “most sparse” convex regularizer. Can we just use least squares?? • What went wrong? – “Good” errors vs. “bad” errors. Can we just use least squares?? • What went wrong? – “Good” errors vs. “bad” errors. Feature Selection Hierarchy • Consider a linear models with higher-order terms, • The number of higher-order terms may be too large. – Can’t even compute them all. – We need to somehow decide which terms we’ll even consider. • Consider the following hierarchical constraint: – You only allow w12 ≠ 0 if w1 ≠ 0 and w2 ≠ 0. – “Only consider feature interaction if you are using both features already.” Hierarchical Forward Selection • Hierarchical Forward Selection: – Usual forward selection, but consider interaction terms obeying hierarchy. – Only consider w12 ≠ 0 once w1 ≠ 0 and w2 ≠ 0. – Only allow w123 ≠ 0 once w12 ≠ 0 and w13 ≠ 0 and w23 ≠ 0. – Only allow w1234 ≠ 0 once all threeway interactions are present. http://arxiv.org/pdf/1109.2397v2.pdf Online Classification with Perceptron • Perceptron for online linear binary classification [Rosenblatt, 1957] – Start with w0 = 0. – At time ‘t’ we receive features xt. – We predict ො𝑦t = sign(wtTxt). – If ො𝑦t ≠ yt, then set wt+1 = wt + ytxt. • Otherwise, set wt+1 = wt. (Slides are old so above I’m using subscripts of ‘t’ instead of superscripts.) • Perceptron mistake bound [Novikoff, 1962]: – Assume data is linearly-separable with a “margin”: • There exists w* with ||w*||=1 such that sign(xt Tw*) = sign(yt) for all ‘t’ and |xTw*| ≥ γ. – Then the number of total mistakes is bounded. • No requirement that data is IID. Perceptron Mistake Bound • Let’s normalize each xt so that ||xt|| = 1. – Length doesn’t change label. • Whenever we make a mistake, we have sign(yt) ≠ sign(wtTxt) and • So after ‘k’ errors we have ||wt||2 ≤ k. Perceptron Mistake Bound • Let’s consider a solution w*, so sign(yt) = sign(xt Tw*). – And let’s choose a w* with ||w*|| = 1, • Whenever we make a mistake, we have: – Note: wtTw* ≥ 0 by induction (starts at 0, then at least as big as old value plus γ). • So after ‘k’ mistakes we have ||wt|| ≥ γk. Perceptron Mistake Bound • So our two bounds are ||wt|| ≤ sqrt(k) and ||wt|| ≥ γk. • This gives γk ≤ sqrt(k), or a maximum of 1/γ2 mistakes. – Note that γ > 0 by assumption and is upper-bounded by one by ||x|| ≤ 1. – After this ‘k’, under our assumptions we’re guaranteed to have a perfect classifier.","libVersion":"0.2.1","langs":""}