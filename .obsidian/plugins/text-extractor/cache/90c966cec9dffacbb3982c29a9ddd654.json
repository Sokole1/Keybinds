{"path":".obsidian/plugins/text-extractor/cache/90c966cec9dffacbb3982c29a9ddd654.json","text":"CPSC 340: Machine Learning and Data Mining Deep Learning Recap of Last Time Deep Learning Hierarchically composed feature representations “Hierarchies of Parts” Motivation for Deep Learning · Each “neuron” might recognize a “part” of a digit. – “Deeper” neurons might recognize combinations of parts . – Represent complex objects as hierarchical combinations of re -useable parts (a simple “grammar”). · Watch the full video here: – https://www.youtube.com/watch?v=aircAruvnKk · Theory: – 1 big -enough hidden layer already gives universal approximation. – But some functions require exponentially -fewer parameters to approximate with more layers (can fight curse of dimensionality). ML and Deep Learning History · 1950 and 1960s: Initial excitement. – Perceptron : linear classifier and stochastic gradient (roughly). – “the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.” New York Times (1958). · https://www.youtube.com/watch?v=IEFRtz68m-8 – Object recognition assigned to students as a summer project · Then drop in popularity: – Quickly realized limitations of linear models . https://mitpress.mit.edu/books/perceptrons/ ML and Deep Learning History · 1970 and 1980s: Connectionism (brain -inspired ML) – Want “connected networks of simple units ”. · Use parallel computation and distributed representations. – Adding hidden layers z i increases expressive power. · With 1 layer and enough sigmoid units, a universal approximator. – Success in optical character recognition. https://en.wikibooks.org/wiki/Sensory_Systems/Visual_Signal_Processing http://www.datarobot.com/blog/a -primer -on-deep-learning/ http://blog.csdn.net/strint/article/details/44163869 ML and Deep Learning History · 1990s and early -2000s: drop in popularity. – It proved really difficult to get multi - layer models working robustly. – We obtained similar performance with simpler models: · Rise in popularity of logistic regression and SVMs with regularization and kernels . – Lots of internet successes (spam filtering, web search, recommendation). – ML moved closer to other fields like numerical optimization and statistics. ML and Deep Learning History · Late 2000s: push to revive connectionism as “deep learning ”. – Canadian Institute For Advanced Research (CIFAR) NCAP program: · “Neural Computation and Adaptive Perception”. · Led by Geoff Hinton, Yann LeCun , and Yoshua Bengio · Unsupervised successes: “deep belief networks” and “autoencoders”. · Could be used to initialize deep neural networks. https://www.cs.toronto.edu/~hinton/science.pdf 2010s: DEEP LEARNING!!! · Bigger datasets, bigger models, parallel computing (GPUs/clusters). – And some tweaks to the models from the 1980s. · Huge improvements in automatic speech recognition (2009). – All phones now have deep learning. · Huge improvements in computer vision (2012). – Changed computer vision field almost instantly. – Now is how most CV (and AI) is done http://www.image - net.org/challenges/LSVRC/2014/ 2010s: DEEP LEARNING!!! · Media hype: – “How many computers to identify a cat? 16,000” New York Times (2012). – “Why Facebook is teaching its machines to think like humans” Wired (2013). – “What is ‘deep learning’ and why should businesses care?” Forbes (2013). – “Computer eyesight gets a lot more accurate” New York Times (2014). · 2015: huge improvement in language understanding ImageNet Challenge · Millions of labeled images, 1000 object classes. http://www.image -net.org/challenges/LSVRC/2014/ ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. · 2015: Won by Microsoft Asia – 3.6% error. – 152 layers, introduced “ResNets”. – Also won “localization” (finding location of objects in images). · 2016: Chinese University of Hong Kong: – Ensembles of previous winners and other existing methods. · 2017: fewer entries, organizers decided this would be last year. http://www.themtank.org/a-year-in-computer-vision https://paperswithcode.com /sota /image -classification-on-imagenet Windsor Tie Anh Nguyen Backpropagation · Overview of how we compute neural network gradient: – Forward propagation : · Compute z i (1) from x i . · Compute z i (2) from z i (1) . · … · Compute Y_hat i from z i (m) , and use this to compute error. – Backpropagation : · Compute gradient with respect to regression weights ‘v’. · Compute gradient with respect to z i (m) weights W (m) . · Compute gradient with respect to z i (m - 1) weights W (m - 1) . · … · Compute gradient with respect to z i (1) weights W (1) . · “Backpropagation” is the chain rule plus some bookkeeping for speed. Backpropagation · Instead of the next few bonus slides, I HIGHLY recommend watching this video from former UBC master’s student Andrej Karpathy (of OpenAI, former director of AI and Autopilot Vision at Tesla) – https://www.youtube.com/watch?v =i94OvYb6noo Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. – Only the first ‘r’ changes if you use a different loss. – With multiple hidden units, you get extra sums. · Efficient if you store the sums rather than computing from scratch. Backpropagation · We’ve made backprop details bonus material · Do you need to know how to do this? – Exact details are probably not vital (there are many implementations). – “Automatic differentiation ” is now standard and has same cost. – But understanding basic idea helps you know what can go wrong. · Or give hints about what to do when you run out of memory. – See discussion by a neural network expert (Andrej!) - https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b Backpropagation · You should know cost of backpropagation: – Forward pass dominated by matrix multiplications by W (1) , W (2) , W (3) , and ‘v’. · If have ‘m’ layers and all z i have ‘k’ elements, cost would be O( dk + mk 2 ). – Backward pass has same cost as forward pass. Next · Finish discussion of how to train deep neural networks – algorithms, tips, and tricks, and miscellaneous key info Deep Learning Vocabulary · “Deep learning ”: Models with many hidden layers. – Usually neural networks. · “Neuron”: node in the neural network graph. – “ Visible unit ”: feature. – “ Hidden unit ”: latent factor z ic or h(z ic ). · “Activation function”: non- linear transform. · “Activation”: h(z i ). · “Backpropagation”: compute gradient of neural network. – Sometimes “backpropagation” means “ training with SGD ”. · “ Weight decay ”: L2- regularization. · “Cross entropy ”: softmax loss. · “Learning rate”: SGD step- size. · “Learning rate decay ”: using decreasing step- sizes. · “ Vanishing/Exploding gradient ”: gradient becoming real small/big for deep net Stochastic Gradient Training · Standard training method is stochastic gradient (SG): – Choose a random example ‘i ’ (more common: mini-batch of samples) – Use backpropagation to get gradient with respect to all parameters. – Take a small step in the negative gradient direction. · Challenging to make SG work: – Often doesn’t work as a “black box” learning algorithm. – But people have developed a lot of tricks/modifications to make it work. · Highly non- convex , so are the problem local minima? – Some empirical/theoretical evidence that local minima are not the problem. – If the network is “deep” and “wide” enough, we think all local minima are good. – But it can be hard to get SG to close to a local minimum in reasonable time. Ne w I s s u e: V a n i s h i n g G r a d i en t s • Co n s i d e r t h e s i gm o i d fu n c t i o n : • A w a y f r o m t h e o r i g i n , t h e gr a d i en t i s n ea r l y z er o . • Th e p r ob l e m g e ts w or s e wh e n y ou t a k e th e si g m o i d o f a si g m o i d : • I n d e e p n e t w o r k s , m a n y gr a d i en t s c a n b e n ea r l y z er o e v er y w h er e . – A n d n u m e ri c a l l y t h e y w i l l b e s e t t o 0 , s o SGD d oe s n ot m o v e . R e c t i f i e d L i n e a r Un i t s ( Re L U ) • Mo de r n ne t w o r k s o f t e n r e pl a c e si g m o i d w i t h pe r c e p t r o n l o ss ( Re L U ): • J u s t se t s ne g a t i v e v a l ue s z ic t o z e r o . – Re d u c e s va n i s h i n g g r a d i e n t p r o b l e m ( p o s i t i v e r e g i o n i s n e v e r f l a t ) . – G i v e s s p a r s e r a c t i v a t i o n s . – St i l l gi v e s a u n i v e r s a l ap p r o x im a t o r i f s i z e o f h i d d e n l a y e r s g r o w s w i t h ‘n ’ . “Swish” Activiation · Recent work searched for “best” activation: · Found that z ic /(1+exp( -z ic )) worked best (“swish” function). – A bit weird because it allows negative values and is non- monotonic. – But basically the same as ReLU when not close to 0. Sk ip Con n e c t ion s De e p L e ar n in g • S k i p c o nne c t i o ns c a n a l s o r e duc e v a ni s hi ng g r a di e n t pr o bl e m : • Ma k e s “ s ho r t c ut s ” be t w e e n l a y e r s ( s o f e w e r t r a ns f o r m a t i o ns ) . – M a n y v a r i a t i o n s e x i s t o n s k i p c o n n e c t i o n s e x i s t . Re s N e t “B l o c k s ” • Re s i d u a l n e t w o r k s ( Re s N e t s ) ar e a v ar ian t o n s k ip c o n n e c t io n s . – C o n s i s t o f r e p e a t e d “ b l o c k s ” , fi r s t m e t h o d s t h a t s u c c es s fu l l y u s ed 1 0 0 + la y e r s . • Us u a l c o m p u t a t i o n o f a ct i v a t i o n b a s e d o n p r e v i o u s 2 l a y e r s : • Re s N e t “ bl o c k ”: – Ad d s a ct i v a t i on s f r om “2 l a y e r s a g o” . • Di f f e r e n ce s f r o m u s u a l s k i p c o n n e ct i o n s : – Act i v a t i on s v e ct or s a l an d a l+ 2 mu s t h a v e th e sa m e si z e . – No w e ig h t s o n a l , s o W l an d W l+ 1 mu s t f o c u s o n “ u p d a ti n g ” a l (f i t “r e s i d u a l ”). • I f y o u us e Re L U , t he n W l = 0 i m pl i e s a l+ 2 =a l . ht t p s : / / e n . w ik ip e d ia. o r g / w ik i/ R e s id u al_ n e u r al_ n e t w o r k ht t p s : / / t o w a r d s d a t a s c i e n c e . c o m / a n - ov e r v i e w - of - re s n e t - an d - it s - va r i a n t s - 5281e 2f 56035 Parameter Initialization · Parameter initialization is crucial: – Can’t initialize weights in same layer to same value, or units will stay the same. · Architecture is symmetric, so gradient would be the same for every hidden unit in the layer, so they’d all just always stay doing the exact same thing. – Can’t initialize weights too large, it will take too long to learn. · A traditional random initialization: – Initialize bias variables to 0. – Sample from standard normal, divided by 10 5 (0.00001* randn). · w = .00001* randn(k,1) – Performing multiple initializations does not seem to be important (except maybe with very small networks) Parameter Initialization · Parameter initialization is crucial: – Can’t initialize weights in same layer to same value, or they will stay same. – Can’t initialize weights too large, it will take too long to learn. · Also common to transform data in various ways: – Subtract mean, divide by standard deviation, “whiten”, standardize y i . · More recent initializations try to standardize initial z i : – Use different initialization in each layer. – Try to make variance of z i the same across layers. · Popular approach is to sample from standard normal, divide by sqrt(2* nInputs ). – Use samples from uniform distribution on [ - b,b], where Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · One approach: manual “babysitting” of the step-size. – Run SG for a while with a fixed step- size. – Occasionally measure error and plot progress: – If error is not decreasing, decrease step- size. Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · Bias step -size multiplier : use bigger step-size for the bias variables. · Momentum (stochastic version of “heavy-ball” algorithm): – Add term that moves in previous direction: – Usually β t = 0.9. Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Good demo to check out: https://distill.pub /2017/momentum/ Setting the Step - Size · Automatic method to set step size is Bottou trick : 1. Grab a small set of training examples (maybe 5% of total). 2. Do a binary search for a step size that works well on them. 3. Use this step size for a long time (or slowly decrease it from there). · Several recent methods using a step size for each variable : – AdaGrad, RMSprop, Adam (often work better “out of the box”). – Some controversy versus plain stochastic gradient (often with momentum). · SGD can often get lower test error, even though it takes longer and requires more tuning of step- size. · Batch size (number of random examples) also influences results. – Bigger batch sizes often give faster convergence but maybe to worse solutions? · Another recent trick is batch normalization: – Try to “standardize” the hidden units within the random samples as we go. – Held as example of deep learning “alchemy ” (blog post here about deep learning claims). · Sounds science - ey and often works, but little theoretical understanding. Com m on De e p L e ar n in g T r ic k s • Da t a st a n d a r d i z a t i o n ( “ c e n t e r i n g ” a n d “ w h i t e n i n g ”) . • Pa r a m e t e r in it ializ a t io n : “ sm a l l but di f f e r e n t “. – I f w e i n i t i a l i z e a l l p a r a m e t e r s i n t h e l a y e r t o s a m e v a l u e , t h e y s t a y t h e s a m e . – Al s o c om m on t o u s e i n i t i a l i z a t i on s t h a t a r e st a n d a r d i z e d wi th i n l a y e r s . • St e p - si z e se l e c t i o n: “ ba b y si t t i ng “. – U s e b i g g e r st e p - s i z e f o r t h e b i a s v a ri a b l e s , o r d i f f e r e n t f o r e a c h l a y e r . – M e t h o d s t h a t u s e a s t e p s i z e f o r e ac h c o o r d in a t e ( Ad a Gr a d , RM S p r op , Ad a m ). • E a r l y s t op p i n g of th e op ti mi z a ti on b a s e d on v a l i d a ti on a c c u r a c y . • Mo m e n t um : ad d s w e ig h t e d s u m o f p r e vio u s S GD d ir e c t io n s . • Ba t c h n o r m a l i z a t i o n : a d a p ti v e s t a n d a r d i z i n g w i th i n l a y e r s . – O f t e n a l l o w s s i g m o i d a c t i v a t i o n s i n d e e p n e t w o rk s . Com m on De e p L e ar n in g T r ic k s • L2 - re g u l a r i z a t i o n or L1 - re g u l a r i z a t i o n ( “ w e i g h t d e c a y ” ) . – Som e t i m e s wi t h d i f f e r e n t ! fo r e a c h l a y e r . – R e c e n t w o r k s h o w s th i s c a n i n tr o d u c e b a d l o c a l o p ti m a . • Dr o p o u t : r a n d o m l y z e r o e s a c t i v a t i o n s ‘z ’ v a l u e s t o d i s c o u r a g e d e p e n d e n c e . • Re c t i f i e d l i n e a r u n i t s ( Re L U ) a s n o n - l i n e a r t r a n s f o r m a t i o n . – Ma k e s o bj e ct i v e no n - d i f f e r e n t i a b l e , b u t w e n o w k n o w S G D s t i l l c o n v e r g e s i n t h i s s e t t i n g . • R e s i d u a l / s k i p c o n n e c t i o n s : c o n n e c t l a y e r s t o m u l t i p l e p r e v i o u s l a y e r s . – We n o w k n o w t h a t s u c h c o n n e c t i o n s m a k e i t m o r e l i k e l y t o c o n v e r g e t o g o o d m i n i m a . • Ne u r a l a r c h i t e c t u r e s e a r c h : t r y t o c l e v e r l y s e a r c h sp a c e o f h y p e r - pa r a me t e r s . – Th i s g e t s e x p en s i v e! • S o m e o f t h e s e tr ic k s a r e e x p l o r e d i n b o n u s s l i d e s . Mi s s i n g T h e o r y B e h i n d T r a i n i n g D e e p N e tw o rk s • Un f o r t u n a t el y , we d o n o t u n d e r s t a n d m a n y o f t h e s e t r i c k s v e r y w e l l . – La r g e p o r ti o n o f th e o r y i s o n d e g e n e r a t e c a s e o f l i n e a r n e u r a l n e tw o r k s . • Or o t h e r w e i r d c a s e s l i k e “ 1 h i d d e n u n i t p e r l a y e r ” . – A l o t o f r e s e a r c h i s p e r f o r m e d u s i n g “ gr a d s tu d e n t d e sc e n t ”. • S e v e r a l v a r i a t i o n s a r e t r i e d , on e s th a t p e r f o r m w e l l e m p i r i c a l l y a r e ke p t ( p o s s i b l y o v e r f i t t i n g ) . • P o p u l a r E x a m p l e s : – Ba t c h n or m a l i z a t i on or i g i n a l l y p r op ose d t o fi x “ i n t e r n a l c o v a r i a t e sh i ft ” . • I n t e r n a l c o v a r i a t e s h i f t n o t d e f i n e d i n o r i g i n a l p a p e r , a n d b a t c h n o r m d o e s s e e m t o r e d u c e i t . – O f t e n s i ng l e d o ut a s a n e x a m pl e o f pr o bl e m s w i t h m a c hi ne l e a r ni ng s c ho l a r s hi p . • L i k e m a n y h e u r i s t i c s , p e o p l e u s e b a t c h n o rm b e c a u s e th e y f o u n d th a t i t o f t e n h e l p s . – M a n y pe o pl e ha v e w o r k e d o n be t t e r e x pl a na t i o ns . – Ad a m o p ti m i z e r is a n ic e c o m b in a t io n s o f id e as f r o m s e v e r al e x is t in g alg o r it h m s . • Suc h as “ m o m e n t um ” and “ Ad a G r a d ” , b o t h o f w h i c h a r e w e l l - unde r s t o o d t he o r e t ic ally . – B ut t he o r y i n t he o r i g i na l pa pe r w a s i nc o r r e c t , a nd A da m f a i l s a t s o l v i ng s o m e v e r y - s i m pl e o p t i m i z a t i o n pr o bl e m s . • Bu t i s A d a m i s o f t en u s ed b ec a u s e i t i s a m a z i n g a t t r a i n i n g s o m e n e t w o r k s . – It ha s be e n h y po t he s i z e d t ha t w e “ c o n v e r g e d” t o w a r ds ne t w o r k s t ha t a r e e a s i e r f o r c ur r e n t S G D m e t ho ds l i k e A da m . Summary · Unprecedented performance on difficult pattern recognition tasks. · Backpropagation computes neural network gradient via chain rule. · Parameter initialization is crucial to neural net performance. · Optimization and step size are crucial to neural net performance. – “Babysitting”, momentum. · ReLU avoid “vanishing gradients”. · Next: The most important idea in computer vision? Autoencoders · Autoencoders are an unsupervised deep learning model: – Use the inputs as the output of the neural network. – Middle layer could be latent features in non - linear latent-factor model. · Can do outlier detection, data compression, visualization, etc. – A non - linear generalization of PCA. · Equivalent to PCA if you don’t have non- linearities . http://inspirehep.net/record/1252540/plots Autoencoders https://www.cs.toronto.edu/~hinton/science.pdf Denoising Autoencoder · Denoising autoencoders add noise to the input: – Learns a model that can remove the noise. http://inspirehep.net/record/1252540/plots CPSC 340: Machine Learning and Data Mining Convolutions Spring 2022 (2021W2) Convolutional Neural Networks – arguably the most important idea in computer vision Motivation: Automatic Brain Tumor Segmentation · Task: labeling tumors and normal tissue in multi - modal MRI data. Input: Output: · Applications: – Radiation therapy target planning, quantifying treatment responses. – Mining growth patterns, image - guided surgery. · Challenges: – Variety of tumor appearances, similarity to normal tissue. – “You are never going to solve this problem.” Naïve Voxel - Level Classifier · We could treat classifying a voxel as supervised learning: – Standard representation of image: each pixel gets “intensity” between 0 and 255. · We can formulate predicting y i given x i as supervised learning. · But it doesn’t work at all with a linear weighting of these features. Need to Summarize Local Context · The individual pixel intensity values are almost meaningless: – The same x i could lead to different y i . · Intensities not standardized. · Non- trivial overlap in signal for different tissue types. · “Partial volume” effects at boundaries of tissue types. Need to Summarize Local Context · We need to represent the “context” of the pixel (what is around it). – Include all the values of neighbouring pixels as extra features? · Run into coupon collection problems: requires lots of data to find patterns. – Measure neighbourhood summary statistics (mean, variance, histogram)? · Variation on bag of words problem: loses spatial information present in voxels. – Standard approach uses convolutions to represent neighbourhood. Example: Measuring “brightness” of an Area - This pixel is in a “bright” area of the image, which reflects “bleeding” of tumour. - But the actual numeric intensity value of the pixel is the same as in darker “gray matter” areas. - I want a feature saying “this pixel is in a bright area of the image”. - This will us help identify that it’s a tumour pixel. - How to measure brightness in area? Easy way: take average pixel intensity in “neighbourhood”. - Applying this “averaging” to every pixel gives a new image: - We can use “pixel value in new image” as a new feature. - New feature helps identify if pixel is in a “bright” area. The annoying thing about squares · “Take the average of a square window” loses spatial information. · Example: Fixing the “square” issues · Consider instead “blurring” the image. – Gets rid of “local” noise, but better preserves spatial information. · How do you “blur”? – Take weighted average of window, putting more “weight” on “close” pixels: Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution: Big Picture · How do you use convolution to get features? – Apply several different convolutions to your image. – Each convolution gives a different “image” value at each location. – Use theses different image values to give features at each location. Convolutions: Big Picture · What can features coming from convolutions represent? – Some filters give you an average value of the neighbourhood. – Some filters detect edges (directionally) (first derivative) · “Is there a change from dark to bright?” · “If so, from which direction in space?” – Some filters detect lines (“second derivative”) · “Is there a spike or is the change speeding up?” 1D Convolution Example · Consider a 1D “signal” (maybe from sound): – We’ll come back to images later. · For each “time”: – Compute dot- product of signal at surrounding times with a “filter ” of weights. · This gives a new “signal” : – Measures a property of “neighbourhood”. – This particular filter shows a local “how spiky ” value. 1D Convolution (notation is specific to this lecture) · 1D convolution input: – Signal ‘x’ which is a vector length ‘n’. · Indexed by i = 1, 2, …, n – Filter ‘w’ which is a vector of length ‘2m+1’: · Indexed by i = - m, - m+1, …, - 2, 0, 1, 2, …, m - 1, m · Output is a vector of length ‘n’ with elements: – You can think of this as centering w at position ‘ i ’, and taking a dot product of ‘w’ with that “part” x i . 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 -1 1D Convolution Examples · Examples: – “Identity” – “Translation” 1D Convolution Examples · Examples: – “Identity” – “Local Average” Boundary Issue · What can we do about the “?” at the edges ? · Can assign values past the boundaries : · “Zero”: · “Replicate”: · “Mirror”: · Or just ignore the “?” values and return a shorter vector : Formal Convolution Definition · We’ve defined the convolution as: · In other classes you may see it defined as: · For simplicity we’re skipping the “reverse” step , and assuming ‘w’ and ‘x’ are sampled at discrete points (not functions). · But keep this mind if you read about convolutions elsewhere. 1D Convolution Examples · Translation convolution shift signal: – “What is my neighbour’s value?” 1D Convolution Examples · Averaging convolution (“is signal generally high in this region?” – Less sensitive to noise (or spikes) than raw signal. 1D Convolution Examples · Laplacian convolution approximates second derivative: – “Sum to zero” filters “respond” if input vector looks like the filter 1D Convolution Examples · Gaussian convolution (“blurring”): – Compared to averaging it’s more smooth and maintains peaks better. We stopped here","libVersion":"0.2.1","langs":""}