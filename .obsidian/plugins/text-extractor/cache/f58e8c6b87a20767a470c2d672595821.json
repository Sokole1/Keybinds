{"path":".obsidian/plugins/text-extractor/cache/f58e8c6b87a20767a470c2d672595821.json","text":"Question 5. (?? points) Match each loss function and regularizer to the corresponding property by writing the corre- sponding number next to each item: (a) 0-1 loss. (b) Absolute loss. (c) Hinge loss. (d) Squared loss. (e) Softmax loss. (f) LO-regularization. (g) Li-regularization. (h) L2-regularization. (i) Loo-regularization. (j) Logistic loss. (k) Max of absolute residuals loss. 1. Gaussian likelihood. 2. Makes least squares solution unique. 3. Makes variables have the same magnitude. 4. Non-convex feature selection. 5. Number of training errors. 6. Regularization and feature selection. 7. Robust to outliers. 8. Smooth loss for binary classification. 9. Training all w. simultaneously. 10. Tries to get the outliers right. 11. Upper bound on 0-1 loss.","libVersion":"0.2.1","langs":"eng"}