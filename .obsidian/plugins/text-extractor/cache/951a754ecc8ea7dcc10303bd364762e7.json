{"path":".obsidian/plugins/text-extractor/cache/951a754ecc8ea7dcc10303bd364762e7.json","text":"CPSC 340: Machine Learning and Data Mining More PCA Last Time: Latent-Factor Models • Latent-factor models take input data ‘X’ and output a basis ‘Z’: – Usually, ‘Z’ has fewer features than ‘X’. • Uses: dimensionality reduction, visualization, factor discovery. http://infoproc.blogspot.ca/2008/11/european-genetic-substructure.html https://new.edu/resources/big-5-personality-traits Last Time: Principal Component Analysis • Principal component analysis (PCA) is a linear latent-factor model: – These models “factorize” matrix X into matrices Z and W: – We can think of rows wc of W as ‘k’ fixed “parts” (used in all examples). – zi is the “part weights” for example xi: “how much of each part wc to use”. Next Topic: PCA Geometry in Low Dimensions PCA with d=2 and k =1PCA with d=2 and k =1PCA with d=2 and k =1PCA with d=2 and k =1 • With d=3, PCA (k=1) finds line minimizing squared distance to xi. • With d=3, PCA (k=2) finds plane minimizing squared distance to xi. PCA with d=3 and k=2. http://www.nlpca.org/fig_pca_principal_component_analysis.png Last Time: PCA Geometry • With any ‘d’, when k=2 the W matrix defines a plane: – Even if the original data is high-dimensional, we can visualize data “projected” onto this plane. http://www.prismtc.co.uk/superheroes-pca/ Next Topic: PCA Loss Function and Prediction PCA Objective Function • In PCA we minimize the squared error of the approximation: • This is equivalent to the k-means objective: – In k-means zi only has a single ‘1’ value and other entries are zero. • But in PCA, zi can be any real number. – We approximate xi as a linear combination of all means/factors. PCA Objective Function • In PCA we minimize the squared error of the approximation: • We can also view this as solving ‘d’ regression problems: – Each wj is trying to predict column ‘xj’ from the basis zi. • The output “yi” we try to predict here is actually the features “xi”. – And unlike in regression we are also learning the features zi. Principal Component Analysis (PCA) • The 3 different ways to write the PCA objective function: Digression: Data Centering (Important) • In PCA, we assume that the data X is “centered”. – Each column of X has a mean of zero. • It’s easy to center the data: • There are PCA variations that estimate “bias in each coordinate”. – In basic model this is equivalent to centering the data. PCA Computation: Prediction • At the end of training, the “model” is the µj and the W matrix. – PCA is parametric. • PCA prediction phase: – Given new data ෨𝑋, we can use µj and W this to form ෨𝑍: PCA Computation: Prediction • At the end of training, the “model” is the µj and the W matrix. – PCA is parametric. • PCA prediction phase: – Given new data ෨𝑋, we can use µj and W this to form ෨𝑍: – The “reconstruction error” is how close approximation is to ෨𝑋: – Our “error” from replacing the xi with the zi and W. Choosing ‘k’ by “Variance Explained” • Common to choose ‘k’ based on variance of the xij. – For a given ‘k’ we compute (variance of errors)/(variance of xij): – Gives a number between 0 (k=d) and 1 (k=0), giving “variance remaining”. • If you want to “explain 90% of variance”, choose smallest ‘k’ where ratio is < 0.10. “Variance Explained” in the Doom Map • Recall the Doom latent-factor model (where map ignores height): • Interpretation of “variance remaining” formula: • If we had a 3D map the “variance remaining” would be 0. https://en.wikipedia.org/wiki/Doom_(1993_video_game) https://forum.minetest.net/viewtopic.php?f=5&t=9666 Next Topic: Eigenfaces Application: Face Detection • Consider problem of face detection: • Classic methods use “eigenfaces” as basis: – PCA applied to images of faces. https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html Application: Face DetectionEigenfaces • Collect a bunch of images of faces under different conditions: EigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfaces Next Topic: Non-Uniqueness of PCA Non-Uniqueness of PCA • Unlike k-means, we can efficiently find global optima of f(W,Z). – Algorithms coming later. • Unfortunately, there never exists a unique global optimum. – There are actually several different sources of non-uniqueness. • To understand these, we’ll need idea of “span” from linear algebra. – This also helps explain the geometry of PCA. – We’ll also see that some global optima may be better than others. Span of 1 Vector • Consider a single vector w1 (k=1). Span of 1 Vector • Consider a single vector w1 (k=1). • The span(w1) is all vectors of the form ziw1 for a scalar zi. Span of 1 Vector • Consider a single vector w1 (k=1). • The span(w1) is all vectors of the form ziw1 for a scalar zi. • If w1 ≠ 0, this forms a line. • But note that the “span” of many different vectors gives same line. – Mathematically: αw1 defines the same line as w1 for any scalar α ≠ 0. – PCA solution can only be defined up to scalar multiplication. • If (W,Z) is a solution, then (αW,(1/α)Z) is also a solution. Span of 1 VectorSpan of 2 Vectors • Consider two vector w1 and w2 (k=2). Span of 2 Vectors • Consider two vector w1 and w2 (k=2). – The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. Span of 2 Vectors • Consider two vector w1 and w2 (k=2). – The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. Span of 2 Vectors • Consider two vector w1 and w2 (k=2). – The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. – For most non-zero 2d vectors, span(w1,w2) is a plane. • In the case of two vectors in R2, the plane will be *all* of R2. • Consider two vector w1 and w2 (k=2). – The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. – For most non-zero 2d vectors, span(w1,w2) is plane. • Exception is if w2 is in span of w1 (“collinear”), then span(w1,w2) is just a line. Span of 2 VectorsSpan of 2 Vectors • Consider two vector w1 and w2 (k=2). – The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. – New issues for PCA (k >= 2): • We have label switching: span(w1,w2) = span(w2,w1). • We can rotate factors within the plane (if not rotated to be collinear). Span of 2 Vectors • 2 tricks to make vectors defining a plane “more unique”: – Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. Span of 2 Vectors • 2 tricks to make vectors defining a plane “more unique”: – Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. Span of 2 Vectors • 2 tricks to make vectors defining a plane “more unique”: – Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. – Orthogonality: enforce that w1 Tw2 = 0 (“perpendicular”). – Now I can’t grow/shrink vectors (though I can still reflect). – Now I can’t rotate one vector (but I can still rotate *both*). Digression: PCA only makes sense for k ≤ d • Remember our clustering dataset with 4 clusters: • It doesn’t make sense to use PCA with k=4 on this dataset. – We only need two vectors [1 0] and [0 1] to exactly represent all 2d points. • With k=2, I could set Z=X and W=I to get X=ZW exactly. Span in Higher Dimensions • In higher-dimensional spaces: – Span of 1 non-zero vector w1 is a line. – Span of 2 non-zero vectors w1 and w2 is a plane (if not collinear). • Can be visualized as a 2D plot. – Span of 3 non-zeros vectors {w1, w2, w3} is a 3d space (if not “coplanar”). – … • This is how the W matrix in PCA defines lines, planes, spaces, etc. – Each time we increase ‘k’, we add an extra “dimension” to the “subspace”. Summary • PCA geometry: – With k=1, PCA projects data onto a line . – With k=2, PCA projects data onto a plane. • PCA objective: – Minimizes squared error between elements of X and elements of ZW. • Choosing ‘k’: – We can choose ‘k’ to explain “percentage of variance” in the data. • PCA non-uniqueness: – Due to scaling, rotation, and label switching. • Next time: cancer signatures and NBA shot charts. 1. Decision trees 2. Naïve Bayes classification 3. Ordinary least squares regression 4. Logistic regression 5. Support vector machines 6. Ensemble methods 7. Clustering algorithms 8. Principal component analysis 9. Singular value decomposition 10.Independent component analysis (bonus) http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html Making PCA Unique • PCA implementations add constraints to make solution unique: – Normalization: we enforce that ||wc|| = 1. – Orthogonality: we enforce that wc Twc’ = 0 for all c ≠ c’. – Sequential fitting: We first fit w1 (“first principal component”) giving a line. • Then fit w2 given w1 (“second principal component”) giving a plane. • Then we fit w3 given w1 and w2 (“third principal component”) giving a space. • … • Even with all this, the solution is only unique up to sign changes: – I can still replace any wc by –wc: • -wc is normalized, is orthogonal to the other wc’, and spans the same space. – Possible fix: require that first non-zero element of each wc is positive. – And this is assuming you don’t have repeated singular values. • In that case you can rotate the repeated ones within the same plane.","libVersion":"0.2.1","langs":""}