{"path":".obsidian/plugins/text-extractor/cache/951a754ecc8ea7dcc10303bd364762e7.json","text":"CPSC 340: Machine Learning and Data Mining More PCA Last Time: Latent-Factor Models â€¢ Latent-factor models take input data â€˜Xâ€™ and output a basis â€˜Zâ€™: â€“ Usually, â€˜Zâ€™ has fewer features than â€˜Xâ€™. â€¢ Uses: dimensionality reduction, visualization, factor discovery. http://infoproc.blogspot.ca/2008/11/european-genetic-substructure.html https://new.edu/resources/big-5-personality-traits Last Time: Principal Component Analysis â€¢ Principal component analysis (PCA) is a linear latent-factor model: â€“ These models â€œfactorizeâ€ matrix X into matrices Z and W: â€“ We can think of rows wc of W as â€˜kâ€™ fixed â€œpartsâ€ (used in all examples). â€“ zi is the â€œpart weightsâ€ for example xi: â€œhow much of each part wc to useâ€. Next Topic: PCA Geometry in Low Dimensions PCA with d=2 and k =1PCA with d=2 and k =1PCA with d=2 and k =1PCA with d=2 and k =1 â€¢ With d=3, PCA (k=1) finds line minimizing squared distance to xi. â€¢ With d=3, PCA (k=2) finds plane minimizing squared distance to xi. PCA with d=3 and k=2. http://www.nlpca.org/fig_pca_principal_component_analysis.png Last Time: PCA Geometry â€¢ With any â€˜dâ€™, when k=2 the W matrix defines a plane: â€“ Even if the original data is high-dimensional, we can visualize data â€œprojectedâ€ onto this plane. http://www.prismtc.co.uk/superheroes-pca/ Next Topic: PCA Loss Function and Prediction PCA Objective Function â€¢ In PCA we minimize the squared error of the approximation: â€¢ This is equivalent to the k-means objective: â€“ In k-means zi only has a single â€˜1â€™ value and other entries are zero. â€¢ But in PCA, zi can be any real number. â€“ We approximate xi as a linear combination of all means/factors. PCA Objective Function â€¢ In PCA we minimize the squared error of the approximation: â€¢ We can also view this as solving â€˜dâ€™ regression problems: â€“ Each wj is trying to predict column â€˜xjâ€™ from the basis zi. â€¢ The output â€œyiâ€ we try to predict here is actually the features â€œxiâ€. â€“ And unlike in regression we are also learning the features zi. Principal Component Analysis (PCA) â€¢ The 3 different ways to write the PCA objective function: Digression: Data Centering (Important) â€¢ In PCA, we assume that the data X is â€œcenteredâ€. â€“ Each column of X has a mean of zero. â€¢ Itâ€™s easy to center the data: â€¢ There are PCA variations that estimate â€œbias in each coordinateâ€. â€“ In basic model this is equivalent to centering the data. PCA Computation: Prediction â€¢ At the end of training, the â€œmodelâ€ is the Âµj and the W matrix. â€“ PCA is parametric. â€¢ PCA prediction phase: â€“ Given new data à·¨ğ‘‹, we can use Âµj and W this to form à·¨ğ‘: PCA Computation: Prediction â€¢ At the end of training, the â€œmodelâ€ is the Âµj and the W matrix. â€“ PCA is parametric. â€¢ PCA prediction phase: â€“ Given new data à·¨ğ‘‹, we can use Âµj and W this to form à·¨ğ‘: â€“ The â€œreconstruction errorâ€ is how close approximation is to à·¨ğ‘‹: â€“ Our â€œerrorâ€ from replacing the xi with the zi and W. Choosing â€˜kâ€™ by â€œVariance Explainedâ€ â€¢ Common to choose â€˜kâ€™ based on variance of the xij. â€“ For a given â€˜kâ€™ we compute (variance of errors)/(variance of xij): â€“ Gives a number between 0 (k=d) and 1 (k=0), giving â€œvariance remainingâ€. â€¢ If you want to â€œexplain 90% of varianceâ€, choose smallest â€˜kâ€™ where ratio is < 0.10. â€œVariance Explainedâ€ in the Doom Map â€¢ Recall the Doom latent-factor model (where map ignores height): â€¢ Interpretation of â€œvariance remainingâ€ formula: â€¢ If we had a 3D map the â€œvariance remainingâ€ would be 0. https://en.wikipedia.org/wiki/Doom_(1993_video_game) https://forum.minetest.net/viewtopic.php?f=5&t=9666 Next Topic: Eigenfaces Application: Face Detection â€¢ Consider problem of face detection: â€¢ Classic methods use â€œeigenfacesâ€ as basis: â€“ PCA applied to images of faces. https://developer.apple.com/library/content/documentation/GraphicsImaging/Conceptual/CoreImaging/ci_detect_faces/ci_detect_faces.html Application: Face DetectionEigenfaces â€¢ Collect a bunch of images of faces under different conditions: EigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfacesEigenfaces Next Topic: Non-Uniqueness of PCA Non-Uniqueness of PCA â€¢ Unlike k-means, we can efficiently find global optima of f(W,Z). â€“ Algorithms coming later. â€¢ Unfortunately, there never exists a unique global optimum. â€“ There are actually several different sources of non-uniqueness. â€¢ To understand these, weâ€™ll need idea of â€œspanâ€ from linear algebra. â€“ This also helps explain the geometry of PCA. â€“ Weâ€™ll also see that some global optima may be better than others. Span of 1 Vector â€¢ Consider a single vector w1 (k=1). Span of 1 Vector â€¢ Consider a single vector w1 (k=1). â€¢ The span(w1) is all vectors of the form ziw1 for a scalar zi. Span of 1 Vector â€¢ Consider a single vector w1 (k=1). â€¢ The span(w1) is all vectors of the form ziw1 for a scalar zi. â€¢ If w1 â‰  0, this forms a line. â€¢ But note that the â€œspanâ€ of many different vectors gives same line. â€“ Mathematically: Î±w1 defines the same line as w1 for any scalar Î± â‰  0. â€“ PCA solution can only be defined up to scalar multiplication. â€¢ If (W,Z) is a solution, then (Î±W,(1/Î±)Z) is also a solution. Span of 1 VectorSpan of 2 Vectors â€¢ Consider two vector w1 and w2 (k=2). Span of 2 Vectors â€¢ Consider two vector w1 and w2 (k=2). â€“ The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. Span of 2 Vectors â€¢ Consider two vector w1 and w2 (k=2). â€“ The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. Span of 2 Vectors â€¢ Consider two vector w1 and w2 (k=2). â€“ The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. â€“ For most non-zero 2d vectors, span(w1,w2) is a plane. â€¢ In the case of two vectors in R2, the plane will be *all* of R2. â€¢ Consider two vector w1 and w2 (k=2). â€“ The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. â€“ For most non-zero 2d vectors, span(w1,w2) is plane. â€¢ Exception is if w2 is in span of w1 (â€œcollinearâ€), then span(w1,w2) is just a line. Span of 2 VectorsSpan of 2 Vectors â€¢ Consider two vector w1 and w2 (k=2). â€“ The span(w1,w2) is all vectors of form zi1w1 + zi2w2 for a scalars zi1 and zi2. â€“ New issues for PCA (k >= 2): â€¢ We have label switching: span(w1,w2) = span(w2,w1). â€¢ We can rotate factors within the plane (if not rotated to be collinear). Span of 2 Vectors â€¢ 2 tricks to make vectors defining a plane â€œmore uniqueâ€: â€“ Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. Span of 2 Vectors â€¢ 2 tricks to make vectors defining a plane â€œmore uniqueâ€: â€“ Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. Span of 2 Vectors â€¢ 2 tricks to make vectors defining a plane â€œmore uniqueâ€: â€“ Normalization: enforce that ||w1|| = 1 and ||w2|| = 1. â€“ Orthogonality: enforce that w1 Tw2 = 0 (â€œperpendicularâ€). â€“ Now I canâ€™t grow/shrink vectors (though I can still reflect). â€“ Now I canâ€™t rotate one vector (but I can still rotate *both*). Digression: PCA only makes sense for k â‰¤ d â€¢ Remember our clustering dataset with 4 clusters: â€¢ It doesnâ€™t make sense to use PCA with k=4 on this dataset. â€“ We only need two vectors [1 0] and [0 1] to exactly represent all 2d points. â€¢ With k=2, I could set Z=X and W=I to get X=ZW exactly. Span in Higher Dimensions â€¢ In higher-dimensional spaces: â€“ Span of 1 non-zero vector w1 is a line. â€“ Span of 2 non-zero vectors w1 and w2 is a plane (if not collinear). â€¢ Can be visualized as a 2D plot. â€“ Span of 3 non-zeros vectors {w1, w2, w3} is a 3d space (if not â€œcoplanarâ€). â€“ â€¦ â€¢ This is how the W matrix in PCA defines lines, planes, spaces, etc. â€“ Each time we increase â€˜kâ€™, we add an extra â€œdimensionâ€ to the â€œsubspaceâ€. Summary â€¢ PCA geometry: â€“ With k=1, PCA projects data onto a line . â€“ With k=2, PCA projects data onto a plane. â€¢ PCA objective: â€“ Minimizes squared error between elements of X and elements of ZW. â€¢ Choosing â€˜kâ€™: â€“ We can choose â€˜kâ€™ to explain â€œpercentage of varianceâ€ in the data. â€¢ PCA non-uniqueness: â€“ Due to scaling, rotation, and label switching. â€¢ Next time: cancer signatures and NBA shot charts. 1. Decision trees 2. NaÃ¯ve Bayes classification 3. Ordinary least squares regression 4. Logistic regression 5. Support vector machines 6. Ensemble methods 7. Clustering algorithms 8. Principal component analysis 9. Singular value decomposition 10.Independent component analysis (bonus) http://www.kdnuggets.com/2016/08/10-algorithms-machine-learning-engineers.html Making PCA Unique â€¢ PCA implementations add constraints to make solution unique: â€“ Normalization: we enforce that ||wc|| = 1. â€“ Orthogonality: we enforce that wc Twcâ€™ = 0 for all c â‰  câ€™. â€“ Sequential fitting: We first fit w1 (â€œfirst principal componentâ€) giving a line. â€¢ Then fit w2 given w1 (â€œsecond principal componentâ€) giving a plane. â€¢ Then we fit w3 given w1 and w2 (â€œthird principal componentâ€) giving a space. â€¢ â€¦ â€¢ Even with all this, the solution is only unique up to sign changes: â€“ I can still replace any wc by â€“wc: â€¢ -wc is normalized, is orthogonal to the other wcâ€™, and spans the same space. â€“ Possible fix: require that first non-zero element of each wc is positive. â€“ And this is assuming you donâ€™t have repeated singular values. â€¢ In that case you can rotate the repeated ones within the same plane.","libVersion":"0.2.1","langs":""}