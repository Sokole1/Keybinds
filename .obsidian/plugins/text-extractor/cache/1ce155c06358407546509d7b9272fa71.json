{"path":".obsidian/plugins/text-extractor/cache/1ce155c06358407546509d7b9272fa71.json","text":"CPSC 320: Divide and Conquer Solutions ∗ Invented by Tony Hoare in the late 1950’s, the Quicksort algorithm was a breakthrough in sorting methods. Variants of Hoare’s original algorithm are still a sorting method of choice today. Here you’ll gain experience with the divide and conquer algorithmic design approach, as well as recurrence analysis, that led Hoare to this breakthrough, and see an application also to ﬁnding the median. Here is a basic version of Quicksort, when the array A[1..n] to be sorted has n distinct elements: function Quicksort(A[1..n]) ▷ returns the sorted array A of n distinct numbers if n > 1 then ▷ Θ(1) Choose pivot element p = A[1] ▷ Θ(1) Let Lesser be an array of all elements from A less than p ▷ Θ(n) Let Greater be an array of all elements from A greater than p ▷ Θ(n) LesserSorted ← QuickSort(Lesser) ▷ TQ(⌈ n 4 ⌉ − 1) GreaterSorted ← QuickSort(Greater) ▷ TQ(⌊ 3n 4 ⌋) return the concatenation of LesserSorted, [p], and GreaterSorted ▷ O(1) else return A ▷ Θ(1) 1 Quicksort Runtime Analysis 1. Suppose that QuickSort happens to always select the ⌈ n 4 ⌉-th largest element as its pivot. Give a recurrence relation for the runtime of QuickSort. SOLUTION: Let TQ(n) be the runtime (number of steps) of QuickSort on an array of length n. For these solutions, we’ve annotated the code above with the runtime of each step. This leads us to the following recurrence. It’s convenient in recurrences to replace big-O or Θ terms with some constant, and it’s ok to use the same constant everywhere. TQ(n) = { c, if n = 0 or n = 1 TQ(⌈ n 4 ⌉ − 1) + TQ(⌊ 3n 4 ⌋) + cn, otherwise. Ignoring ﬂoors, ceilings, and constants we get a slightly simpler recurrence: TQ(n) = { c, if n = 0 or n = 1 TQ( n 4 ) + TQ( 3n 4 ) + cn, otherwise. ∗Copyright Notice: UBC retains the rights to this document. You may not distribute this document without permission. 1 2. Using the recurrence, draw a recursion tree for QuickSort. Label each node by the number of elements in the array at that node’s call (the root is labeled n) and the amount of time taken by that node but not its children. Also, label the total work (time) for each \"level\" of calls. Show the root at level 0 and the next two levels below the root, and also the node at the leftmost and rightmost branches of level i. SOLUTION: We show the work done at each node (and not its children) in black, and the array size at that node in blue. 3. Find the following two quantities. (a) The number of levels in the tree down to the shallowest leaf. Hint: Is the shallowest leaf on the leftmost side of the tree, the rightmost side, or somewhere else? If you’ve already described the problem size of the leftmost and rightmost nodes at level i as a function of i, then set that equal to the problem size you expect at the leaves and solve for i. SOLUTION: The n 4i branch will reach the base case fastest. If we set that equal to 1, we get n 4i = 1, or n = 4i. Taking logs on both sides, log4 n = i. Also log4 n = log2 n/2 (recall that more generally, logb n = logc n/ logc b). So the number of levels to the shallowest leaf is lg n/2. (b) The number of levels in the tree down to the deepest leaf. SOLUTION: Similarly, the ( 3 4 )in branch reaches the base case slowest, and by a similar analysis, we ﬁnd i = log 4 3 n. That looks nasty but is only a constant factor away from log4 n. (Since 4 3 > 1, this really is a nice, normal log.) 2 4. Use the work from the previous parts to ﬁnd asymptotic upper and lower bounds for the solution of your recurrence. SOLUTION: For the lower bound, we perform cn work at each level up to the level of the ﬁrst leaf, which has depth Ω(log n), so the total work done is Ω(n log n). For the upper bound, we perform at most cn work at each level, including levels after that of the ﬁrst leaf. There are O(log n) levels in total, so the total work done is O(n log n). Putting the upper and lower bounds together we see that the total work done is Θ(n log n). 5. Now, we’ll relax our assumption that the algorithm always selects the ⌈ n 4 ⌉-th largest element as its pivot. Instead, consider a weaker assumption that the rank of the pivot is always in the range between ⌈ n 4 ⌉ and ⌊ 3n 4 ⌋. The rank of an element is k if the element is the kth largest in the array. What can you say about the runtime of Quicksort in this case? SOLUTION: If the pivot always lies in the range between ⌈ n 4 ⌉ and ⌊ 3n 4 ⌋, the shallowest leaf has depth at least lg n, and the deepest leaf has depth at most log4/3 n. Also, the total work per level is cn, up to the level of the shallowest leaf, and at most cn for the remaining levels. So our asymptotic upper and lower bounds still hold, and the running time is still Θ(n log n). 6. Draw the recursion tree generated by QuickSort([10, 3, 5, 18, 1000, 2, 100, 11, 14]). As- sume that QuickSort: (1) selects the ﬁrst element as pivot and (2) maintains elements’ relative order when producing Lesser and Greater. SOLUTION: Here it is with each node containing the array passed into that node’s call, the pivot in bold, and the number 11 in blue. 3 2 An Algorithm for Finding the Median Suppose that you want to ﬁnd the median number in an array of length n. The algorithm below can be generalized to ﬁnd the element of rank k, i.e., the kth largest element in an array, for any 1 ≤ k ≤ n. (Note: the lower the rank, the larger the element.) The median is the element of rank ⌈n/2⌉. For example, the median in the array [10, 3, 5, 18, 1000, 2, 100, 11, 14] is the element of rank 5 (or 5th largest element), namely 11. 1. In your speciﬁc recursion tree above, mark the nodes in which the median (11) appears. (The ﬁrst of these is the root.) SOLUTION: These are the ones with the blue 11 in them above. 2. Look at the second recursive call you marked—the one below the root. 11 is not the median of the array in that recursive call! SOLUTION: (a) In this array, what is the median? Answer: In this array the median is 18. (b) In this array, what is the rank of the median? The element 18 has rank 3, i.e., 18 is the 3rd largest element. (c) In this array, what is the rank of 11? The element 11 has rank 5; it is the 5th largest element. (d) How does the rank of 11 in this array relate to the rank of 11 in the original array (at the root)? Why does this relationship hold? Note that we’re looking at the array [18, 1000, 100, 11, 14]. The rank of 11 has not changed; it is 5 here and also at the root. Why? 11 ended up on the right side (in Greater). So, everything that was larger than it in the original array (at the root) is still in the array [18, 1000, 100, 11, 14]. Thus, its rank has not changed. 3. Look at the third recursive call you marked. What is the rank of 11 in this array? How does this relate to 11’s rank in the second recursive call, and why? SOLUTION: Note that we’re looking at the node containing [11, 14]. This is the Lesser array of its parent, and the rank of 11 is 2. The rank changed by 3 because to obtain the Lesser array from its parent array, three elements were removed, namely the pivot and the two elements in Greater. With 3 fewer larger elements, it is now the 2nd largest element rather than the 5th largest. 4. If you’re looking for the element of rank 42 (i.e., the 42nd largest element) in an array of 100 elements, and Greater has 41 elements, where is the element you’re looking for? SOLUTION: If Greater has 41 elements, then there are 41 elements larger than the pivot. That makes the pivot the 42nd largest element. In other words, if the size of Greater is k − 1, then the pivot has rank k. (This is what happened—with much smaller numbers—in the node where 11 is also the pivot.) 5. How could you determine before making QuickSort’s recursive calls whether the element of rank k is the pivot or appears in Lesser or Greater? SOLUTION: Putting together what we saw above: • If |Greater| is equal to k − 1, then the k-th largest element is the pivot. • If |Greater| < k then the pivot is larger than the k-th largest element, which puts it in the Lesser group (and the left recursive call). • If |Greater| > k then the k-th largest element is in Greater (and the right recursive call). 4 6. Modify the QuickSort algorithm so that it ﬁnds the element of rank k. Just cross out or change parts of the code below as you see ﬁt. Change the function’s name! Add a parameter! Feel the power! SOLUTION: The key diﬀerence is that we no longer need the recursive calls on both sides, only on the side with the element we seek. function QuickSelect(A[1..n], k) // returns the element of rank k in an array A of n distinct numbers, where 1 ≤ k ≤ n if n > 1 then Choose pivot element p = A[1] Let Lesser be an array of all elements from A less than p Let Greater be an array of all elements from A greater than p if |Greater| = k - 1 then return p else if |Greater| > k - 1 then // all larger elts are in Greater; k is unchanged return QuickSelect(Greater, k) else// |Greater| < k − 1 // subtract from k the number of larger elts removed return QuickSelect(Lesser, k− |Greater| −1) else return A[1] 7. Once again, suppose that the rank of the pivot in your median-ﬁnding algorithm on a problem of size n is always in the range between ⌈ n 4 ⌉ and ⌊ 3n 4 ⌋. Draw the recursion tree that corresponds to the worst-case running time of the algorithm, and give a tight big-O bound in the algorithm’s running time. Also, provide an asymptotic lower bound on the algorithm’s running time. SOLUTION: Here’s the recursion tree (well, stick): Summing the work at each level, we get: ∑? i=0( 3 4 )icn = cn ∑? i=0( 3 4 )i. We’ve left the top of that sum as ? because it turns out not to be critical. This is a converging sum. If we let the sum go to inﬁnity (which is ﬁne for an upper-bound, since we’re not making the sum any smaller by adding positive terms!), it still approaches a constant: 1 1− 3 4 = 1 1 4 = 4. So the whole quantity approaches 4cn ∈ O(n). 5 Unlike the Quicksort algorithm, there’s no need to analyze recursion tree structure to conclude that the algorithm takes Ω(n) time. Since the algorithm partitions the n elements of A before any recursive call, it must take Ω(n) time, and thus Θ(n) time. In case you’re curious, here’s one way to analyse the above sum. Let S = ∑∞ i=0( 3 4 )i. Then: S = ∞∑ i=0( 3 4 ) i = 1 + ∞∑ i=1( 3 4 ) i = 1 + ∞∑ i=0( 3 4 ) i+1 = 1 + 3 4 ∞∑ i=0( 3 4 )i = 1 + 3 4 S Solving S = 1 + 3 4 S for S, we get 1 4 S = 1, and S = 4. While recursion trees provide a reliable way to solve pretty much all recurrences that we’ll see in this class, it’s good to know about alternative methods too. One popular method is the Master theorem, covered later in this handout. Again, if we assume that we \"get lucky\" with the choice of pivot at every recursive call, so the size of the subproblem is at most 3n/4, we can express the QuickSelect runtime using the following recurrence (ignoring ﬂoors and ceilings): TQS(n) = { 1 if n = 0 or n = 1 TQS(3n/4) + cn otherwise This has the form of the Master theorem, where a = 1, b = 4/3, and f (n) = cn. Since logb a = log4/3 1 = 0 (the log of 1 is 0, no matter what the base is), we see that f (n) = cn = Ω(n(logb a+ϵ)), where we can choose ϵ to be any positive number that is at most 1. So case 3 of the Master theorem applies, and we can conclude that TQS(n) = Θ(n). 6 The Master Theorem Here we’ll wrap up divide and conquer recurrences, and cover the Master Theorem. But ﬁrst some handy math. Geometric Sums. Here are useful formulas, when summing up runtimes over levels of a recurrence tree. A geometric sum has the form ∑n i=0 xi, where x > 0. Note that when x = 1, ∑n i=0 xi = n. When x ̸= 1 we have that: n∑ i=0 xi = xn+1 − 1 x − 1 = 1 − xn+1 1 − x . Now, for x < 1, take the limit as n → ∞ to get an expression for the inﬁnite sum: SOLUTION: ∞∑ i=0 xi = 1 1 − x , if 0 < x < 1. 1. Section 5.5 of the text describes an integer multiplication algorithm, where the inputs are n-bit numbers. The runtime of this algorithm is given by the recurrence: T (n) = { c, if n = 1, 3T (n/2) + cn, otherwise. Draw out and label enough of the recurrence tree, so that you can express T (n) as a sum of the total work per level. SOLUTION: Summing up over the levels, the total runtime is ( log2 n∑ i=0 (3/2) i)cn. 7 2. Apply the formula for geometric sums to simplify your expression for T (n). SOLUTION: First, we’ll simplify the sum. log2 n∑ i=0 (3/2) i) = ((3/2) log2 n+1 − 1)/(3/2 − 1)) = Θ((3/2) log2 n) = Θ((2log2(3/2))log2 n) = Θ((2 log2 n) log2(3/2)) = Θ(nlog2(3/2)). Here, we’ve applied our handy exponent trick (from an earlier worksheet) that a = clogc a. Plugging this in to the total runtime, we get ( log2 n∑ i=0 (3/2) i)cn = Θ(nlog2(3/2)n) = Θ(nlog2 3nlog2(1/2)n) = Θ(nlog2 3) = Θ(n1.585). 3. More generally, suppose that the recursive part of a recurrence for some runtime T (n) has the form: T (n) = aT (n/b) + cn k for n ≥ n0, where a > 0, b > 1, c > 0, k > 0 are constants. What is the total work done at level 2 of the recurrence tree? SOLUTION: Let’s do level 1 ﬁrst. There are a subproblems, each of size n/b, so the total work is ac(n/b) k = (a/b k)cn k. At level 2 the total work is a 2c(n/b2) k = (a/b k)2cn k. What is the total work done at level i? SOLUTION: a ic(n/b i)k = (a/b k) icn k. Write a sum for the total runtime. Note that the tree has logb n levels. SOLUTION: cn k logb n∑ i=0 (a/b k) i. 8 4. Looking back over the runtime recurrences for problems you’ve seen so far, give examples where the recurrence has the form shown in part 3, and also satisﬁes the following constraints. Write down what a, b, and k are for these examples. • a > bk: SOLUTION: This is illustrated by the integer multiplication recurrence, where a = 3, b = 2, and k = 1. • a = bk: SOLUTION: This is illustrated by the quicksort recurrence when the pivot always results in a perfect split, in which case a = b = 2 and k = 1. It’s also illustrated by MergeSort. • a < bk: SOLUTION: This is illustrated by the median-ﬁnding algorithm, where a = 1, b = 4/3 and k = 1. 5. Here is a statement of the Master Theorem. Suppose that T : N → R≥0 satisﬁes T (n) = { c, for n < n0, aT (n/b) + cnk, for n ≥ n0, where a > 0, b > 1, c > 0, and k ≥ 0 are constants. • If a > bk, then T (n) = Θ(nlogb a). • If a = bk, then T (n) = Θ(nk log n). • If a < bk, then T (n) = Θ(nk). Go back and verify that the results we obtained using recurrence trees for the problems you list in part 4 match those provided by the Master Theorem. SOLUTION: Left to the reader. Note: The Master Theorem is handy for analyzing runtime of algorithms where an instance of size n is solved by recursively solving one or more subproblems of size n/b for some b > 1 (plus some extra work to break the problem down and/or piece the solutions of subproblems back together). It is not as useful when the subproblems have diﬀerent sizes, or if subproblems have size n − b rather than n/b. 9","libVersion":"0.2.1","langs":""}