{"path":".obsidian/plugins/text-extractor/cache/13026a74a28ec3326ad25ce8431cc36d.json","text":"(b) Which of the following changes would typically reduce training error? Circle all that apply. Note: for regression and unsupervised models, assume we use the squared training error (squared distance to cluster mean for k-means, and squared prediction error for regression). i. using the fast decision stump fitting method based on sorting, instead of the naive approach ii. increasing k in KNN classification iii. increasing the maximum tree depth in a random forest iv. increasing k in k-means clustering v. using a higher degree polynomial basis for linear regression vi. running more iterations of gradient descent when fitting least squares (assuming « is small enough) vii. adding some relevant features to your model viii. adding some irrelevant features to your model ix. switching from the squared error to the absolute error when fitting a linear regression model x. increasing the parameter € in the Huber loss","libVersion":"0.2.1","langs":"eng"}