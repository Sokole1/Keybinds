{"path":".obsidian/plugins/text-extractor/cache/f52ad6688b6040ce8955673fb3f26166.json","text":"CPSC 340: Machine Learning and Data Mining More Linear Classifiers Last Time: Classification using Regression and SVMs â€¢ Binary classification using sign of linear models: â€¢ We considered different training â€œerrorâ€ functions: â€“ Squared error: (oi â€“ yi)2. â€¢ If yi = +1 and oi = +100, then squared error (oi â€“ yi)2 is huge. â€“ 0-1 classification error: (sign(oi) = yi)? â€¢ Non-convex and hard to minimize in terms of â€˜wâ€™ (unless optimal error is 0). â€“ Degenerate convex approximation to 0-1 error: max{0,-yioi}. â€¢ Has a degenerate solution of 0. â€“ Hinge loss: max{0,1-yioi}. â€¢ Convex upper bound on number of classification errors. â€¢ With L2-regularization, itâ€™s called a support vector machine (SVM). Logistic Loss â€¢ We can alternately smooth the degenerate loss with log-sum-exp: â€¢ Summing over all examples gives: â€¢ This is the â€œlogistic lossâ€ and model is called â€œlogistic regressionâ€. â€“ Itâ€™s not degenerate: w=0 now gives an error of log(2) instead of 0. â€“ Convex and differentiable: minimize this with gradient descent. â€“ You should also add regularization. â€“ We will see later that it has a probabilistic interpretation. Convex Approximations to 0-1 LossLogistic Regression and SVMs â€¢ Logistic regression and SVMs are used EVERYWHERE! â€“ Fast training and testing. â€¢ Training on huge datasets using â€œstochasticâ€ gradient descent (next week). â€¢ Prediction is just computing wTxi and then taking sign. â€“ Weights wj are easy to understand. â€¢ Itâ€™s how much wj changes the prediction and in what direction. â€“ We can often get a good good test error. â€¢ With low-dimensional features using RBFs and regularization. â€¢ With high-dimensional features and regularization. â€“ Smoother predictions than random forests. Comparison of â€œBlack Boxâ€ Classifiers â€¢ Fernandez-Delgado et al. [2014]: â€“ â€œDo we Need Hundreds of Classifiers to Solve Real World Classification Problems?â€ â€¢ Compared 179 classifiers on 121 datasets. â€¢ Random forests are most likely to be the best classifier. â€¢ Next best class of methods was SVMs (L2-regularization, RBFs). â€¢ â€œWhy should I care about logistic regression if I know about deep learning?â€ Next Topic: Maximizing the Margin Maximum-Margin Perspective â€¢ Consider a linearly-separable dataset. Maximum-Margin Perspective â€¢ Consider a linearly-separable dataset. â€“ Perceptron algorithm finds some classifier with zero error. â€“ But are all zero-error classifiers equally good? Maximum-Margin Perspective â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Maximum-Margin Perspective â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Maximum-Margin Perspective â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Maximum-Margin Perspective â€¢ For linearly-separable data: â€¢ With small-enough Î» > 0, SVMs find the maximum-margin classifier. â€“ Need Î» small enough that hinge loss is 0 in solution. â€“ Origin of the name: the â€œsupport vectorsâ€ are the points closest to the line (see bonus). â€¢ Recent result: logistic regression also finds maximum-margin classifier. â€“ With Î»=0 and if you fit it with gradient descent (not true for many other optimizers). Next Topic: Converting to Probabilities Previously: Identifying Important E-mails â€¢ Recall problem of identifying â€˜importantâ€™ e-mails: â€¢ We can do binary classification by taking sign of linear model: â€“ Convex loss functions (hinge/logistic loss) let us find an appropriate â€˜wâ€™. â€¢ But what if we want a probabilistic classifier? â€“ Want a model of p(yi = â€œimportantâ€ | xi) for use in decision theory. Predictions vs. Probabilities â€¢ With oi = wTxi, linear classifiers make prediction using sign(oi): â€¢ For predictions, â€œsignâ€ maps from oi to the elements {-1,+1}. â€“ If oi is positive we predict +1, if oi negative we predict -1. â€¢ For probabilities, we want to map from oi to the range [0,1]. â€“ If oi is very positive, we output a value close to +1 (confident yi=1). â€“ If oi is very negative, we output a value close to 0 (confident yi=-1). â€“ If oi is close to 0, we output a value close to 0.5 (classes equally likely). â€¢ So we want a transformation of oi = wTxi that looks like this: â€¢ The most common choice is the sigmoid function: â€¢ Values of h(oi) match what we want: Sigmoid FunctionProbabilities for Linear Classifiers using Sigmoid â€¢ Using sigmoid function, we output probabilities for linear models using: â€¢ Visualization for 2 features: https://www.youtube.com/watch?v=Zc7ouSD0DEQ Probabilities for Linear Classifiers using Sigmoid â€¢ Using sigmoid function, we output probabilities for linear models using: â€¢ By rules of probability: â€¢ We then use these for â€œprobability that e-mail is importantâ€. â€¢ This may seem heuristic, but later weâ€™ll see that: â€“ Minimizing logistic loss does â€œmaximum likelihood estimationâ€ in this model. Next Topic: Multi-Class Linear Classifiers Multi-Class Linear Classification â€¢ We have been considering linear models for binary classification: â€¢ E.g., is there a cat in this image or not? https://www.youtube.com/watch?v=tntOCGkgt98 Multi-Class Linear Classification â€¢ Today we will discuss linear models for multi-class classification: â€¢ For example, classify image as â€œcatâ€, â€œdogâ€, or â€œpersonâ€. â€“ This was natural for methods of Part 1 (decision trees, naÃ¯ve Bayes, KNN). â€“ For linear models, we need some new notation. â€œOne vs Allâ€ Classification â€¢ Suppose you only know how to do binary classification: â€“ â€œOne vs allâ€ is a way to turn a binary classifier into a multi-class method. â€¢ Training phase: â€“ For each class â€˜câ€™, train binary classifier to predict whether example is a â€˜câ€™. â€¢ For example, train a â€œcat detectorâ€, a â€œdog detectorâ€, and a â€œhuman detectorâ€. â€¢ If we have â€˜â„“â€™ possible labels/classes, this gives â€˜â„“â€™ binary classifiers . â€¢ Prediction phase: â€“ Apply the â€˜â„“â€™ binary classifiers to get a â€œscoreâ€ for each class â€˜câ€™. â€“ Predict the â€˜câ€™ with the highest score. â€œOne vs Allâ€ Linear Classification â€¢ â€œOne vs allâ€ logistic regression for classifying as cat/dog/person. â€“ Train a separate classifier for each class. â€¢ Classifier 1 tries to predict +1 for â€œcatâ€ images and -1 for â€œdogâ€ and â€œpersonâ€ images. â€¢ Classifier 2 tries to predict +1 for â€œdogâ€ images and -1 for â€œcatâ€ and â€œpersonâ€ images. â€¢ Classifier 3 tries to predict +1 for â€œpersonâ€ images and -1 for â€œcatâ€ and â€œdogâ€ images. â€“ This gives us a weight vector wc for each class â€˜câ€™. â€¢ Trained to have output oic = wcTxi having sign(oic)= +1 for true class â€˜câ€™. â€“ And sign(oicâ€™)=-1 for other classes câ€™. â€œOne vs Allâ€ Linear Classification â€¢ With â„“ classes, one vs. all trains â„“ binary classifiers w1, w2,â€¦, wâ„“ â€¢ To make predictions with these classifiers: â€“ Ideally, sign(oic) = +1 for one â€˜câ€™ and sign(oicâ€™) = -1 for all other classes câ€™. â€¢ In practice, it might be +1 for multiple classes or no class. â€“ To predict class, we take maximum value of oic (â€œhighest scoreâ€). â€¢ In the example above, predict â€œhumanâ€ (0.9 is higher than -0.8 and 0.1). Digression: Multi-Label Classification â€¢ A related problem is multi-label classification: â€¢ Which of the â€˜â„“â€™ objects are in this image? â€“ There may be more than one â€œcorrectâ€ class label. â€“ Here we can also fit â€˜â„“â€™ binary classifiers. â€¢ But we would take all the sign(oic)=+1 as the labels. http://image-net.org/challenges/LSVRC/2013/ Multi-Class Linear Classification (MEMORIZE) â€¢ Notation for multi-class linear classifiers: â€¢ Weâ€™ll use â€˜ â€™ as classifier where c=yi (weights of correct class). â€“ So if yi=2 then = w2, and = oic. â€¢ Similar matrix â€˜Wâ€™ used in k-means (here k=â„“, number of classes). â€œOne vs Allâ€ Multi-Class Linear Classification â€¢ Problem: We didnâ€™t train the wc so that c=yi would maximize oic. â€“ Each classifier is just trying to get the sign right. â€“ Here the classifier incorrectly predicts â€œdogâ€. â€¢ â€œOne vs Allâ€ doesnâ€™t try to put oi2 and oi3 on same scale for decisions like this. â€¢ We should try to make oi3 positive and oi2 negative relative to each other. â€¢ The multi-class hinge losses and the multi-class logistic loss do this. https://laughingsquid.com/pug-mask-a-latex-mask-so-you-can-look-like-a-dog/ Multi-Class SVMs â€¢ Can we define a loss that encourages c=yi to maximize oic? â€“ So when we maximizing over oic, we choose correct label yi. â€¢ Recall our derivation of the hinge loss (SVMs) with one output oi. â€“ We wanted yioi > 0 for all â€˜iâ€™ to classify correctly. â€“ We avoided non-degeneracy by aiming for yioi â‰¥ 1. â€“ We used the constraint violation as our loss: max{0,1-yioi}. â€¢ We can derive multi-class SVMs using the same stepsâ€¦ Multi-Class SVMs â€¢ Can we define a loss that encourages c=yi to maximize oic? â€¢ For here, there are two ways to measure constraint violation: Multi-Class SVMs â€¢ Can we define a loss that encourages c=yi to maximize oic? â€¢ For each training example â€˜iâ€™: â€“ â€œSumâ€ rule penalizes for each â€˜câ€™ that violates the constraint. â€“ â€œMaxâ€ rule penalizes for one â€˜câ€™ that violates the constraint the most. â€¢ â€œSumâ€ gives a penalty of â€˜k-1â€™ for W=0, â€œmaxâ€ gives a penalty of â€˜1â€™. â€¢ If we add L2-regularization, both are called multi-class SVMs: â€“ â€œMaxâ€ rule is more popular, â€œsumâ€ rule usually works better. â€“ Both are convex upper bounds on the 0-1 loss. Multi-Class Logistic Regression â€¢ We derived binary logistic loss by smoothing a degenerate â€˜maxâ€™. â€“ A degenerate constraint in the multi-class case can be written as: â€¢ We want the right side to be as small as possible. â€¢ Letâ€™s smooth the max with the log-sum-exp: â€“ This is no longer degenerate: with W=0 this gives a loss of log(â„“). â€¢ Called the softmax loss, the loss for multi-class logistic regression. Multi-Class Logistic Regression â€¢ We sum the loss over examples and add regularization: â€¢ This objective is convex (should be clear for 1st and 3rd terms). â€“ It is differentiable so you can use gradient descent. â€¢ When â„“=2, equivalent to using binary logistic loss. â€“ Not obvious at the moment. Multi-Class Linear Prediction in Matrix Notation â€¢ In multi-class linear classifiers our weights are: â€¢ To predict on all training examples, we first compute all wcTxi. â€“ Or in matrix notation: â€“ So predictions are maximum column indices of XWT (which is â€˜nâ€™ by â€˜â„“â€™). Digression: Frobenius Norm â€¢ The Frobenius norm of a (â€˜â„“â€™ by â€˜dâ€™) matrix â€˜Wâ€™ is defined by: â€¢ We can use this to write regularizer in matrix notation: Summary â€¢ Logistic loss uses a smooth convex approximation to the 0-1 loss. â€¢ SVMs and logistic regression are very widely-used. â€“ A lot of ML consulting: â€œfind good features, use L2-regularized logistic/SVMâ€. â€“ Under certain conditions, can be viewed as â€œmaximizing the marginâ€. â€“ Both are just linear classifiers (a hyperplane dividing into two halfspaces). â€¢ Sigmoid function is a way to turn linear predictions into probabilities. â€¢ One vs all turns a binary classifier into a multi-class classifier. â€¢ Multi-class SVMs measure violation of classification constraints. â€¢ Softmax loss is a multi-class version of logistic loss. â€¢ Next time: what makes good features? Hinge-Loss Perceptron â€¢ A perceptron-like algorithm for minimizing the hinge loss: â€“ Start with any w0. â€“ Go through examples until you find an example with yiwTxi > 1. â€¢ Set wt+1 = wt + 1âˆ’ğ‘¦ğ‘–(ğ‘¤ğ‘¡)ğ‘‡ğ‘¥ğ‘– ğ‘¥ğ‘– ğ‘‡ğ‘¥ğ‘– yixi (minimum change to wt that satisfies constraint). â€¢ If a classifier with hinge loss of 0 exists, this converges to one. â€“ Looks like perceptron, but with a step size added to update (green term). â€¢ Get perceptron algorithm if you replace green term with â€˜1â€™. â€“ A special case of the â€œprojection onto convex setsâ€ (POCS) algorithm. Maximum-Margin Classifier â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Maximum-Margin Classifier â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Maximum-Margin Classifier â€¢ Consider a linearly-separable dataset. â€“ Maximum-margin classifier: choose the farthest from both classes. Support Vector Machines â€¢ For linearly-separable data, SVM minimizes: â€“ Subject to the constraints that: (see Wikipedia/textbooks) â€¢ But most data is not linearly separable. â€¢ For non-separable data, try to minimize violation of constraints: Support Vector Machines â€¢ Try to maximizing margin and also minimizing constraint violation: â€¢ We typically control margin/violation trade-off with parameter â€œÎ»â€: â€¢ This is the standard SVM formulation (L2-regularized hinge). â€“ Some formulations use Î» = 1 and multiply hinge by â€˜Câ€™ (equivalent). Support Vector Machines for Non-Separable â€¢ Non-separable case: Support Vector Machines for Non-Separable â€¢ Non-separable case: Support Vector Machines for Non-Separable â€¢ Non-separable case: Support Vector Machines for Non-Separable â€¢ Non-separable case: Discussion of Various Linear Classifiers â€¢ Perceptron vs. logistic vs. SVM: â€“ These linear classifiers are all extremely similar. They are basically just variations on reasonable methods to learn a classifier that uses the rule $$\\hat{y}_i = \\text{sign}(w^Tx_i)$$. (The online vs. offline issue is a red herring, you can train logistic/SVMs online using stochastic gradient and you can write a linear program that will give you a minimizer of the perceptron objective). â€“ If you want to explore the small differences, these are some of the usual arguments: â€¢ The perceptron has largely been replaced by logistic/SVM, except in certain subfields like theory (it is easy to prove things about perceptrons) and natural language processing (mostly historical reasons). Perceptrons have the potential disadvantages of non-regularized models (non-uniqueness and potential non-existence of the solution, potential high sensitivity to small changes in the data, and non-robustness to irrelevant features). However, perceptrons do not interact well with regularization: if you add L2-regularization and the dataset is linearly-separable, then the solution only exists as a limit and it is actually $$w=0$$ (although it may still work in practice). â€¢ A usual criticism of logistic regression by people that favour SVMs is that, if the data is linearly separable, then the solution only exists as a limit as some elements $$w$$ go to plus or minus $$\\infty$$. However, this argument disappears if you add regularization. A second argument traditionally made by SVM people is that you can't kernelize logistic regression, but this is now known to be incorrect (we'll cover a general kernelization strategy for L2- regularized linear classifiers in one of the next two classes). â€¢ The remaining differences between logistic and SVMs is that logistic regression is smooth while SVMs have support vectors. This means that the logistic regression training problem is easier from an optimization perspective (we'll get to this next class). But if you have very few support vectors, you can only take advantage of this with SVMs (or perceptrons), and this is especially important if you are using kernels. â€¢ Regarding other linear predictors for binary classification, there are a few more: â€“ Probit regression uses the Gaussian CDF in place of the logistic sigmoid function. This has very similar properties to logistic regression, but it's harder to generalize to the multi-class case (while probit regression is better if you are using a â€œBayesianâ€ estimator). You could actually use any CDF as your sigmoid function, and if there is some asymmetry between the classes using an extreme value distribution is sometimes advocated in statistics. â€“ In neural networks, they sometimes use tanh in place of the logistic sigmoid function, and the reason to do this is to get values into the interval [-1,1] instead of [0,1]. â€“ If you want to keep support vectors but get a smooth optimization problem, you can square the hinge loss (making it once but not twice differentiable), and this is called smooth SVMs. Alternately, you could replace the non-differentiable kink with a small smooth part, and this is called Huberized SVMs. â€“ Finally, some people actually just apply least squares to classification problems. If you use a flexible enough basis/kernel, then the 'bad' errors may not actually be that harmful. Robustness and Convex Approximations â€¢ Because the hinge/logistic grow like absolute value for mistakes, they tend not to be affected by a small number of outliers. Robustness and Convex Approximations â€¢ Because the hinge/logistic grow like absolute value for mistakes, they tend not to be affected by a small number of outliers. â€¢ But performance degrades if we have many outliers. Non-Convex 0-1 Approximations â€¢ There exists some smooth non-convex 0-1 approximations. â€“ Robust to many/extreme outliers. â€“ Still NP-hard to minimize. â€“ But can use gradient descent. â€¢ Finds â€œlocalâ€ optimum. â€œRobustâ€ Logistic Regression â€¢ A recent idea: add a â€œfudge factorâ€ vi for each example. â€¢ If wTxi gets the sign wrong, we can â€œcorrectâ€ the mis-classification by modifying vi. â€“ This makes the training error lower but doesnâ€™t directly help with test data, because we wonâ€™t have the vi for test data. â€“ But having the vi means the â€˜wâ€™ parameters donâ€™t need to focus as much on outliers (they can make |vi| big if sign(wTxi) is very wrong). â€œRobustâ€ Logistic Regression â€¢ A recent idea: add a â€œfudge factorâ€ vi for each example. â€¢ If wTxi gets the sign wrong, we can â€œcorrectâ€ the mis-classification by modifying vi. â€¢ A problem is that we can ignore the â€˜wâ€™ and get a tiny training error by just updating the vi variables. â€¢ But we want most vi to be zero, so â€œrobust logistic regressionâ€ puts an L1-regularizer on the vi values: â€¢ You would probably also want to regularize the â€˜wâ€™ with different Î». Support Vector Regression â€¢ Support vector regression objective (with hyper-parameter ğœ–): â€“ Looks like L2-regularized robust regression with the L1-loss. â€“ But have loss of 0 if à·œğ‘¦ğ‘– within ğœ– of à·¤ğ‘¦ğ‘–. â€¢ So doesnâ€™t try to fit data exactly. â€“ This can help fight overfitting. â€“ Support vectors are points with loss>0. â€¢ Points outside the â€œepsilon-tubeâ€. â€“ Example with Gaussian-RBFs as features: 1-Class SVMs â€¢ 1-class SVMs for outlier detection. â€“ Variables are â€˜wâ€™ (vector) and â€˜w0â€™ (scalar). â€“ Only trains on â€œinliersâ€. â€¢ Tries to make wTxi bigger than w0 for inliers. â€¢ At test time: says â€œoutlierâ€ if wTxi < w0. â€¢ Usually used with RBFs. â€“ The above is one possible 1-class formulation, but there are many more. https://scikit-learn.org/stable/auto_examples/svm/plot_oneclass.html","libVersion":"0.2.1","langs":""}