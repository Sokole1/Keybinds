{"path":".obsidian/plugins/text-extractor/cache/c210ddd88714a56dc54ed4a2a0637016.json","text":"CPSC 340: Machine Learning and Data Mining Feature Selection Last Time: Finding the â€œTrueâ€ Model â€¢ What if yi really is a polynomial function of xi? â€“ How can we find the â€œtrueâ€ degree â€˜pâ€™ of the polynomial? â€¢ Training error does not work: â€“ It goes down as â€˜pâ€™ goes up. â€¢ Cross-validation may also not work: â€“ Tends to overestimate â€˜pâ€™. â€“ Due to optimization bias. http://www.cs.ubc.ca/~arnaud/stat535/slides5_revised.pdf Last Time: Complexity Penalties â€¢ We discussed putting a penalty on the model complexity. â€“ Want to fit the data and have a simple model. â€“ â€œTo increase the degrees of freedom by one, need to decrease error by Î»â€. â€¢ Prefers smaller degrees of freedom, if errors are similar. â€“ Canâ€™t optimize this using gradient descent, since itâ€™s discontinuous in â€˜pâ€™. â€¢ Need to search over values of â€˜pâ€™. Bayesian Information Criterion (BIC) â€¢ A disadvantage of these methods: â€“ Still prefers a larger â€˜pâ€™ as â€˜nâ€™ grows. â€¢ Solution: make Î» depend on â€˜nâ€™. â€¢ For example, the Bayesian information criterion (BIC) uses: â€¢ BIC penalizes a bit more than AIC for large â€˜nâ€™. â€“ As â€˜nâ€™ goes to âˆ, recovers â€œtrueâ€ model (â€œconsistentâ€ for model selection). â€¢ In practice, we usually just try a bunch of different Î» values. â€“ Picking Î» is like picking â€˜kâ€™ in k-means. Discussion of other Scores for Model Selection â€¢ There are many other scores: â€“ Elbow method (corresponds to specific choice of Î»). â€¢ You could also use BIC for choosing â€˜kâ€™ in k-means. â€“ Methods based on validation error. â€¢ â€œTake smallest â€˜pâ€™ within one standard error of minimum cross-validation errorâ€. â€“ Minimum description length. â€“ Risk inflation criterion. â€“ False discovery rate. â€“ Marginal likelihood (CPSC 440). â€¢ These can adapted to use the L1-norm and other errors. Next Topic: Feature Selection Motivation: Discovering Food Allergies â€¢ Recall the food allergy example: â€¢ What I want to know which foods are making me sick? â€“ Rather than building a black box that tells me if I will be sick. â€¢ Instead of prediction, we want to do feature selection: â€“ Which foods are â€œrelevantâ€ for predicting â€œsickâ€. Egg Milk Fish Wheat Shellfish Peanuts â€¦ 0 0.7 0 0.3 0 0 0.3 0.7 0 0.6 0 0.01 0 0 0 0.8 0 0 0.3 0.7 1.2 0 0.10 0.01 Sick? 1 1 0 1 Feature Selection â€¢ General feature selection problem: â€“ Find the features (columns) of â€˜Xâ€™ that are important for predicting â€˜yâ€™. â€¢ â€œWhat are the relevant factors?â€ â€¢ â€œWhich basis functions should I use among these choices?â€ â€¢ â€œWhat types of new data should I collect?â€ â€¢ â€œHow can I speed up computation?â€ â€¢ One of most important problems in ML/statistics, but very messy. â€“ For now, we will say a feature is â€œrelevantâ€ if it helps predict yi from xi. â€œAssociationâ€ Approach â€¢ A simple/common way to do feature selection: â€“ For each feature â€˜jâ€™, compute correlation between feature values xj and â€˜yâ€™. â€¢ Say that â€˜jâ€™ is relevant if correlation is above 0.5 or below -0.5. â€¢ Turns feature selection into hypothesis testing for each feature. â€¢ There are many other measures of â€œdependenceâ€ (Wikipedia). â€¢ Usually gives unsatisfactory results as it ignores variable interactions: â€“ Includes irrelevant variables: â€œTaco Tuesdaysâ€. â€¢ If tacos make you sick, and you often eat tacos on Tuesdays, it will say â€œTuesdayâ€ is relevant. â€“ Excludes relevant variables: â€œDiet Coke + Mentos Eruptionâ€. â€¢ Diet coke and Mentos donâ€™t make you sick on their own, but together they make you sick. Genome-Wide Association Studies â€¢ Genome-wide association studies: â€“ Measure if there exists a dependency between each individual â€œsingle- nucleotide polymorphismâ€ in the genome and a particular disease. â€“ Has identified thousands of genes â€œassociatedâ€ with diseases. â€¢ But by design this has a huge numbers of false positives (and many false negatives). https://en.wikipedia.org/wiki/Genome-wide_association_study â€œRegression Weightâ€ Approach â€¢ Another simple/common approach to feature selection: â€“ Fit regression weights â€˜wâ€™ based on all features (maybe with least squares). â€“ Take all features â€˜jâ€™ where weight |wj| is greater than a threshold. â€¢ For example: you fit a least squares model with 5 features and get: â€“ Feature 3 looks the most relevant. â€“ Feature 4 also looks relevant. â€“ Feature 5 seems irrelevant. â€œRegression Weightâ€ Approach â€¢ Another simple/common approach to feature selection: â€“ Fit regression weights â€˜wâ€™ based on all features (maybe with least squares). â€“ Take all features â€˜jâ€™ where weight |wj| is greater than a threshold. â€¢ This could recognize that â€œTuesdayâ€ is irrelevant. â€“ It could assign a large weight to â€œtacosâ€, and a small weight to â€œTuesdayâ€. â€¢ Since the tacos would â€œexplainâ€ the correlation between â€œTuesdayâ€ and â€œsickâ€. â€¢ Assuming you get enough data, and you sometimes eat tacos on other days. (And the relationship is actually linear.) â€œRegression Weightâ€ Approach â€¢ Another simple/common approach to feature selection: â€“ Fit regression weights â€˜wâ€™ based on all features (maybe with least squares). â€“ Take all features â€˜jâ€™ where weight |wj| is greater than a threshold. â€¢ Has major problems with collinearity: â€“ If the â€œTuesdayâ€ variable always equals the â€œtacoâ€ variable, it could say that Tuesdays are relevant but tacos are not. â€“ If you have two copies of an irrelevant feature, it could take both irrelevant copies. Digression: â€œFeatureâ€ vs. â€œModelâ€ Selection? â€¢ Model selection: â€œwhich model should I use?â€ â€“ KNN vs. decision tree, depth of decision tree, degree of polynomial basis. â€¢ Feature selection: â€œwhich features should I use?â€ â€“ Using feature 10 or not, using xi 2 as part of basis. â€¢ These two tasks are highly-related: â€“ It is a different â€œmodelâ€ if we add xi 2 to linear regression. â€“ But the xi 2 term is just a â€œfeatureâ€ that could be â€œselectedâ€ or not. â€“ Usually, â€œfeature selectionâ€ means choosing from some â€œoriginalâ€ features. â€¢ You could say that â€œfeatureâ€ selection is a special case of â€œmodelâ€ selection. Model Selection Feature Selection Next Topic: Search and Score Methods Can it help prediction to throw features away? â€¢ Yes, because linear regression can overfit with large â€˜dâ€™. â€“ Even though itâ€™s â€œjustâ€ a hyper-plane. â€¢ Consider using d=n, with random features: X=randn(n,d). â€“ With high probability, you will be able to get a training error of 0. â€“ But the features were random, this is completely overfitting. â€¢ You could view â€œnumber of featuresâ€ as a hyper-parameter. â€“ Model gets more complex as you add more features. Search and Score Methods â€¢ Most common feature selection framework is search and score: 1. Define score function f(S) that measures quality of a set of features â€˜Sâ€™. 2. Now search for the variables â€˜Sâ€™ with the best score. â€¢ Example with 3 features: â€“ Compute â€œscoreâ€ of selecting only feature 1. â€“ Compute â€œscoreâ€ of selecting only feature 2. â€“ Compute â€œscoreâ€ of selecting only feature 3. â€“ Compute â€œscoreâ€ of selecting only features {1,2}. â€“ Compute â€œscoreâ€ of selecting only features {1,3}. â€“ Compute â€œscoreâ€ of selecting only features {2,3}. â€“ Compute â€œscoreâ€ of selecting all features {1,2,3}. â€“ Compute â€œscoreâ€ of selecting no features {}. â€“ Return the set of features â€˜Sâ€™ with the best â€œscoreâ€. Which Score Function? â€¢ The score cannot be the training error. â€“ Training error goes down as you add features, so will select all features. â€¢ A more logical score is the validation error. â€“ â€œFind the set of features that gives the lowest validation error.â€ â€“ To minimize test error, this is what we want. â€¢ But there are problems due to the large number of sets of variables: â€“ If we have â€˜dâ€™ variables, there are 2d sets of variables. â€“ Optimization bias is high: weâ€™re optimizing over 2d models (not 10). â€¢ So prone to false positives: irrelevant variables will sometimes help by chance. â€œNumber of Featuresâ€ Penalties â€¢ To reduce false positives, we can again use complexity penalties: â€“ Weâ€™re using â€˜xiSâ€™ as the features â€˜Sâ€™ of example xi. â€“ Above we minimize squared error plus a penalty on number of features. â€¢ â€œYou can include an extra feature if it reduces training error by at least ğœ†.â€ â€¢ If two â€˜Sâ€™ have similar error, this prefers the smaller set. â€“ It prefers removing feature 3 instead of having w3 = 0.00001. â€¢ We often this the â€œL0-normâ€ instead of writing â€œsize(S)â€â€¦ â€œL0-Normâ€ and â€œNumber of Features We Useâ€ â€¢ In linear models, setting wj = 0 is the same as removing feature â€˜jâ€™: â€¢ The L0 â€œnormâ€ is the number of non-zero values (||w||0 = size(S)). â€“ Not actually a true norm. â€“ If â€˜wâ€™ has a small L0-norm, then it does not use many features. L0-penalty: optimization â€¢ L0-norm penalty for feature selection: â€¢ Suppose we want to use this to evaluate the features S = {1,2}: â€“ First fit the â€˜wâ€™ just using features 1 and 2. â€“ Now compute the training error with this â€˜wâ€™ and features 1 and 2. â€“ Add Î»*2 to the training error to get the score. â€¢ We repeat this with other choices of â€˜Sâ€™ to find the â€œbestâ€ features. 21 L0-penalty: interpretation â€¢ L0-norm penalty for feature selection: â€¢ Balances between training error and number of features we use. â€“ For Î»=0, we get least squares with all features (no penalty on non-zeroes). â€“ For Î»=âˆ, we must set w=0 and not use any features (infinite penalty). â€“ For other Î», balances between training error and number of non-zeroes. â€¢ Larger Î» puts more emphasis on having zeroes in â€˜wâ€™ (more features removed). â€¢ Different values give AIC, BIC, and so on. 22 Forward Selection (Greedy Search Heuristic) â€¢ In search and score, itâ€™s also just hard to search for the best â€˜Sâ€™. â€“ There are 2d possible sets. â€¢ A common greedy search procedure is forward selection: Forward Selection (Greedy Search Heuristic) â€¢ Forward selection algorithm for variable selection: 1. Start with an empty set of features, S = [ ]. 2. For each possible feature â€˜jâ€™: â€¢ Compute scores of features in â€˜Sâ€™ combined with feature â€˜jâ€™. 3. Find the â€˜jâ€™ that has the best score when added to â€˜Sâ€™. 4. Check if {S âˆª j} improves on the best score found so far. 5. Add â€˜jâ€™ to â€˜Sâ€™ and go back to Step 2. â€¢ A variation is to stop if no â€˜jâ€™ improves the score over just using â€˜Sâ€™. â€¢ Runtime of forward selection: â€“ We fit O(d2) models, out of the 2d possible models with different features â€¢ Each step requires fitting up to â€˜dâ€™ models, and there are up to â€˜dâ€™ steps. â€“ Total cost will be O(d2) times the cost of fitting an individual model. â€¢ See bonus for the case of least squares, and how you fit â€œupdatedâ€ model faster than re-fitting. â€¢ Not guaranteed to find the best set, but fitting fewer models reduces many problems: â€“ Cheaper, overfits less, has fewer false positives. Backward Selection and RFE â€¢ Forward selection often works better than naÃ¯ve methods. â€¢ A related method is backward selection: â€“ Start with all features, compute score after removing each feature, remove the one that improves the score the most. â€¢ If you consider adding or removing features, itâ€™s called stagewise selection. â€¢ Stochastic local search is a class of fancier methods. â€“ Simulated annealing, genetic algorithms, ant colony optimization, etc. â€¢ Recursive feature elimination is another related method: â€“ Fit parameters of a regression model, prune features with small regression weights, repeat. â€¢ See bonus slide for discussion of feature selection in random forests. Next Topic: Ambiguity of Feature Selection Is â€œRelevanceâ€ Clearly Defined? â€¢ Consider a supervised classification task: â€¢ Predict whether someone has particular genetic variation (SNP). â€“ Location of mutation is in â€œmitochondrialâ€ DNA. â€¢ â€œYou almost always have the same value as your momâ€. â€“ For simplicity weâ€™ll assume 1950s-style gender and parentage. gender mom dad F 1 0 M 0 1 F 0 0 F 1 1 SNP 1 0 0 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ Consider a supervised classification task: â€¢ True model: â€“ (SNP = mom) with very high probability. â€“ (SNP != mom) with some very low probability. â€¢ What are the â€œrelevantâ€ features for this problem? â€“ Mom is relevant and {gender, dad} are not relevant. gender mom dad F 1 0 M 0 1 F 0 0 F 1 1 SNP 1 0 0 1 https://en.wikipedia.org/wiki/Human_mitochondrial_genetics Is â€œRelevanceâ€ Clearly Defined? â€¢ What if â€œmomâ€ feature is repeated? â€¢ Are â€œmomâ€ and â€œmom2â€ relevant? â€“ Should we pick them both? â€“ Should we pick one because it predicts the other? â€¢ If features can be predicted from features, canâ€™t know which to pick. â€“ Collinearity is a special case of â€œdependenceâ€ (which may be non-linear). gender mom dad mom2 F 1 0 1 M 0 1 0 F 0 0 0 F 1 1 1 SNP 1 0 0 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ What if we add (maternal) â€œgrandmaâ€? â€¢ Is â€œgrandmaâ€ relevant? â€“ You can predict SNP very accurately from â€œgrandmaâ€ alone. â€“ But â€œgrandmaâ€ is irrelevant if I know â€œmomâ€. â€¢ There is no information gained from â€œgrandmaâ€ if you already have â€œmomâ€. gender mom dad grandma F 1 0 1 M 0 1 0 F 0 0 0 F 1 1 1 SNP 1 0 0 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ What if we donâ€™t know â€œmomâ€? â€¢ Now is â€œgrandmaâ€ is relevant? â€“ Without â€œmomâ€ variable, using â€œgrandmaâ€ is the best you can do. â€¢ A feature is only â€œrelevantâ€ in the context of available features. â€“ Adding/removing features can make features relevant/irrelevant. SNP 1 0 0 1 gender grandma dad F 1 0 M 0 1 F 0 0 F 1 1 Summary â€¢ Feature selection is task of choosing the â€œrelevantâ€ features. â€“ Obvious simple approaches have obvious simple problems. â€¢ Search and score: find features that optimize some score. â€“ L0-norm penalties are the most common scores. â€“ Forward selection is a heuristic to search over a smaller set of features. â€¢ â€œRelevanceâ€ depends on context. â€“ Adding/removing features can make things relevant/irrelevant. â€¢ Next time: getting a good test error even with irrelevant features. Feature Selection in Random Forests â€¢ Decision trees naturally do feature selection while learning: â€“ The features used for the splits are the ones that are â€œselectedâ€. â€¢ There are a variety of ways to evaluate features in random forests: â€“ Compute proportion of trees that use feature â€˜jâ€™. â€“ Compute average infogain increase when using feature â€˜jâ€™. â€“ Permute all values of feature â€˜jâ€™, and see how â€œout of bagâ€ error increases. â€¢ You could use any of above to select features from random forest. Mallowâ€™s Cp â€¢ Older than AIC and BIC is Mallowâ€™s Cp: â€¢ Minimizing this score is equivalent to L0-regularization: â€¢ So again, viewing Î» as hyper-parameter, this score is special case. Adjusted R2 â€¢ Older than AIC and BIC and Mallowâ€™s Cp is adjusted R2: â€¢ Maximizing this score is equivalent to L0-regularization: â€¢ So again, viewing Î» as hyper-parameter, this score is special case. ANOVA â€¢ Some people also like to compute this â€œANOVAâ€ quantity: â€¢ This is based on the decomposition of â€œtotal squared errorâ€ as: â€¢ Notice that â€œexplained errorâ€ goes up as our usual (â€œresidualâ€) error goes down. â€¢ Trying to find the â€˜kâ€™ features that maximize â€˜fâ€™ (â€œexplain the most varianceâ€) is equivalent to L0-regularization with a particular Î» (so another special case). Information Criteria with Noise Variance â€¢ We defined AIC/BIC for feature selection in least squares as: â€¢ The first term comes from assuming yi = wTxi + Îµ, where Îµ comes from a normal distribution with a variance of 1. â€“ Weâ€™ll discuss why when we discuss MLE and MAP estimation. â€“ If you arenâ€™t doing least squares, replace first term by â€œlog-likelihoodâ€. â€¢ If you treat variance as a parameter, then after some manipulation: â€¢ However, this is again equivalent to just changing Î». Complexity Penalties for Other Models â€¢ Scores like AIC and BIC can also be used in other contexts: â€“ When fitting a decision tree, only split a node if it improves BIC. â€“ This makes sense if weâ€™re looking for the â€œtrue treeâ€, or maybe just a simple/interpretable tree that performs well. â€¢ In these cases we replace â€œL0-normâ€ with â€œdegrees of freedomâ€. â€“ In linear models fit with least squares, degrees of freedom is number of non-zeroes. â€“ Unfortunately, it is not always easy to measure â€œdegrees of freedomâ€. Cost of Forward Selection â€¢ Each step of forward selection fits up to â€˜dâ€™ model. â€¢ And we do â€˜dâ€™ steps of forward selection. â€¢ So cost of forward selection is O(d2) times cost of fitting one model. â€¢ For linear regression with squared error, cost is O(nd2 + d3). â€“ So total cost of forward selection would be O(nd4 + d5). Faster Forward Selection for Least Squares â€¢ Instead of fitting models from scratch, we can often speed up forward selection by re-using computation and/or updating models. â€¢ For linear regression with the squared error: â€“ Can reduce O(nd4) term from repeatedly compute O(d2) sub-matrices of XTX: â€¢ Compute XTX once for all, then grab relevant sub-matrix for each model. â€¢ Costs O(nd2) to compute XTX, then O(d2) to grab each sub-matrix. â€¢ Reduces cost of this step to O(nd2 + d4). â€“ Can reduce O(d5) term from solving O(d2) linear systems involving sub-matrices of XTX: â€¢ Each time you add or remove a feature, it is a rank-1 updated to the sub-matrix of XTX. â€¢ By storing factorized XTX sub-matrix, you could do a rank-1 update for O(d2). â€“ And cost of solving a linear system for a factorized matrix is also O(d2). â€¢ Total cost is O(d4) to do this O(d2) times. â€“ So by updating models, can reduce cost from O(nd4 + d5) down to O(nd2 + d4). â€¢ Which is similar to cost of solving one least squares problem, particularly if n>>d. Structure Learning: Unsupervised Feature Selection â€¢ â€œNewsâ€ data: presence of 100 words in 16k newsgroup posts: â€¢ Which words are related to each other? â€¢ Problem of structure learning: unsupervised feature selection. Structure Learning: Unsupervised Feature Selection â€¢ Optimal tree structure: (ignore arrows) NaÃ¯ve Approach: Association Networks â€¢ A naÃ¯ve approach to structure learning (â€œassociation networksâ€): â€“ For each pair of variables, compute a measure of similarity or dependence. â€¢ Using these n2 similarity values either: â€“ Select all pairs whose similarity is above a threshold. â€“ Select the â€œtop kâ€ most similar features to each feature â€˜jâ€™. â€¢ Main problems: â€“ Usually, most variables are dependent (too many edges). â€¢ â€œSickâ€ is getting connected to â€œTuesdaysâ€ even if â€œtacosâ€ are a variable. â€“ â€œTrueâ€ neighbours may not have the highest dependence. â€¢ â€œSickâ€ might get connected to â€œTuesdaysâ€ before it gets connected to â€œmilkâ€. â€¢ (Variation: best tree can be found as minimum spanning tree problem.) Example: Vancouver Rain Data â€¢ Consider modeling the â€œVancouver rainâ€ dataset. â€¢ The strongest signal in the data is the simple relationship: â€“ If it rained yesterday, itâ€™s likely to rain today (> 50% chance that xt-1 = xt). â€“ But an â€œassociation networkâ€ might connect all days (all dependent). Day 1 Day 2 Day 3 Day 4 Day 5 Day 6 Day 7 Day 8 Day 9 â€¦ 0 0 0 1 1 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 Dependency Networks â€¢ A better approach is dependency networks: â€“ For each variable â€˜jâ€™, make it the target in a supervised learning problem. â€“ Now we can use any feature selection method to choose jâ€™s â€œneighboursâ€. â€¢ Forward selection, L1-regularization, ensemble methods, etc. â€¢ Can capture conditional independence: â€“ Might connect â€œsickâ€ to â€œtacosâ€, and â€œtacosâ€ to â€œTuesdaysâ€. â€¢ Without connecting â€œsickâ€ directly to â€œTuesdaysâ€. â€“ Might connect â€œgrandmaâ€ to â€œmomâ€, and â€œmomâ€ to â€œSNPâ€. â€¢ Without connection â€œgrandmaâ€ directly to â€œSNPâ€. Dependency Networks â€¢ Dependency network fit to Vancouver rain data (different Î» values): Dependency Networks â€¢ Variation on dependency networks on digit image pixels:","libVersion":"0.2.1","langs":""}