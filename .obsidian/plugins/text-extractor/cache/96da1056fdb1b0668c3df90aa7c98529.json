{"path":".obsidian/plugins/text-extractor/cache/96da1056fdb1b0668c3df90aa7c98529.json","text":"CPSC 340: Machine Learning and Data Mining Convolutions Spring 2022 (2021W2) Hel p g i v e y o u r T A s t ea c h i n g f eed b a c k! • T h i s t e rm , s o m e o f y o u r T A s g a v e l e c t u r e s . T h e w o u l d l o v e y o u r f e e d b a c k ! • M a n y a r e c o n s i d e r i n g b e c o m i n g p r o f e s s o r s Fr e d ML E & MA P No v e mb e r 3r d Al a n PC A No v e mb e r 6 th (a l s o T r a n s f o r m e r s o n M o n d a y , D e c e m b e r 4 t h ) Be t t y Mo r e P C A No v e mb e r 8 th Cu r t i s Sp a r s e Ma t r i x Fa c t o r i z a t i o n No v e mb e r 10 th Wi l d e r Re c o m m e n d e r Sy ste m s & M D S No v e mb e r 17 th Pl e a s e ad d fe e d b a c k in th e T A se c ti o n o f y o u r n o rm a l co u r s e e v als : Be t t y : Th a n k s fo r yo u r … Al a n : Y o u r l e c t u r e w a s … Deep Learning Convolutional Neural Networks – arguably the most important idea in computer vision Motivation: Automatic Brain Tumor Segmentation · Task: labeling tumors and normal tissue in multi - modal MRI data. Input: Output: · Applications: – Radiation therapy target planning, quantifying treatment responses. – Mining growth patterns, image - guided surgery. · Challenges: – Variety of tumor appearances, similarity to normal tissue. – “You are never going to solve this problem.” Naïve Voxel - Level Classifier · We could treat classifying a voxel as supervised learning: – Standard representation of image: each pixel gets “intensity” between 0 and 255. · We can formulate predicting y i given x i as supervised learning. · But it doesn’t work at all with a linear weighting of these features. Need to Summarize Local Context · The individual pixel intensity values are almost meaningless: – The same x i could lead to different y i . · Intensities not standardized. · Non- trivial overlap in signal for different tissue types. · “Partial volume” effects at boundaries of tissue types. Need to Summarize Local Context · We need to represent the “context” of the pixel (what is around it). – Include all the values of neighbouring pixels as extra features? · Run into coupon collection problems: requires lots of data to find patterns. – Measure neighbourhood summary statistics (mean, variance, histogram)? · Variation on bag of words problem: loses spatial information present in voxels. – Standard approach uses convolutions to represent neighbourhood. Example: Measuring “brightness” of an Area - This pixel is in a “bright” area of the image, which reflects “bleeding” of tumour. - But the actual numeric intensity value of the pixel is the same as in darker “gray matter” areas. - I want a feature saying “this pixel is in a bright area of the image”. - This will us help identify that it’s a tumour pixel. - How to measure brightness in area? Easy way: take average pixel intensity in “neighbourhood”. - Applying this “averaging” to every pixel gives a new image: - We can use “pixel value in new image” as a new feature. - New feature helps identify if pixel is in a “bright” area. The annoying thing about squares · “Take the average of a square window” loses spatial information. · Example: Fixing the “square” issues · Consider instead “blurring” the image. – Gets rid of “local” noise, but better preserves spatial information. · How do you “blur”? – Take weighted average of window, putting more “weight” on “close” pixels: Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution · Taking a “weighted average of neighbours ” is called “convolution”. – Gives you a new (transformed) feature for each pixel Convolution: Big Picture · How do you use convolution to get features? – Apply several different convolutions to your image. – Each convolution gives a different “image” value at each location. – Use theses different image values to give features at each location. Convolutions: Big Picture · What can features coming from convolutions represent? – Some filters give you an average value of the neighbourhood. – Some filters detect edges (directionally) (first derivative) · “Is there a change from dark to bright?” · “If so, from which direction in space?” – Some filters detect lines (“second derivative”) · “Is there a spike or is the change speeding up?” 1D Convolution Example · Consider a 1D “signal” (maybe from sound): – We’ll come back to images later. · For each “time”: – Compute dot- product of signal at surrounding times with a “filter ” of weights. · This gives a new “signal” : – Measures a property of “neighbourhood”. – This particular filter shows a local “how spiky ” value. 1D Convolution (notation is specific to this lecture) · 1D convolution input: – Signal ‘x’ which is a vector length ‘n’. · Indexed by i = 1, 2, …, n – Filter ‘w’ which is a vector of length ‘2m+1’: · Indexed by i = - m, - m+1, …, - 2, 0, 1, 2, …, m - 1, m · Output is a vector of length ‘n’ with elements: – You can think of this as centering w at position ‘ i ’, and taking a dot product of ‘w’ with that “part” x i . 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 1D Convolution · 1D convolution example: – Signal ‘x’: 0 1 1 2 3 5 8 13 – Filter ‘w’: 0 -1 2 -1 0 – Convolution ‘z’: -1 0 -1 -1 1D Convolution Examples · Examples: – “Identity” – “Translation” 1D Convolution Examples · Examples: – “Identity” – “Local Average” Boundary Issue · What can we do about the “?” at the edges ? · Can assign values past the boundaries : · “Zero”: · “Replicate”: · “Mirror”: · Or just ignore the “?” values and return a shorter vector : Formal Convolution Definition · We’ve defined the convolution as: · In other classes you may see it defined as: · For simplicity we’re skipping the “reverse” step , and assuming ‘w’ and ‘x’ are sampled at discrete points (not functions). · But keep this mind if you read about convolutions elsewhere. 1D Convolution Examples · Translation convolution shift signal: – “What is my neighbour’s value?” 1D Convolution Examples · Averaging convolution (“general value of signal in this region?”) – Less sensitive to noise (or spikes) than raw signal. 1D Convolution Examples · Laplacian convolution approximates second derivative: – “Sum to zero” filters “respond” if input vector looks like the filter 1D Convolution Examples · Gaussian convolution (“blurring”): – Compared to averaging it’s more smooth and maintains peaks better. 1D Convolution Examples · Gaussian convolution (“blurring”): – Compared to averaging it’s more smooth and maintains peaks better. 1D Convolution Examples · Centered difference convolution approximates first derivative: – Positive means change from low to high (negative means high to low). 1D Convolution Examples · Sharpen convolution enhances peaks. – An “average” that places negative weights on the surrounding pixels. Digression: Derivatives and Integrals · Numerical derivative approximations can be viewed as filters: – Centered difference: [ - 1, 0, 1] (like check_correctness in the homework code) · Numerical integration approximations can be viewed as filters: – “Simpson’s” rule: [1/6, 4/6, 1/6] (a bit like Gaussian filter). · Derivative filters add to 0 , integration filters add to 1 – For constant function, derivative should be 0 and average = constant. 34 Laplacian of Gaussian Filter · Laplacian of Gaussian is a smoothed 2 nd -derivative approximation: Images and Higher- Order Convolution · 2D convolution: – Signal ‘x’ is the pixel intensities in an ‘n’ by ‘n’ image. – Filter ‘w’ is the pixel intensities in a ‘2m+1’ by ‘2m+1’ image. · The 2D convolution is given by: · 3D and higher-order convolutions are defined similarly. https://towardsdatascience.com/intuitively-understanding-convolutions -for-deep-learning-1f6f42faee1 Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples http://setosa.io/ev/image -kernels Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples Image Convolution Examples 3D Convolution 3D Convolution 3D Convolution 3D Convolution 3D Convolution Convolutions as Features · Classic vision methods use convolutions as features : – Usually have different types/variances/orientations. – Can take maxes across locations/orientations/scales. · Notable convolutions: – Gaussian (blurring/averaging). – Laplace of Gaussian (second- derivative). – Gabor filters (directional first- or higher- derivative). Filter Banks · To characterize context, we used to use filter banks like “MR8”: – 1 Gaussian filter, 1 Laplacian of Gaussian filter. – 6 max(abs(Gabor)) filters: · 3 scales of sine/cosine (maxed over 6 orientations). · Convolutional neural networks (next time!) are replacing filter banks. http://www.robots.ox.ac.uk/~vgg/research/texclass/filters.html Summary · Convolutions are flexible class of signal/image transformations. – Can detect edges (approximate directional derivatives) at different scales – Max(convolutions) can yield features invariant to some transformations. · Filter banks : – Make features for a vision problem by taking a bunch of convolutions. · Next: – Combining this with deep learning. Global and Local Features for Domain Adaptation · Suppose you want to solve a classification task, where you have very little labeled data from your domain. · But you have access to a huge dataset with the same labels, from a different domain. · Example: – You want to label POS tags in medical articles, and pay a few $$$ to label some. – You have access the thousands of examples of Wall Street Journal POS labels. · Domain adaptation: using data from different domain to help. Global and Local Features for Domain Adaptation · “Frustratingly easy domain adaptation”: – Use “global” features across the domains, and “local” features for each domain. – “Global” features let you learn patterns that occur across domains. · Leads to sensible predictions for new domains without any data. – “Local” features let you learn patterns specific to each domain. · Improves accuracy on particular domains where you have more data. – For linear classifiers this would look like: Image Coordinates · Should we use the image coordinates? – E.g., the pixel is at location (124, 78) in the image. · Considerations: – Is the interpretation different in different areas of the image? – Are you using a linear model? · Would “distance to center” be more logical? – Do you have enough data to learn about all areas of the image? Alignment- Based Features · The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. · We aligned the images with a “template image”. Alignment- Based Features · The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. · We aligned the images with a “template image”. – Allowed “alignment- based” features: Motivation: Automatic Brain Tumor Segmentation · Final features for brain tumour segmentation: – Gaussian convolution of original/template/priors/symmetry, Laplacian of Gaussian on original. · All with 3 variances. · Max(Gabor) with sine and cosine on orginal (3 variances). Motivation: Automatic Brain Tumour Segmentation · Logistic regression and SVMs among best methods. – When using these 72 features from last slide. – If you used all features I came up with, it overfit . · Possible solutions to overfitting: – Forward selection was too slow. · Just one image gives 8 million training examples. – I did manual feature selection (“guess and check”). – L2- regularization with all features also worked. · But this is slow at test time . · L1- regularization gives best of regularization and feature selection. FFT implementation of convolution · Convolutions can be implemented using fast Fourier transform: – Take FFT of image and filter, multiply elementwise, and take inverse FFT. · It has faster asymptotic running time but there are some catches: – You need to be using periodic boundary conditions for the convolution. – Constants matter: it may not be faster in practice. · Especially compared to using GPUs to do the convolution in hardware. – The gains are largest for larger filters (compared to the image size). 74 SIFT Features · Scale-invariant feature transform (SIFT): – Features used for object detection (“is particular object in the image”?) – Designed to detect unique visual features of objects at multiple scales. – Proven useful for a variety of object detection tasks. http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html CPSC 340: Machine Learning and Data Mining Convolutional Neural Networks (and miscellaneous deep learning tricks) Spring 2022 (2021W2) 1D Convolution as Matrix Multiplication · 1D convolution: – Takes signal ‘x’ and filter ‘w’ to produces vector ‘z’: – Can be written as a matrix multiplication: Typo: the signs of -1 and 2 are wrong in the matrix 1D Convolution as Matrix Multiplication · Each element of a convolution is an inner product: · So convolution is a matrix multiplication (I’m ignoring boundaries): · The shorter ‘w’ is, the more sparse the matrix is. 2D Convolution as Matrix Multiplication · 2D convolution: – Signal ‘x’, filter ‘w’, and output ‘z’ are now all images/matrices: – Vectorized ‘z’ can be written as a matrix multiplication with vectorized ‘x’: Motivation for Convolutional Neural Networks · Consider training neural networks on 256 by 256 images. – This is 256 by 256 by 3 ≈ 200,000 inputs. · If first layer has k=10,000, then it has about 2 billion parameters . – We want to avoid this huge number (due to storage and overfitting). · Key idea: make Wx i act like several convolutions (to make it sparse): 1. Each row of W only applies to part of x i . 2. Use the same parameters between rows. · Forces most weights to be zero, reduces number of parameters. Motivation for Convolutional Neural Networks · Classic vision methods uses fixed convolutions as features: – Usually have different types/variances/orientations. – Can do subsampling or take maxes across locations/orientations/scales. Motivation for Convolutional Neural Networks · Convolutional neural networks learn the convolutions: – Learning ‘W’ and ‘v’ automatically chooses types/variances/orientations. – Don’t pick from fixed convolutions, but learn the elements of the filters. Motivation for Convolutional Neural Networks · Convolutional neural networks learn the convolutions: – Learning ‘W’ and ‘v’ automatically chooses types/variances/orientations. – Can do multiple layers of convolution to get deep hierarchical features. http://fortune.com/ai-artificial-intelligence -deep-machine -learning/ Two Main Motivations · Translation invariance (data -efficient to learn, less likely to overfit) · Hierarchy Hierarchically composed feature representations Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. – Pooling layer : combine results of convolutions. · Can add some invariance or just make the number of parameters smaller. · Often ‘max pooling ’: Convolutional Neural Networks · Convolutional Neural Networks classically have 3 layer “types”: – Fully connected layer : usual neural network layer with unrestricted W. – Convolutional layer : restrict W to act like several convolutions. – Pooling layer : combine results of convolutions. · Can add some invariance or just make the number of parameters smaller. · Often ‘max pooling’ or else ‘average pooling ’: Max Pooling vs Average Pooling · Both downsample the image · Max pooling: “any of these options is present” – Much more common, especially in early layers – “There’s an edge here, but I don’t really care how thick it is” · Average pooling: “all/most of these options are present” – If used, more often at the end of the network – “Most of the big patches look like a picture of a train“ LeNet for Optical Character Recognition http://blog.csdn.net/strint/article/details/44163869 Deep Hierarchies in the Visual System http://www.strokenetwork.org/newsletter/articles/vision.htm https://en.wikibooks.org/wiki/Sensory_Systems/Visual_Signal_Processing Deep Hierarchies in Optics http://www.argmin.net/2018/01/25/optics/ We stopped here","libVersion":"0.2.1","langs":""}