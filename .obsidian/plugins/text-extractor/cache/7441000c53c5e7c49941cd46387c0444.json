{"path":".obsidian/plugins/text-extractor/cache/7441000c53c5e7c49941cd46387c0444.json","text":"UBC CPSC 340 2018W1 MIDTERM EXAM Oct 18th, 2018 Instructors: Mark Schmidt and Mike Gelbart TIME: 80 minutes We are providing a copy of this exam to help you prepare for the style of questions we may ask during the midterm and ﬁnal. However, the solution ﬁle is meant for you alone and we do not give permission to share these solution ﬁles with anyone. Both distributing solution ﬁles to other people or using solution ﬁles provided to you by other people are considered academic misconduct. Please see UBC’s policy on this topic if you are not familiar with it: http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,959 http://www.calendar.ubc.ca/vancouver/index.cfm?tree=3,54,111,960 • Do not open the exam until you are directed to do so. • Once you open the exam, make sure that it contains this cover page plus 9 pages of exam questions. • One letter-size sheet (both sides) of notes is allowed. No other material or accessories may be used. • You may use either pen or pencil, but exams written in pencil may not be eligible for regrading. • Please be prepared to present, upon request, a student card for identiﬁcation. • If you need more space, use the blank page at the end of the exam, and clearly indicate that your work continues there. • Most questions require a short answer. Work eﬃciently and avoid writing lengthy answers. • Unless otherwise stated, n refers to the number of training examples and d is the number of features. • If anything is unclear or seems ambiguous, state your assumptions. Question: 1 2 3 4 5 6 7 8 Total Points: 8 6 6 6 6 6 6 6 50 Score: CPSC 340 Midterm Exam Question 1. (8 points) Answer the questions below using 1-2 short sentences. (a)2 pts What are two diﬀerences between KNN and k-means? (No more than two, please.) (b)2 pts Is it possible to have a machine learning model that makes predictions in O(1) time? If yes, give an example; if no, explain why not. (c)2 pts You’re working on a machine learning problem and decide you need more data. You collect twice as much training data but end up with the same validation error for your parametric model. Are you likely experiencing underﬁtting or overﬁtting? Brieﬂy justify your answer. (d)2 pts What is an advantage and a disadvantage of using more folds with cross-validation? Page 1 of 9 CPSC 340 Midterm Exam Question 2. (6 points) Answer the questions below using 1-2 short sentences. (a)2 pts Assume you have a classiﬁer that takes O(nd) time to train and O(td) time to predict on t examples, like naive Bayes. If you are testing p possible values of a hyper-parameter, what is the cost of choosing the best value of the hyper-parameter using k-fold cross-validation? Express the result in terms of n, d, t, k, and p (if these aﬀect the runtime). (b)2 pts Consider the “consistent nearest neighbour” classiﬁer: it runs our usual KNN classiﬁer but instead of viewing k as a hyper-parameter it always sets k = ⌈log(n)⌉ (the logarithm of n rounded up to the nearest integer). Would call this a parametric classiﬁer or a non-parametric classiﬁer? Brieﬂy justify your answer. (c)2 pts Consider the “condensed nearest neighbour” classiﬁer: at training time it chooses the c “best” training examples (where c is a hyper-parameter), and at test time uses the usual KNN prediction but based only on these c training examples. Would call this a parametric classiﬁer or a non-parametric classiﬁer? Brieﬂy justify your answer. Page 2 of 9 CPSC 340 Midterm Exam Question 3. (6 points) Answer the questions below using 1-2 short sentences. (a)2 pts Does it make sense to do k-means clustering with k > n? Brieﬂy justify your answer. (b)2 pts Does it make sense to do k-means clustering with k > d? Brieﬂy justify your answer. (c)2 pts In k-means we can often obtain a much better clustering by using a large number of random initializations of the initial means. In DBSCAN (density-based clustering), we could randomize the order of the training examples that we test for new clusters. Is it generally a good idea to run DBSCAN with a large number of diﬀerent random orderings? Brieﬂy justify your answer. Page 3 of 9 CPSC 340 Midterm Exam Question 4. (6 points) Answer the questions below using 1-2 short sentences. (a)2 pts Name one advantage and one disadvantage of using gradient descent instead of the normal equations to ﬁt a least squares linear regression model. (b)2 pts When we do regression with a polynomial basis, how does the degree of the polynomial p aﬀect the two parts of the fundamental trade-oﬀ? (c)2 pts Construct a matrix X where the least squares solution would not be unique. Page 4 of 9 CPSC 340 Midterm Exam Question 5. (6 points) Loss functions. (a)2 pts Describe a situation where using a linear regression model with the squared error could give very misleading results. (b)2 pts Consider the loss function f (r) = ∑n i=1 max{ri, −2ri}. Write down a version of this loss function that is smoothed with the log-sum-exp. (c)2 pts The Huber loss has a hyperparameter, δ, which controls where you switch from a parabola to a constant slope. What could go wrong if δ is set to an extremely large (far from zero) value? Page 5 of 9 CPSC 340 Midterm Exam Question 6. (6 points) Consider the binary classiﬁcation training data set shown below. The two classes are denoted with o’s and ×’s. (a)3 pts What is the minimum depth of decision tree needed to get zero training error on this data set? Brieﬂy explain your reasoning. Feel free to draw on the plot if it helps. (b)3 pts What is the maximum value of k such that KNN gets zero training error on this data set? Assume ties are broken by voting for class +1 (the circles). Brieﬂy explain your reasoning. Feel free to draw on the plot if it helps. Page 6 of 9 CPSC 340 Midterm Exam Question 7. (6 points) Consider the following one-dimensional data set: X =     −3 4 −1 3     , y =     1 2 3 4     . (a)2 pts Write down the Z matrix for using a polynomial basis with p = 2 (quadratic) on this data set. Use the same standard format/notation that we used in the lectures and assignments. (b)2 pts Let’s say the weights come out to be v =   −1 3 0  . What is the model’s prediction for ˜x = 2? (c)2 pts If we instead used p = 1 on this dataset, would we expect the weights to be [ −1 3 ] (i.e. the ﬁrst two elements of our previous v), or something diﬀerent? Brieﬂy justify your answer. Page 7 of 9 CPSC 340 Midterm Exam Question 8. (6 points) (a)3 pts Consider the following objective, which considers a weighted worst-case error with a penalty on the absolute value of the weights, f (w) = max i∈{1,2,...,n} {vi|wT xi − yi|} + λ d∑ j=1 |wj|, where λ is a non-negative scalar. Re-write this objective function in matrix and norm notation. You can use V as a diagonal matrix with the elements vi along the diagonal. (b)3 pts Consider the L2-regularized tilted least squares objective, f (w) = 1 2 n∑ i=1(wT xi − yi)2 + λ 2 d∑ j=1 w2 j + d∑ j=1 vjwj, where the λ is a non-negative scalar and the vj are real-valued “tilting” variables. Write down a linear system whose solution minimizes this (convex and quadratic) objective function. You can use v as a vector containing the vj values. Page 8 of 9 CPSC 340 Midterm Exam This page is intentionally blank. You can use it for scratch work or to continue an answer if you run out of space somewhere. End of exam Page 9 of 9 End of exam","libVersion":"0.2.1","langs":""}