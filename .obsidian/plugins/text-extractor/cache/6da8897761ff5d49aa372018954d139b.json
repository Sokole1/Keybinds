{"path":".obsidian/plugins/text-extractor/cache/6da8897761ff5d49aa372018954d139b.json","text":"CPSC 340: Machine Learning and Data Mining Regularization Admin â€“ Assignment Issues â€¢ If you work in groups, please submit together on Gradescope! â€“ Do not submit two identical assignments, the TAs will appreciate it. â€¢ Can you spot the difference between these two answers? â€“ One of the above is correct and the other could be academic misconduct. Last Time: Feature Selection â€¢ Last time we discussed feature selection: â€“ Choosing set of â€œrelevantâ€ features. â€¢ Most common approach is search and score: â€“ Define â€œscoreâ€ and â€œsearchâ€ for features with best score. â€¢ But itâ€™s hard to define the â€œscoreâ€ and itâ€™s hard to â€œsearchâ€. â€“ So we often use greedy methods like forward selection. â€¢ Methods work ok on â€œtoyâ€ data, but are frustrating on real data. â€“ Different methods may return very different results. â€“ Defining whether a feature is â€œrelevantâ€ is complicated and ambiguous. Last Time: Is â€œRelevanceâ€ Clearly Defined? â€¢ Consider a supervised classification task: â€¢ True model: â€“ (SNP = mom) with very high probability. â€“ (SNP != mom) with some very low probability. â€¢ What about (maternal) â€œgrandmaâ€? â€“ Irrelevant since provides no extra information beyond â€œmomâ€. â€“ But relevant if you do not have the â€œmomâ€ feature. gender mom dad F 1 0 M 0 1 F 0 0 F 1 1 SNP 1 0 0 1 https://en.wikipedia.org/wiki/Human_mitochondrial_genetics Is â€œRelevanceâ€ Clearly Defined? â€¢ What if we donâ€™t know â€œmomâ€ or â€œgrandmaâ€? â€¢ Now there are no relevant variables, right? â€“ But â€œdadâ€ and â€œmomâ€ must have some common maternal ancestor. â€“ â€œMitochondrial Eveâ€ estimated to be ~200,000 years ago. â€¢ A â€œrelevantâ€ feature may have a tiny effect. SNP 1 0 0 1 gender dad F 0 M 1 F 0 F 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ What if we donâ€™t know â€œmomâ€ or â€œgrandmaâ€? â€¢ Now there are no relevant variables, right? â€“ What if â€œmomâ€ likes â€œdadâ€ because he has the same SNP as her? â€¢ Confounding factors can change â€œrelevanceâ€ of variables. SNP 1 0 0 1 gender dad F 0 M 1 F 0 F 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ What if we add â€œsiblingâ€? â€¢ Sibling is â€œrelevantâ€ for predicting SNP, but it is not the cause. â€¢ â€œRelevanceâ€ for prediction does not imply a causal relationship. â€“ Causality can even be reversedâ€¦ SNP 1 0 0 1 gender dad sibling F 0 1 M 1 0 F 0 0 F 1 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ What if donâ€™t have â€œmomâ€ but we have â€œbabyâ€? â€¢ â€œBabyâ€ is relevant when (gender == F). â€“ â€œBabyâ€ is relevant (though causality is reversed). â€“ Is â€œgenderâ€ relevant? â€¢ If we want to find relevant causal factors, â€œgenderâ€ is not relevant. â€¢ If we want to predict SNP, â€œgenderâ€ is relevant. â€¢ â€œRelevanceâ€ may depend on values of certain features. â€“ â€œContext-specificâ€ relevance. SNP 1 0 0 1 gender dad baby F 0 1 M 1 1 F 0 0 F 1 1 Is â€œRelevanceâ€ Clearly Defined? â€¢ Warnings about feature selection: â€“ If features can be predicted from features, you canâ€™t know which to pick. â€“ A feature is only â€œrelevantâ€ in the context of available features. â€“ A â€œrelevantâ€ feature may have a tiny effect. â€“ Confounding factors can change whether features are relevant. â€“ â€œRelevanceâ€ for prediction does not imply a causal relationship. â€“ â€œRelevanceâ€ may be conditional on values of certain features. Is this hopeless? â€¢ We often want to do feature selection we so have to try! â€¢ Different methods are affected by problems in different ways. â€¢ These â€œproblemsâ€ donâ€™t have right answers but have wrong answers: â€“ Variable dependence (â€œmomâ€ and â€œmom2â€ have same information). â€¢ But should take at least one. â€“ Conditional independence (all â€œgrandmaâ€ information is captured by â€œmomâ€). â€¢ Should take â€œgrandmaâ€ only if â€œmomâ€ missing. â€¢ These â€œproblemsâ€ have application-specific answers: â€“ Tiny effects. â€“ Context-specific relevance (is â€œgenderâ€ relevant if given â€œbabyâ€?). â€¢ See bonus slides for discussion of causality and confounding issues. â€“ Unless you control data collection, standard feature selection methods cannot address those issues. My advice if you want the â€œrelevantâ€ variables. â€¢ Try the association approach. â€¢ Try forward selection with different values of Î». â€¢ Try out a few other feature selection methods too. â€¢ Discuss the results with the domain expert. â€“ They probably have an idea of why some variables might be relevant. â€¢ Do not be overconfident: â€“ These methods are probably not discovering how the world truly works. â€“ â€œThe algorithm has found that these variables are helpful in predicting yi.â€ â€¢ Then a warning that these models are not perfect at finding relevant variables. Related: Survivorship Bias â€¢ Plotting location of bullet holes on planes returning from WW2: â€¢ Where are the â€œrelevantâ€ parts of the plane to protect? â€“ â€œRelevantâ€ parts are actually where there are no bullets. â€“ Planes shot in other places did not come back (armor was needed). https://en.wikipedia.org/wiki/Survivorship_bias Related: Survivorship Bias â€¢ Plotting location of bullet holes on planes returning from WW2: â€¢ This is an example of â€œsurvivorship biasâ€: â€“ Data is not IID because you only sample the â€œsurvivorsâ€. â€“ Causes havoc for feature selection, and ML methods in general. https://en.wikipedia.org/wiki/Survivorship_bias Related: Survivorship Bias â€¢ Plotting location of bullet holes on planes returning from WW2: â€¢ People come to wrong conclusions due to survivor bias all the time. â€“ Article on â€œsecrets of successâ€, focusing on traits of successful people. â€¢ But ignoring the number of non-super-successful people with the same traits. â€“ Article hypothesizing about various topics (allergies, mental illness, etc.). https://en.wikipedia.org/wiki/Survivorship_bias Next Topic: Regularization Recall: Polynomial Degree and Training vs. Testing â€¢ Weâ€™ve said that complicated models tend to overfit more. â€¢ But what if we need a complicated model? http://www.cs.ubc.ca/~arnaud/stat535/slides5_revised.pdf Controlling Complexity â€¢ Usually â€œtrueâ€ mapping from xi to yi is complex. â€“ Might need high-degree polynomial. â€“ Might need to combine many features, and do not know â€œrelevantâ€ ones. â€¢ But complex models can overfit. â€¢ So what do we do??? â€¢ Our main tools: â€“ Model averaging: average over multiple models to decrease variance. â€“ Regularization: add a penalty on the complexity of the model. Would you rather? â€¢ Consider the following dataset and 3 linear regression models: â€¢ Which line should we choose? Would you rather? â€¢ Consider the following dataset and 3 linear regression models: â€¢ What if you are forced to choose between red and green? â€“ And assume they have the same training error. â€¢ You should pick green. â€“ Since slope is smaller, small change in xi has a smaller change in prediction yi. â€¢ Green lineâ€™s predictions are less sensitive to having â€˜wâ€™ exactly right. â€“ Since green â€˜wâ€™ is less sensitive to data, test error might be lower. Size of Regression Weights and Overfitting â€¢ The regression weights wj with degree-7 are huge in this example. â€¢ The degree-7 polynomial would be less sensitive to the data, if we â€œregularizedâ€ the wj so that they are small. L2-Regularization â€¢ Standard regularization strategy is L2-regularization: â€“ For some regularization parameter ğœ† > 0. â€¢ Intuition: large slopes wj tend to lead to overfitting. â€¢ Objective balances getting low error vs. having small slopes â€˜wjâ€™. â€“ â€œYou can increase the training error if it makes â€˜wâ€™ much smaller.â€ â€“ Nearly-always reduces overfitting. â€¢ In terms of fundamental trade-off: â€“ Regularization increases training error. â€“ Regularization decreases generalization gap. L2-Regularization â€¢ Visualizing squared error as a function of parameters (d=2): L2-Regularization â€¢ L2-regularized least squares: â€“ Regularization parameter Î» > 0 controls â€œstrengthâ€ of regularization. â€¢ Large Î» puts large penalty on slopes (worse training error, better generalization gap). â€¢ How should you choose Î»? â€“ Theory: as â€˜nâ€™ grows Î» should be in the range O(1) to (âˆšn). â€“ Practice: optimize validation set or cross-validation error. â€¢ This almost always decreases the test error. L2-Regularization â€œShrinkingâ€ Example â€¢ Solution to a â€œleast squares with L2-regularizationâ€ for different Î»: â€¢ We get least squares with Î» = 0. â€“ But we can achieve similar training error with smaller ||w||. â€¢ ||Xw â€“y || increases with Î», and ||w|| decreases with Î». â€“ Though individual wj can increase or decrease with lambda. â€“ Because we use the L2-norm, the large ones decrease the most. Î» w1 w2 w3 w4 w5 0 -1.88 1.29 -2.63 1.78 -0.63 1 -1.88 1.28 -2.62 1.78 -0.64 4 -1.87 1.28 -2.59 1.77 -0.66 16 -1.84 1.27 -2.50 1.73 -0.73 64 -1.74 1.23 -2.22 1.59 -0.90 256 -1.43 1.08 -1.70 1.18 -1.05 1024 -0.87 0.73 -1.03 0.57 -0.81 4096 -0.35 0.31 -0.42 0.18 -0.36 ||Xw â€“ y||2 ||w||2 285.64 15.68 285.64 15.62 285.64 15.43 285.71 14.76 286.47 12.77 292.60 8.60 321.29 3.33 374.27 0.56 Regularization Path â€¢ Regularization path is a plot of the optimal weights â€˜wjâ€™ as â€˜Î»â€™ varies: â€¢ Starts with least squares with Î»= 0, and wj converge to 0 as Î» grows. Regularizing L2-norm vs. Squared L2-norm â€¢ Why do we regularize the square of the L2-norm? â€“ Why not regularize the L2-norm instead? â€¢ Regularization path is same for L2-norm and squared L2-norm. â€“ Though with different values of ğœ†. â€¢ But squaring the L2-norm makes sure the gradient exists. â€“ The L2-norm is non-differentiable at w=0. â€“ But the squared L2-norm is still differentiable at 0. Solving L2-Regularized Least Squares Problem â€¢ Solving for ğ›»f(w)=0 to compute L2-regularized least squares: â€“ Objective: â€“ Gradient: â€“ Setting gradient equal to zero vector: â€“ Factorize â€˜wâ€™ on the left side (identity matrix makes dimensions match): Gradient Descent for L2-Regularized Least Squares â€¢ The L2-regularized least squares objective and gradient: â€¢ Gradient descent iterations for L2-regularized least squares: â€¢ Cost of gradient descent iteration is still O(nd). â€“ Can show number of iterations decrease as Î» increases (not obvious). Why use L2-Regularization? â€¢ Itâ€™s a weird thing to do, but Mark says â€œalways use regularizationâ€. â€“ â€œAlmost always decreases test errorâ€ should already convince you. â€¢ But here are 6 more reasons: 1. Solution â€˜wâ€™ is unique. 2. XTX does not need to be invertible (no collinearity issues). 3. Less sensitive to changes in X or y. 4. Gradient descent converges faster (bigger Î» means fewer iterations). 5. Steinâ€™s paradox: if d â‰¥ 3, â€˜shrinkingâ€™ moves us closer to â€˜trueâ€™ w. 6. Worst case: just set Î» small and get the same performance. Next Topic: Standardizing Features Features with Different Scales â€¢ Consider continuous features with different scales: â€¢ Should we convert to some standard â€˜unitâ€™? â€“ It doesnâ€™t matter for decision trees or naÃ¯ve Bayes. â€¢ They only look at one feature at a time. â€“ It does not matter for least squares: â€¢ wj*(100 mL) gives the same model as wj*(0.1 L) with a different wj. Egg (#) Milk (mL) Fish (g) Pasta (cups) 0 250 0 1 1 250 200 1 0 0 0 0.5 2 250 150 0 Features with Different Scales â€¢ Consider continuous features with different scales: â€¢ Should we convert to some standard â€˜unitâ€™? â€“ It matters for k-nearest neighbours: â€¢ â€œDistanceâ€ will be affected more by large features than small features. â€“ It matters for regularized least squares: â€¢ Penalizing (wj)2 means different things if features â€˜jâ€™ are on different scales. Egg (#) Milk (mL) Fish (g) Pasta (cups) 0 250 0 1 1 250 200 1 0 0 0 0.5 2 250 150 0 Standardizing Features â€¢ It is common to standardize continuous features: â€“ For each feature: 1. Compute mean and standard deviation: 2. Subtract mean and divide by standard deviation (â€œz-scoreâ€) â€“ Now measures â€œstandard deviations from meanâ€. â€¢ And changes in â€˜wjâ€™ have similar effect for any feature â€˜jâ€™ â€¢ How should we standardize test data? â€“ Wrong approach: use mean and standard deviation of test data. â€¢ Training and test mean and standard deviation might be very different. â€“ Right approach: use mean and standard deviation of training data. Standardizing Features â€¢ It is common to standardize continuous features: â€“ For each feature: 1. Compute mean and standard deviation: 2. Subtract mean and divide by standard deviation (â€œz-scoreâ€) â€“ Now measures â€œstandard deviations from meanâ€. â€¢ And changes in â€˜wjâ€™ have similar effect for any feature â€˜jâ€™ â€¢ If we are doing 10-fold cross-validation: â€“ Compute Âµj and Ïƒj based on the 9 training folds (e.g., average over 9/10s of data). â€¢ Standardize the remaining (â€œvalidationâ€) fold with this â€œtrainingâ€ Âµj and Ïƒj. â€“ Re-standardizing for different folds. â€¢ If you standardized before splitting, then validation fold would be influencing training. Standardizing Target â€¢ In regression, we sometimes standardize the targets yi. â€“ Puts targets on the same standard scale as standardized features: â€¢ With standardized target, setting w = 0 predicts average yi: â€“ High regularization makes us predict closer to the average value. â€¢ Again, make sure you standardize test data with the training stats. â€“ And do not forget to â€œun-standardizeâ€ predictions to get back to original space. â€¢ Other common transformations of yi are logarithm/exponent: â€“ Makes sense for geometric/exponential processes. Regularizing the y-Intercept? â€¢ Should we regularize the y-intercept? â€¢ No! Why encourage it to be closer to zero? (It could be anywhere.) â€“ You should be allowed to shift function up/down globally. â€¢ Yes! It makes the solution unique and it easier to compute â€˜wâ€™. â€¢ Compromise: regularize by a smaller amount than other variables. Summary â€¢ â€œRelevanceâ€ is really hard to define. â€“ Post-lecture bonus: â€œrough guideâ€ to how different methods deal with this issue. â€¢ Regularization: â€“ Adding a penalty on model complexity. â€¢ L2-regularization: penalty on L2-norm of regression weights â€˜wâ€™. â€“ Trades training error against size of weights, almost always improves test error. â€¢ Standardizing features: â€“ For some models it makes sense to have features on the same scale. â€¢ Next time: learning with an exponential number of irrelevant features. Rough Guide to Feature Selection Method\\Issue Dependence Conditional Independence Tiny effects Context-Specific Relevance Association (e.g., measure correlation between features â€˜jâ€™ and â€˜yâ€™) Ok (takes â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Ignores Bad (misses features that must interact, â€œgenderâ€ irrelevant given â€œbabyâ€) Rough Guide to Feature Selection Method\\Issue Dependence Conditional Independence Tiny effects Context-Specific Relevance Association (e.g., measure correlation between features â€˜jâ€™ and â€˜yâ€™) Ok (takes â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Ignores Bad (misses features that must interact, â€œgenderâ€ irrelevant given â€œbabyâ€) Regression Weight (fit least squares, take biggest |wj|) Bad (can take irrelevant but collinear, can take none of â€œmom1-3â€) Ok (takes â€œmomâ€ not â€œgrandmaâ€, if linear and â€˜nâ€™ large. Ignores (unless collinear) Ok (if linear, â€œgenderâ€ relevant give â€œbabyâ€) Rough Guide to Feature Selection Method\\Issue Dependence Conditional Independence Tiny effects Context-Specific Relevance Association (e.g., measure correlation between features â€˜jâ€™ and â€˜yâ€™) Ok (takes â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Ignores Bad (misses features that must interact, â€œgenderâ€ irrelevant given â€œbabyâ€) Regression Weight (fit least squares, take biggest |wj|) Bad (can take irrelevant but collinear, can take none of â€œmom1-3â€) Ok (takes â€œmomâ€ not â€œgrandmaâ€, if linear and â€˜nâ€™ large. Ignores (unless collinear) Ok (if linear, â€œgenderâ€ relevant give â€œbabyâ€) Search and Score w/ Validation Error Ok (takes at least one of â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Allows Ok (â€œgenderâ€ relevant given â€œbabyâ€) Rough Guide to Feature Selection â€¢ gvhc Method\\Issue Dependence Conditional Independence Tiny effects Context-Specific Relevance Association (e.g., measure correlation between features â€˜jâ€™ and â€˜yâ€™) Ok (takes â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Ignores Bad (misses features that must interact, â€œgenderâ€ irrelevant given â€œbabyâ€) Regression Weight (fit least squares, take biggest |wj|) Bad (can take irrelevant but collinear, can take none of â€œmom1-3â€) Ok (takes â€œmomâ€ not â€œgrandmaâ€, if linear and â€˜nâ€™ large. Ignores (unless collinear) Ok (if linear, â€œgenderâ€ relevant give â€œbabyâ€) Search and Score w/ Validation Error Ok (takes at least one of â€œmomâ€ and â€œmom2â€) Bad (takes â€œgrandmaâ€, â€œgreat-grandmaâ€, etc.) Allows (many false positives) Ok (â€œgenderâ€ relevant given â€œbabyâ€) Search and Score w/ L0-norm Ok (takes exactly one of â€œmomâ€ and â€œmom2â€) Ok (takes â€œmomâ€ not grandma if linear-ish). Ignores (even if collinear) Ok (â€œgenderâ€ relevant given â€œbabyâ€) Alternative to Search and Score: good old p-values â€¢ Hypothesis testing (â€œconstraint-basedâ€) approach: â€“ Generalization of the â€œassociationâ€ approach to feature selection. â€“ Performs a sequence of conditional independence tests. â€“ If they are independent (like â€œp < .05â€), say that â€˜jâ€™ is â€œirrelevantâ€. â€¢ Common way to do the tests: â€“ â€œPartialâ€ correlation (numerical data). â€“ â€œConditionalâ€ mutual information (discrete data). Testing-Based Feature Selection â€¢ Hypothesis testing (â€œconstraint-basedâ€) approach: â€¢ Two many possible tests, â€œgreedyâ€ method is for each â€˜jâ€™ do: â€¢ â€œAssociation approachâ€ is the greedy method where you only do the first test (subsequent tests remove a lot of false positives). Hypothesis-Based Feature Selection â€¢ Advantages: â€“ Deals with conditional independence. â€“ Algorithm can explain why it thinks â€˜jâ€™ is irrelevant. â€“ Doesnâ€™t necessarily need linearity. â€¢ Disadvantages: â€“ Deals badly with exact dependence: doesnâ€™t select â€œmomâ€ or â€œmom2â€ if both present. â€“ Usual warning about testing multiple hypotheses: â€¢ If you test p < 0.05 more than 20 times, youâ€™re going to make errors. â€“ Greedy approach may be sub-optimal. â€¢ Neither good nor bad: â€“ Allows tiny effects. â€“ Says â€œgenderâ€ is irrelevant when you know â€œbabyâ€. â€“ This approach is sometimes better for finding relevant factors, not to select features for learning. Causality â€¢ None of these approaches address causality or confounding: â€“ â€œMomâ€ is the only relevant direct causal factor. â€“ â€œDadâ€ is really irrelevant. â€“ â€œGrandmaâ€ is causal but is irrelevant if we know â€œmomâ€. â€¢ Other factors can help prediction but arenâ€™t causal: â€¢ â€œSiblingâ€ is predictive due to confounding of effect of same â€œmomâ€. â€¢ â€œBabyâ€ is predictive due to reverse causality. â€¢ â€œGenderâ€ is predictive due to common effect on â€œbabyâ€. â€¢ We can sometimes address this using interventional dataâ€¦ Interventional Data â€¢ The difference between observational and interventional data: â€“ If I see that my watch says 10:45, class is almost over (observational). â€“ If I set my watch to say 10:45, it doesnâ€™t help (interventional). â€¢ The intervention can help discover causal effects: â€“ â€œWatchâ€ is only predictive of â€œtimeâ€ in observational setting (so not causal). â€¢ General idea for identifying causal effects: â€“ â€œForceâ€ the variable to take a certain value, then measure the effect. â€¢ If the dependency remains, there is a causal effect. â€¢ We â€œbreakâ€ connections from reverse causality, common effects, or confounding. Causality and Dataset Collection â€¢ This has to do with the way you collect data: â€“ You canâ€™t â€œlookâ€ for variables taking the value â€œafter the factâ€. â€“ You need to manipulate the value of the variable, then watch for changes. â€¢ This is the basis for randomized control trial in medicine: â€“ Randomly assigning pills â€œforcesâ€ value of â€œtreatmentâ€ variable. â€¢ Randomization means they arenâ€™t taking the pill due to confounding factors. â€¢ Differences between people who did and did not take pill should be caused by pill. â€“ Include a â€œcontrolâ€ as a value to prevent placebo effect as confounding. â€¢ See also Simpsonâ€™s Paradox: â€“ https://www.youtube.com/watch?v=ebEkn-BiW5k L2-Regularization â€¢ Standard regularization strategy is L2-regularization: â€¢ Equivalent to minimizing squared error but keeping L2-norm small. Regularization/Shrinking Paradox â€¢ We throw darts at a target: â€“ Assume we donâ€™t always hit the exact center. â€“ Assume the darts follow a symmetric pattern around center. Regularization/Shrinking Paradox â€¢ We throw darts at a target: â€“ Assume we donâ€™t always hit the exact center. â€“ Assume the darts follow a symmetric pattern around center. â€¢ Shrinkage of the darts : 1. Choose some arbitrary location â€˜0â€™. 2. Measure distances from darts to â€˜0â€™. Regularization/Shrinking Paradox â€¢ We throw darts at a target: â€“ Assume we donâ€™t always hit the exact center. â€“ Assume the darts follow a symmetric pattern around center. â€¢ Shrinkage of the darts : 1. Choose some arbitrary location â€˜0â€™. 2. Measure distances from darts to â€˜0â€™. 3. Move misses towards â€˜0â€™, by small amount proportional to distance from 0. â€¢ If small enough, darts will be closer to center on average. Regularization/Shrinking Paradox â€¢ We throw darts at a target: â€“ Assume we donâ€™t always hit the exact center. â€“ Assume the darts follow a symmetric pattern around center. â€¢ Shrinkage of the darts : 1. Choose some arbitrary location â€˜0â€™. 2. Measure distances from darts to â€˜0â€™. 3. Move misses towards â€˜0â€™, by small amount proportional to distance from 0. â€¢ If small enough, darts will be closer to center on average. Visualization of the related higher-dimensional paradox that the mean of data coming from a Gaussian is not the best estimate of the mean of the Gaussian in 3-dimensions or higher: https://www.naftaliharris.com/blog/steinviz","libVersion":"0.2.1","langs":""}