{"path":".obsidian/plugins/text-extractor/cache/66d1fc24d05378967a2cf5024a683e47.json","text":"Selected Solutions to Exercises - Chapter 7 CPSC 302 Fall 2023 Question 7.3 We would never use an iterative method for solving a 2 × 2 linear system, but this question is useful in understanding the concept of an iteration matrix. (a) The iteration matrices are given by T = I − M −1A, where M corresponds to the splitting A = M − N . For Jacobi, Gauss-Seidel, and SOR we have TJ = (0 1 2 1 2 0 ) ; TGS = ( 0 1 2 0 1 4 ) ; TSOR = ( 1 − ω ω 2 ω 2 (1 − ω) ( ω 2 − 1)2 ) . (b) The spectral radii are ρJ = max( 1 2 , − 1 2 ) = 1 2 and ρGS = max( 1 4 , 0) = 1 4 . Jacobi has an asymptotic rate of convergence − log10 ρJ ≈ 0.3010 and Gauss-Seidel has an asymptotic rate of convergence − log10 ρGS ≈ 0.6020. (c) This question involves writing a Matlab program and is skipped (d) In this case the matrix is a tiny version of the 1D Laplacian, and therefore the formula for the optimal parameter on slide 27 of the pre-lecture slides (Chapter 7) holds; this is not obvious in general, but we know it is correct for Laplacians. The optimal relaxation parameter is thus ωopt = 2 1+sin(π/3) = 1.0718, and the optimal spectral radius is ρSOR = ωopt − 1 = 0.0718. We have − log10 ρSOR ≈ 1.1439. This convergence rate is approximately 1.9 times larger than that of Gauss-Seidel and 3.8 larger than that of Jacobi. Question 7.7 (a) If the matrix is strictly diagonally dominant, then |ai,i| > ∑ i̸=j |ai,j| for j = 1, . . . , n, which implies 1 > ∑ i̸=j |ai,j|/|ai,i|. The elements of the Jacobi iteration matrix are Ti,i = 0 and Ti,j = −ai,j/ai,i for j ̸= i. Thus ∥T ∥∞ = max i ∑ j̸=i |ai,j| |ai,i| < 1. Note that by strict diagonal dominance we are guaranteed that ai,i ̸= 0. (b) A symmetric positive deﬁnite 2 × 2 matrix given by A = (a b b c ) satisﬁes by its deﬁnition a, c > 0 and ac > b2. The Jacobi iteration matrix associated with A is T = ( 0 −b/a −b/c 0 ) , and its eigenvalues are thus the roots of λ2 − b 2 ac . By the above inequalities these eigenvalues must be smaller than 1 in magnitude. 1 Question 7.9 We in fact provide much of the solution to this question on pagess 239-240 of the textbook, and see also Piazza thread @269. (a) M = α−1I and T = I − αA. (b) i. We want ρ(T ) < 1. The eigenvalues of T are 1 − αλi, i = 1, . . . , n. Thus, we require −1 < 1 − αλi < 1. This condition simpliﬁes to 0 < α < 2 λi , so we must require that 0 < α < 2 maxi λi . ii. This question is in fact misplaced - it was intended to be rephrased and combined with part iii; so please don’t worry about it and proceed to part iii. iii. To minimize the spectral radius, we require that the two extremal eigenvalues of T be equal in magnitude: 1−αλ1 = −(1−αλn). It follows that α is as speciﬁed in the question, and the spectral radius is ρ(T ) = 1 − αλn = 1 − 2λn λ1 + λn = λ1 − λn λ1 + λn = κ(A) − 1 κ(A) + 1 . (c) This statement is false. From part (b) we know that α must be smaller than 2 maxi λi . The choice α = 1 may fall outside this range, for example if maxi λi > 2. Properties such as diagonal dominance or positive deﬁniteness cannot prevent this from potentially happening. Question 7.10 You are not required to know how to discretize diﬀerential equations, but this question is focused on linear algebra and gives you all the necessary information, so it is useful to know how to answer it. (a) The matrix is symmetric positive deﬁnite for all ω based on given eigenvalues, and hence the condition number is given by the ratio of the maximal eigenvalue (attained for l = m = N ) and the minimal eigenvalue (attained for l = m = 1): κ(A) = 4 − 4 cos(N πh) + ω2h 2 4 − 4 cos(πh) + ω2h2 ≈ 8 + ω2h 2 (2π2 + ω2)h2 . The approximation on the right-hand side is valid for N large. If ω is ﬁxed then as N grows, κ(A) = O(N 2). (b) All eigenvalues of A(0) are positive because the cosine values are strictly between −1 and 1 (never equal to −1 or 1 because 1 ≤ l, m ≤ N and h = 1 N +1 ). For any ω ̸= 0 the eigenvalues of A(ω) are equal to those of A(0) plus ω2h 2, and therefore are positive too. (c) From the expression given in part (a) we can see that A(ω) would be better conditioned. Please show this algebraically. (d) The iteration matrix T is given by I − (4 + ω2h 2) −1A, and hence its eigenvalues are µl,m = 1 − 4 4 + ω2h2 + 2(cos(lπh) + cos(mπh)) 4 + ω2h2 , 1 ≤ l, m ≤ N. Thus, the spectral radius of the Jacobi iteration matrix is given by ρ(T ) = 1 − 4 4 + ω2h2 + 2(cos(πh) + cos(πh)) 4 + ω2h2 ≈ 1 − 2(πh) 2 4 + ω2h2 . Note that for ω small and N large the spectral radius is less than 1 only by O(n = N 2), which makes the Jacobi iteration very slow. 2 Question 7.11 (a) A = M − N where M = ω−1D. (b) We have T = I − M −1A = I − ωD−1A = I − ω 4 A. Therefore, the eigenvalues of the iteration matrix are µl,m = 1 − ω 4 λl,m = 1 − ω + ω 2 (cos(lπh) + cos(mπh)), 1 ≤ l, m ≤ N. Convergence clearly holds for 0 < ω ≤ 1 (hence the term “damped”), but not for ω > 1: there is no “SOR-Jacobi” method. The performance, in terms of the usual iteration error decrease, is not better for ω < 1 than for ω = 1, as the spectral radius max µl,m is not smaller. Question 7.12 We have ρ(M −1N ) ≤ ∥M −1N ∥2 ≤ ∥M −1∥∥N ∥ = ρ(N ) λmin(M ) . Hence, if λmin(M ) > ρ(N ) then ρ(M −1N ) < 1, and the stationary iteration is guaranteed to converge. Question 7.15 (a) Approximately − 6 log10(999/1001) ≈ 6907 iterations. (b) Doing the same but with square roots we get approximately 218 iterations. (c) CG is signiﬁcantly faster, by an order of magnitude. Question 7.16 (a) Since xk+1 = xk + αkrk, we have rk+1 = rk − αkArk. Taking an inner product on the left with rk and substituting αk = r T k rk rT k Ark gives us the desired result. (In the question the index is shifted by 1; this does not matter.) (b) A bit harder - skipped. Most of the remaining questions in this chapter are signiﬁcantly more advanced. 3","libVersion":"0.2.1","langs":""}