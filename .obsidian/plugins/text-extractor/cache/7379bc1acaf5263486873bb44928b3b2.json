{"path":".obsidian/plugins/text-extractor/cache/7379bc1acaf5263486873bb44928b3b2.json","text":"Deriving the Gradient and Hessian of Linear and Quadratic Functions in Matrix Notation Mark Schmidt February 6, 2019 1 Gradient of Linear Function Consider a linear function of the form f (w) = a T w, where a and w are length-d vectors. We can derive the gradeint in matrix notation as follows: 1. Convert to summation notation: f (w) = d∑ j=1 ajwj, where aj is element j of a and wj is element j of w. 2. Take the partial derivative with respect to a generic element k: ∂ ∂wk   d∑ j=1 ajwj   = ak. 3. Assemble the partial derivatives into a vector: ∇f (w) =      ∂ ∂w1 ∂ ∂w2 ... ∂ ∂wd      =      a1 a2 ... ad      4. Convert to matrix notation: ∇f (w) =      a1 a2 ... ad      = a. So our ﬁnal results is that ∇f (w) = a. This generalizes the scalar case where d dw [αw] = α. We can also consider general linear functions of the form f (w) = a T w + β, for a scalar β. But in this case we still have ∇f (w) = a since the y-intercept β does not depend on w. 1 2 Gradient of Quadratic Function Consider a quadratic function of the form f (w) = wT Aw, where w is a length-d vector and A is a d by d matrix. We can derive the gradeint in matrix notation as follows 1. Convert to summation notation: f (w) = wT      ∑n j=1 a1jwj∑n j=1 a2jwj ...∑n j=1 adjwj      | {z } Aw = d∑ i=1 d∑ j=1 wiaijwj. where aij is the element in row i and column j of A. To help with computing the partial derivatives, it helps to re-write it in the form f (w) = d∑ i=1 d∑ j=1 wiaijwj = d∑ i=1(aiiw2 i + ∑ j̸=i wiaijwj). 2. Take the partial derivative with respect to a generic element k: ∂ ∂wk   d∑ i=1(aiiw2 i + ∑ j̸=i wiaijwj).   = 2akkwk + ∑ j̸=k wjajk + ∑ j̸=k akjwj. The ﬁrst term comes from the akk term that is quadratic in wk, while the two sums come from the terms that are linear in wk. We can move one akkwk into each of the sums to simplify this to ∂ ∂wk   d∑ i=1(aiiw2 i + ∑ j̸=i wiaijwj).   = d∑ j=1 wjajk + d∑ j=1 akjwj. 3. Assemble the partial derivatives into a vector: ∇f (w) =      ∂ ∂w1 ∂ ∂w2 ... ∂ ∂wd      =       ∑d j=1 wjaj1 + ∑d j=1 a1jwj ∑d j=1 wjaj2 + ∑d j=1 a2jwj ... ∑d j=1 wjajd + ∑d j=1 adjwj       =       ∑d j=1 wjaj1 ∑d j=1 wjaj2 ... ∑d j=1 wjajd       +       ∑d j=1 a1jwj ∑d j=1 a2jwj ... ∑d j=1 adjwj       4. Convert to matrix notation: ∇f (w) =       ∑d j=1 wjaj1 ∑d j=1 wjaj2 ... ∑d j=1 wjajd       +       ∑d j=1 a1jwj ∑d j=1 a2jwj ... ∑d j=1 adjwj       = AT w + Aw = (AT + A)w. 2 So our ﬁnal result is that ∇f (w) = (AT + A)w. Note that if A is symmetric (AT = A) then we have (AT + A) = (A + A) = 2A so we have ∇f (w) = 2Aw. This generalizes the scalar case where d dw [αw2] = 2αw. We can also consider general quadratic functions of the form f (w) = 1 2 wT Aw + bT w + γ. Using the above results we have ∇f (w) = 1 2 (AT + A)w + b, and if A is symmetric then ∇f (w) = Aw + b. 3 Hessian of Linear Function For a linear function of the form, f (w) = a T w, we show above the partial derivatives are given by ∂f ∂wk = ak. Since these ﬁrst partial derivatives don’t depend on any wk, the second partial derivatives are thus given by ∂2f ∂wk∂wk′ = 0, which means that the Hessian matrix is the zero matrix, ∇2f (w) =      ∂ ∂w1∂w1 f (w) ∂ ∂w1∂w2 f (w) · · · ∂ ∂w1∂wd f (w) ∂ ∂w2∂w1 f (w) ∂ ∂w2∂w2 f (w) · · · ∂ ∂w2∂wd f (w) ... ... . . . ... ∂ ∂wd∂w1 f (w) ∂ ∂wd∂w2 f (w) · · · ∂ ∂wd∂wd f (w)      =      0 0 · · · 0 0 0 · · · 0 ... ... . . . ... 0 0 · · · 0      , and using 0 to denote the zero matrix we have ∇2f (w) = 0. 4 Hessian of Quadratic Function For a quadratic function of the form, f (w) = wT Aw, we show above the partial derivatives are given by linear functions, ∂f ∂wk = d∑ j=1 wjajk + d∑ j=1 akjwj. 3 The second partial derivatives are thus constant functions of the form ∂2f ∂wk∂wk′ = ak′k + akk′, which means that the Hessian matrix has a simple form ∇2f (w) =      ∂ ∂w1∂w1 f (w) ∂ ∂w1∂w2 f (w) · · · ∂ ∂w1∂wd f (w) ∂ ∂w2∂w1 f (w) ∂ ∂w2∂w2 f (w) · · · ∂ ∂w2∂wd f (w) ... ... . . . ... ∂ ∂wd∂w1 f (w) ∂ ∂wd∂w2 f (w) · · · ∂ ∂wd∂wd f (w)      =      a11 + a11 a21 + a12 · · · ad1 + a1d a12 + a21 a22 + a22 · · · ad2 + a2d ... ... . . . ... a1d + ad1 a2d + ad2 · · · add + add      . This gives a result of ∇2f (w) = A + AT , and if A is symmetric this simpliﬁes to ∇2f (w) = 2A. 5 Example of Least Squares The least squares objective function has the form f (w) = 1 2 ∥Xw − y∥2, which can be written as f (w) = 1 2 wT X T X| {z } A w − wT X T y | {z } b + 1 2 yT y | {z } γ . By using that X T X and symmetric and using the results above we have that ∇f (w) = X T Xw − X T y, and that ∇2f (w) = X T X. 4","libVersion":"0.2.1","langs":""}