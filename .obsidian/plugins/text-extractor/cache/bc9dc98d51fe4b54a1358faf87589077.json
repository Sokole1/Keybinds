{"path":".obsidian/plugins/text-extractor/cache/bc9dc98d51fe4b54a1358faf87589077.json","text":"CPSC 340: Machine Learning and Data Mining More Clustering Fall 2019 Admin â€¢ A2: one week left â€¢ crush it early, crush it often â€¢ New class coming soon (maybe as early as Fall 2023) â€¢ DSCI 430 (3) Fairness, Accuracy, Transparency and Ethics (FATE) in Data Science â€¢ Title may change a bit Last Time: K-Means Clustering â€¢ We want to cluster data: â€“ Assign examples to groups. â€¢ K-means clustering: â€“ Define groups by â€œmeansâ€ â€“ Assigns examples to nearest mean. (And updates means during training.) â€¢ Issues with k-means: â€“ Fast but sensitive to initialization. â€“ Choosing â€˜kâ€™ is annoying. Vector Quantization â€¢ K-means originally comes from signal processing. â€¢ Designed for vector quantization: â€“ Replace examples with the mean of their cluster (â€œprototypeâ€). â€¢ Example: â€“ Facebook places: 1 location summarizes many. â€“ What sizes of clothing should I make? http://wannabite.com/wp-content/uploads/2014/10/ragu-pasta-sauce-printable-coupon.jpg Vector Quantization for Basketball Players â€¢ Clustering NBA basketball players based on shot type/percentage: â€¢ The â€œprototypesâ€ (means) give offensive styles (like â€œcatch and shootâ€). https://fansided.com/2018/08/23/nylon-calculus-shooting-volume-versus-efficiency/ Vector Quantization Example(Bad) Vector Quantization in Practice â€¢ Political parties can be thought as a form of vector quantization: â€“ Hope is that parties represent what a cluster of voters want. â€¢ With larger â€˜kâ€™ more voters have a party that closely reflects them. â€¢ With smaller â€˜kâ€™, parties are less accurate reflections of peopleâ€™s views. https://globalnews.ca/news/5191123/federal-election-seat-projection-trudeau-liberals-minority/ Shape of K-Means Clusters â€¢ Recall that k-means assigns cluster based on nearest mean. â€¢ This leads to partitions the space : â€¢ Observe that the clusters are convex regions (proof in bonus). Convex Sets â€¢ A set is convex if line between two points in the set stays in the set. Convex Convex Not Convex Shape of K-Means Clusters Animation K-Means with Non-Convex Clusters https://corelifesciences.com/human-long-non-coding-rna-expression-microarray-service.html K-Means with Non-Convex Clusters https://corelifesciences.com/human-long-non-coding-rna-expression-microarray-service.html K-means cannot separate some non-convex clusters K-Means with Non-Convex Clusters https://corelifesciences.com/human-long-non-coding-rna-expression-microarray-service.html K-means cannot separate some non-convex clusters Though over-clustering can help (â€œhierarchicalâ€) John Snow and Cholera Epidemic â€¢ John Snowâ€™s 1854 spatial histogram of deaths from cholera: â€¢ Found cluster of cholera deaths around a particular water pump. â€“ Went against airborne theory, but pump later found to be contaminated. â€“ â€œFatherâ€ of epidemiology. https://en.wikipedia.org/wiki/John_Snow Motivation for Density-Based Clustering â€¢ Density-based clustering: â€“ Clusters are defined by â€œdenseâ€ regions. â€“ Examples in non-dense regions donâ€™t get clustered. â€¢ Not trying to â€œpartitionâ€ the space. â€¢ Clusters can be non-convex: â€“ Elephant clusters affected by vegetation, mountains, rivers, water access, etc. â€¢ Itâ€™s a non-parametric clustering method: â€“ No fixed number of clusters â€˜kâ€™. â€“ Clusters can become more complicated with more data. http://www.defenders.org/elephant/basic-facts Other Potential Applications â€¢ Where are high crime regions of a city? â€¢ Where should taxis patrol? â€¢ Where does Iguodala make/miss shots? â€¢ Which products are similar to this one? â€¢ Which pictures are in the same place? â€¢ Where can proteins â€˜dockâ€™? â€¢ Where are people tweeting? https://en.wikipedia.org/wiki/Cluster_analysis https://www.flickr.com/photos/dbarefoot/420194128/ http://letsgowarriors.com/replacing-jarrett-jack/2013/10/04/ http://www.dbs.informatik.uni-muenchen.de/Forschung/KDD/Clustering/ Density-Based Clustering in Action Interactive demo Density-Based Clustering â€¢ Density-based clustering algorithm (DBSCAN) has two hyperparameters: â€“ Epsilon (Îµ): distance we use to decide if another point is a â€œneighbourâ€. Density-Based Clustering â€¢ Density-based clustering algorithm (DBSCAN) has two hyperparameters: â€“ Epsilon (Îµ): distance we use to decide if another point is a â€œneighbourâ€. Density-Based Clustering â€¢ Density-based clustering algorithm (DBSCAN) has two hyperparameters: â€“ Epsilon (Îµ): distance we use to decide if another point is a â€œneighbourâ€. â€“ MinNeighbours: number of neighbours needed to say a region is â€œdenseâ€. â€¢ If you have at least minNeighbours â€œneighboursâ€, you are called a â€œcoreâ€ point. â€¢ Main idea: merge all neighbouring core points to form clusters. Density-Based ClusteringDensity-Based ClusteringDensity-Based ClusteringDensity-Based Clustering â€¢ Intuitively, density-based clustering algorithm implements a â€œchain reactionâ€ throughout the dense areas. â€¢ For each example x i : â€“ If x i is already assigned to a cluster, do nothing. â€“ Test whether x i is a â€˜coreâ€™ point (â‰¥ minNeighbours examples within â€˜Îµâ€™). â€¢ If x i is not core point, do nothing (this could be an outlier). â€¢ If x i is a core point, make a new cluster and call the â€œexpand clusterâ€ function. â€“ Which spreads the â€œreactionâ€ to nearby points. Density-Based Clustering Pseudo-CodeDensity-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Pseudo-Code â€¢ â€œExpand clusterâ€ function: â€“ Assign to this cluster all x j within distance â€˜Îµâ€™ of core point x i to this cluster. â€“ For each new â€œcoreâ€ point found, call â€œexpand clusterâ€ (recursively). Density-Based Clustering Issues â€¢ Some points are not assigned to a cluster. â€“ Good or bad, depending on the application. â€¢ Ambiguity of â€œnon-coreâ€ (boundary) points: â€¢ Sensitive to the choice of Îµ and minNeighbours. â€“ Original paper proposed an â€œelbowâ€ method (see bonus slide). â€“ Otherwise, not sensitive to initialization (except for boundary points). â€¢ If you get a new example, finding cluster is expensive. â€“ Need to compute distances to core points (or maybe all training points). â€¢ In high-dimensions, need a lot of points to â€˜fillâ€™ the space. Density-Based Clustering in Bacteria â€¢ Quorum sensing: â€“ Bacteria continuously release a particular molecule. â€“ They have sensors for this molecule. â€¢ If sensors become very active: â€“ It means cell density is high. â€“ Causes cascade of changes in cells. (Some cells â€œstick togetherâ€ to form a physical cluster via â€œbiofilmâ€.) https://en.wikipedia.org/wiki/Quorum_sensing Density-Based Clustering in People â€¢ â€œHigh density leading a chain reactionâ€ can also happen in people. â€“ â€œSocial distancingâ€: try to reduce the number of people within . â€“ â€œWearing masksâ€: try to increase the needed for a chain reaction. ğœ– ğœ– Next Topic: Ensemble Clustering Ensemble Clustering â€¢ We can consider ensemble methods for clustering. â€“ â€œConsensus clusteringâ€ â€¢ Itâ€™s a good/important idea: â€“ Bootstrapping is widely-used. â€“ â€œDo clusters change if the data was slightly different?â€ â€¢ But we need to be careful about how we combine models. Ensemble Clustering â€¢ E.g., run k-means 20 times and then cluster using the mode of each i . â€¢ Normally, averaging across models doing different things is good. â€¢ But this is a bad ensemble method: worse than k-means on its own. ^ ğ‘¦ Label Switching Problem â€¢ This doesnâ€™t work because of â€œlabel switchingâ€ problem: â€“ The cluster labels i are meaningless. â€“ We could get same clustering with permuted labels (â€œexchangeableâ€): â€“ All i become equally likely as number of initializations increases. ^ ğ‘¦ ^ ğ‘¦ Addressing Label Switching Problem â€¢ Ensembles canâ€™t depend on label â€œmeaningâ€: â€“ Donâ€™t ask â€œis point x i in red square cluster?â€, which is meaningless. â€“ Ask â€œis point x i in the same cluster as x j ?â€, which is meaningful. â€“ Bonus slides give an example method (â€œUBClusteringâ€). Next Topic: Hierarchical Clustering Differing Densities â€¢ Consider density-based clustering on this data: Differing Densities â€¢ Increase epsilon and run it again: â€¢ There may be no density-level that gives you 3 clusters. Differing Densities â€¢ Here is a worse situation: â€¢ Now you need to choose between coarse/fine clusters. â€¢ Instead of fixed clustering, we often want hierarchical clustering. Hierarchical Clustering â€¢ Hierarchical clustering produces a tree of clusterings. â€“ Each node in the tree splits the data into 2 or more clusters. â€“ Much more information than using a fixed clustering. â€“ Often have individual data points as leaves. GIF Application: Phylogenetics â€¢ We sequence genomes of a set of organisms. â€¢ Can we construct the â€œtree of lifeâ€? â€¢ Comments on this application: â€“ On the right are individuals. â€“ As you go left, clusters merge. â€“ Merges are â€˜common ancestorsâ€™. â€¢ More useful information in the plot: â€“ Line lengths: chosen here to approximate time. â€“ Numbers: #clusterings across bootstrap samples. â€“ â€˜Outgroupsâ€™ (walrus, panda) are a sanity check. http://www.nature.com/nature/journal/v438/n7069/fig_tab/nature04338_F10.html Application: Phylogenetics â€¢ Interactive demo of model of full tree of life: www.onezoom.org Application: Phylogenetics â€¢ Model of Covid-19 variants: https://erictopol.substack.com/p/the-ba5-story Application: Phylogenetics â€¢ Comparative method in linguistics studies evolution of languages: https://en.wikipedia.org/wiki/Comparative_method_(linguistics) Application: Phylogenetics â€¢ January 2016: evolution of fairy tales. â€“ Evidence that â€œDevil and the Smithâ€ goes back to bronze age. â€“ â€œBeauty and the Beastâ€ published in 1740, but might be 2500-6000 years old. http://rsos.royalsocietypublishing.org/content/3/1/150645 Application: Phylogenetics â€¢ January 2016: evolution of fairy tales. â€“ Evidence that â€œDevil and the Smithâ€ goes back to bronze age. â€“ â€œBeauty and the Beastâ€ published in 1740, but might be 2500-6000 years old. â€¢ September 2016: evolution of myths. â€“ â€œCosmic huntâ€ story: â€¢ Person hunts animal that becomes constellation. â€“ Previously known to be at least 15,000 years old. â€¢ May go back to paleololithic period. http://www.nature.com/nature/journal/v438/n7069/fig_tab/nature04338_F10.html Application: Fashion? â€¢ Hierarchical clustering of clothing material words in Vogue: http://dh.library.yale.edu/projects/vogue/fabricspace/ Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. https://en.wikipedia.org/wiki/Hierarchical_clustering Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. 2. Each step merges the two â€œclosestâ€ clusters. https://en.wikipedia.org/wiki/Hierarchical_clustering Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. 2. Each step merges the two â€œclosestâ€ clusters. https://en.wikipedia.org/wiki/Hierarchical_clustering Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. 2. Each step merges the two â€œclosestâ€ clusters. https://en.wikipedia.org/wiki/Hierarchical_clustering Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. 2. Each step merges the two â€œclosestâ€ clusters. https://en.wikipedia.org/wiki/Hierarchical_clustering Agglomerative (Bottom-Up) Clustering â€¢ Most common hierarchical method: agglomerative clustering. 1. Starts with each point in its own cluster. 2. Each step merges the two â€œclosestâ€ clusters. 3. Stop with one big cluster that has all points. https://en.wikipedia.org/wiki/Hierarchical_clustering Animation Agglomerative (Bottom-Up) Clustering â€¢ Reinvented by different fields under different names (â€œUPGMAâ€). â€¢ Needs a â€œdistanceâ€ between two clusters. â€¢ A standard choice: distance between means of the clusters. â€“ Not necessarily the best, many choices exist (bonus slide). â€¢ Cost is O(n 3 d) for basic implementation. â€“ Each step costs O(n 2 d), and each step might only cluster 1 new point. Summary â€¢ Vector quantization: â€“ Compressing examples by replacing them with the mean of their cluster. â€¢ Shape of K-means clusters: â€“ Partitions space into convex sets. â€¢ Density-based clustering: â€“ â€œExpandâ€ and â€œmergeâ€ dense regions of points to find clusters. â€“ Not sensitive to initialization or outliers. â€“ Useful for finding non-convex connected clusters. â€¢ Ensemble clustering: combines multiple clusterings. â€“ Can work well but need to account for label switching. â€¢ Hierarchical clustering: more informative than fixed clustering. â€¢ Agglomerative clustering: standard hierarchical clustering method. â€“ Each point starts as a cluster, sequentially merge clusters. â€¢ Next time: â€¢ Discovering (and then ignoring) a hole in the ozone layer. Why are k-means clusters convex? â€¢ K-means clusters are formed by the intersection of half-spaces. Half-space Why are k-means clusters convex? â€¢ K-means clusters are formed by the intersection of half-spaces. Half-space Intersection Half-space Why are k-means clusters convex?Why are k-means clusters convex? â€œCloser to redâ€ half-space â€œCloser to greenâ€ half-space Why are k-means clusters convex? â€œCloser to redâ€ half-space â€œCloser to greenâ€ half-space Why are k-means clusters convex? Blue over green half-space Green over blue half-space Why are k-means clusters convex? Magenta over green half-space Green over magenta half-space Why are k-means clusters convex?Why are k-means clusters convex? â€¢ Half-spaces are convex sets. â€¢ Intersection of convex sets is a convex set. â€“ Line segment between points in each set are still in each set. â€¢ So intersection of half-spaces is convex. Half-space Intersection Half-space Voronoi Diagrams â€¢ The k-means partition can be visualized as a Voronoi diagram: â€¢ Can be a useful visualization of â€œnearest availableâ€ problems. â€“ E.g., nearest tube station in London. http://datagenetics.com/blog/may12017/index.html Density-Based Clustering Runtimeâ€œElbowâ€ Method for Density-Based Clustering â€¢ From the original DBSCAN paper: â€“ Choose some â€˜kâ€™ (they suggest 4) and set minNeighbours=k. â€“ Compute distance of each points to its â€˜kâ€™ nearest neighbours. â€“ Sort the points based on these distances and plot the distances: â€“ Look for an â€œelbowâ€ to choose . ğœ– https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf OPTICS â€¢ Related to the DBSCAN â€œelbowâ€ is â€œOPTICSâ€. â€“ Sort the points so that neighbours are close to each other in the ordering. â€“ Plot the distance from each point to the next point. â€“ Clusters should correspond to sequencers with low distance. https://en.wikipedia.org/wiki/OPTICS_algorithm UBClustering Algorithm â€¢ Letâ€™s define a new ensemble clustering method: UBClustering. 1. Run k-means with â€˜mâ€™ different random initializations. 2. For each example i and j: â€“ Count the number of times x i and x j are in the same cluster. â€“ Define p(i,j) = count(x i in same cluster as x j )/m. 3. Put x i and x j in the same cluster if p(i,j) > 0.5. â€¢ Like DBSCAN merge clusters in step 3 if i or j are already assigned. â€“ You can implement this with a DBSCAN code (just changes â€œdistanceâ€). â€“ Each x i has an x j in its cluster with p(i,j) > 0.5. â€“ Some points are not assigned to any cluster. UBClustering AlgorithmDistances between Clusters â€¢ Other choices of the distance between two clusters: â€“ â€œSingle-linkâ€: minimum distance between points in clusters. â€“ â€œAverage-linkâ€: average distance between points in clusters. â€“ â€œComplete-linkâ€: maximum distance between points in clusters. â€“ Wardâ€™s method: minimize within-cluster variance. â€“ â€œCentroid-linkâ€: distance between a representative point in the cluster. â€¢ Useful for distance measures on non-Euclidean spaces (like Jaccard similarity). â€¢ â€œCentroidâ€ often defined as point in cluster minimizing average distance to other points. Cost of Agglomerative Clustering â€¢ One step of agglomerative clustering costs O(n 2 d): â€“ We need to do the O(d) distance calculation between up to O(n 2 ) points. â€“ This is assuming the standard distance functions. â€¢ We do at most O(n) steps: â€“ Starting with â€˜nâ€™ clusters and merging 2 clusters on each step, after O(n) steps weâ€™ll only have 1 cluster left (though typically it will be much smaller). â€¢ This gives a total cost of O(n 3 d). â€¢ This can be reduced to O(n 2 d log n) with a priority queue: â€“ Store distances in a sorted order, only update the distances that change. â€¢ For single- and complete-linkage, you can get it down to O(n 2 d). â€“ â€œSLINKâ€ and â€œCLINKâ€ algorithms. Bonus Slide: Divisive (Top-Down) Clustering â€¢ Start with all examples in one cluster, then start dividing. â€¢ E.g., run k-means on a cluster, then run again on resulting clusters. â€“ A clustering analogue of decision tree learning.","libVersion":"0.2.1","langs":""}