{"path":".obsidian/plugins/text-extractor/cache/1ddbbef24b19f5aea7f44535e443ef25.json","text":"CPSC 320: Asymptotic Runtime Analysis Questions that you’ll face throughout this course are: How can I ﬁgure out the runtime of an algorithm? Is one runtime faster than another? In this worksheet you’ll review some basics of algorithm runtime analysis, and touch also on algorithm correctness. We’ll usually focus on worst-case runtime, as a function of the input size, say n. Asymptotic analysis helps us understand the runtime for large n. 1 Asymptotic notation This table summarizes the asymptotic notation we’ll use to compare algorithm runtimes. Here we use statements such as “f = O(g)”, which is often also written as “f ∈ O(g)”. Statement Formal Deﬁnition Intuition f = O(g) ∃c > 0, ∃n0 ∈ N such that f (n) ≤ cg(n) for all n ≥ n0 f ≤ g f = o(g) ∀c > 0, ∃n0 ∈ N such that f (n) < cg(n) for all n ≥ n0 f < g or limn→∞ f (n)/g(n) = 0 f = Ω(g) ∃c > 0, ∃n0 ∈ N such that f (n) ≥ cg(n) for all n ≥ n0 g ≤ f g = O(f ) f = ω(g) g = o(f ) f > g f = Θ(g) f = O(g) and g = O(f ) f = g 1 2 Comparing function growth part 1 Arrange these functions by increasing order of growth, and give a good Θ bound for each. Notation that we use here and throughout the course: lg n = log2 n, ln n = loge n, and log n = log10 n. n + n2 2n 55n + 4 1.5n lg n n! ln n 2n log(n2) n log n (n lg n)(n + 1) (n + 1)! 1.62n tricky, but doable! Notation Review and Handy Math: • Logs: lg n = log2 n, ln n = loge n, and log n = log10 n. Also, we use both logc n and (log n)c to denote log n to the power c. • Handy identities: logb a = logc a logc b for b, c > 1 cacb = ca+b a = clogc a ca cb = ca−b (ca)b = cab 2 (Repeated from last page) Arrange the functions below by increasing order of growth, and give a good Θ bound for each. n + n2 2n 55n + 4 1.5n lg n n! ln n 2n log(n2) n log n (n lg n)(n + 1) (n + 1)! 1.62n tricky, but doable! 3 3 Comparing function growth part 2 Here are good rules of thumb when comparing how functions grow: • Logarithmic functions of n grow much more slowly than functions that are n to some constant power: For every b > 1 and every x > 0, we have logb n = o(nx). • Functions that are powers of n grow much more slowly than functions with n in the exponent. A weak form of this rule of thumb is stated in the textbook (2.9, page 42): For every c > 1 and every x > 0, we have nx = o(cn). 1. Show that for every b > 1 and every x > 0, logb n = o(nx). You can use the following weaker fact, stated in the textbook (2.8, page 41): For every b > 1 and every x > 0, we have logb n = O(nx). 2. Show that √n = o(n/ log3 n). 4 4 Oscillating functions: These types of functions don’t usually arise in algorithms analysis, but studying them can sharpen your understanding of big-O notation. 1. Let f (n) = { n, if n is even, 1, if n is odd and let g(n) = n. (a) Is f = O(g)? (b) Is f = Ω(g)? (c) Is f = Θ(g)? 5 5 Polynomial time 1. Sometimes you might see the statement “f (n) = nO(1)”. Write down a deﬁnition of what you think that means. Deﬁnition of f (n) = nO(1): Which of the following functions f (n) satisﬁes f (n) = nO(1), according to your deﬁnition? • f (n) = n3. • f (n) = n4 log n. • f (n) = 2n. • f (n) = n2 + 3n + 5. 6 6 Worst-case algorithm runtime Find a good Θ bound on the worst-case running time of each of the following algorithm. Justify your bound. function CountInversions(A[1..n]) ▷ A[1..n] is an array of numbers inversions ← 0 for i from 1 to n do for j from i + 1 to n do if a[i] > a[j] then inversions ← inversions + 1 return inversions 7 7 Challenge Problem 1. Give the best Θ bound you can ﬁnd for √n√n and then arrange it with respect to the other functions from Section 2. 8","libVersion":"0.2.1","langs":""}