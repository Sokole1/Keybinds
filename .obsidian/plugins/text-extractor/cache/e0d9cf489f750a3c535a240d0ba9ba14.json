{"path":".obsidian/plugins/text-extractor/cache/e0d9cf489f750a3c535a240d0ba9ba14.json","text":"CPSC 320: Clustering Solutions ∗ You’re working on software to organize people’s photos. Your algorithm receives as input: • A bunch of uncategorized photos. • A similarity measure for each pair of photos, where a 0 similarity indicates two photos are nothing like each other; a 1 indicates two photos are exactly the same. All other similarities are in between. • The number of categories to group them into. Your algorithm should create a categorization: the requested number of categories, where a category is a non-empty set of photos. Every photo belongs to some category, and no photo belongs to more than one category. So, a categorization is a partition. We’d like similar photos to be in the same category. Step 1: Build intuition through examples. 1. Write down small and trivial instances of the problem. What data structure is useful to represent a problem instance? Write down also potential solutions for your instances. Are some solutions better than others? How so? SOLUTION: An empty graph (with zero categories) is clearly trivial, though it’s not clear this should be counted as an instance. Any instance with one category is also trivial because every photo must go in that category. At the other end of the spectrum, if the number of photos equals the number of categories, then each photo must go in its own category, which is trivial. You might also find instances with all similarities equal or other interesting cases to be trivial. A small nontrivial instance could have three photos and two categories. We’ll use a weighted graph representation, where nodes correspond to photos and edge weights represent similarities. We also want the graph to be complete—to contain every possible edge (except self-loops). Here is an example: # categories = 2 For this small instance, a solution that groups 1 and 3 together, and leaves 2 in its own category, seems better than the alternatives. ∗Copyright Notice: UBC retains the rights to this document. You may not distribute this document without permission. 1 Step 2: Develop a formal problem specification 1. Develop notation for describing a problem instance. SOLUTION: We’ll let n be the number of photos. It’s reasonable to assume that n ≥ 1. We can think of an instance as a graph G = (V, E) where each node in V is a photo. For simplicity, let’s assume that the photos are numbered, so V is just the set {1, 2, . . . , n}. For every pair p, p′ ∈ V there is an undirected, weighted edge of the form (p, p′, s) in E with 0 ≤ s ≤ 1. The edge similarities could be represented as a 2D matrix, say S, where S[p][p′] is the similarity s of the pair (p, p′) of photos. An instance can then be n, the set E, plus a number, say c, of categories, where 1 ≤ c ≤ n. For example, the small instance above has n = 3 and c = 2, and is represented as: (3, {(1, 2, 0.5), (1, 3, 0.8), (2, 3, 0.2)}, 2). 2. Use your notation to flesh out the following group of photos into an instance. SOLUTION: There are many reasonable solutions. Here’s one. We simply chose plausible similari- ties; the correspondence between images and numbers is not too important. We’ll choose c = 2. c = 2 It’s hard to eyeball a solution to this with no metric to guide us, but it does seem that images 1 and 3 should be together given their strong similarity (0.9). We might then put images 2 and 4 together. 2 3. Develop notation for describing a potential solution. Describe what you think makes a solution good. Can you come up with a reasonable criterion for deciding if one solution is better than another? SOLUTION: A potential solution (or just “solution”) will be a set of categories {C1, C2, . . . , Cc}, where a category Ci is a set of nodes from V . This set should be a partition. That is, every photo must belong to one and only one category, and every category Ci is non-empty. It’s much harder to say what a good solution is. Clearly we want to reward having nodes with high similarity in the same category and penalize having nodes with low similarity in the same category. If we divide edges into intra-category (between nodes in the same category) and inter- category (between nodes in different categories), then we might choose a metric like \"sum of the intra-category similarities minus sum of the inter-category similarities\". However, this will push us toward large categories. (Since the number of intra-category edges in a category C scales as O(|C|2), there are more intra-category edges in a categorization with one big category and many small ones than with all even-sized categories (and the same total number of categories).) We could instead look for a similarity measure based on the average similarity of each category. We have to decide then what to do with categories of a single node, since their average is undefined. Rate them zero? Note that there’s no “right” choice. We just have to decide what does a good job modeling what we care about and how efficient a solution we get. There’s something fundamental about the fact that we will soon pick a metric that’s totally reasonable. . . and which we really chose because it is “good enough” and admits a highly efficient solution. How much of our lives are now ruled by metrics that are easy to compute rather than “best”? 3","libVersion":"0.2.1","langs":""}