{"path":".obsidian/plugins/text-extractor/cache/0f25c27ddb0e3bbad42f6a6c679bf7a9.json","text":"1 Matrix Notation and Minimizing Quadratics [12 points] 1.1 Converting to Matrix/Vector/Norm Notation [6 points] Using our standard supervised learning notation (X, y, w) express the following functions in terms of vectors, matrices, and norms (there should be no summations or maximuns). 1. maxie (1, ny [wTw; — yil. This is “brittle regression”. 2. 57 viw”a; — )2 + 3 S0, w?. This is regularized least squares with a weight v; for each training example: Hint: You can use V to denote a diagonal matrix that has the values v; along the diagonal. What does a” Vb look like in summation form (for some arbitrary vectors a, b)? 3. (2, lwha; — y.|)2+% Z]d:, Aj|w;|. This is L1-regularized least squares with a different regularization strength for each dimension: Hint: You can use A to denote a diagonal matrix that has the A; values along the diagonal. Note: you can assume that all the v; and ); values are non-negative.","libVersion":"0.2.1","langs":"eng"}