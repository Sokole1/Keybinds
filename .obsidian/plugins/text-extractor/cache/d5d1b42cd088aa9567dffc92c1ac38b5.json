{"path":".obsidian/plugins/text-extractor/cache/d5d1b42cd088aa9567dffc92c1ac38b5.json","text":"CPSC 340: Machine Learning and Data Mining Deep Learning Admin · A5 deadline Friday (good luck!) - two bonus days · A6 out today, due Dec. 6th (last day of class) - if you use two bonus days for A5 (i.e. if you do not start A6 today) you will have two days less than you would otherwise - because we don’t want something due after the last day of class Admin · Course surveys – Please fill them out – We care deeply about your education, so we take them very seriously – You will be able to evaluate the class overall, and then Prof. Schmidt and I separately – As always, please remember we’re real people, so both praise and constructive criticism feedback are great. Please avoid personal, hurtful, or unconstructive negative comments. Tone matters! Last Time Supervised Learning Roadmap Re g r e s s i o n v s . Bi n a r y C l a s s i fi c a t i o n • Fo r re g re s s i o n pr o bl e m s , o ur pr e di c t i o n ( i g no r i ng bi a s e s ) i s : • A nd w e m i g h t t r a i n t o m i ni m i z e t he s qua r e d r e s i dua l : R e g r e s s i o n v s . B i n a r y C l a s s i f i c a ti o n • Fo r bi na r y c l a s s i f i c a t i o n pr o bl e m s , o ur pr e di c t i o n i s : • A nd w e m i g h t t r a i n t o m i ni m i z e t he lo gis t ic lo ss : – T h i s i s l i k e l o g i s t i c r e g r e s s i o n w i t h l e a r n e d f e a t u r e s . Neu r a l Ne t w o r k f o r M u l t i - Class Classif ic a t ion • Mu l t i - cl a s s cl a s s i f i c a t i o n a neur a l ne tw o r k : – I nput i s c o nne c t e d t o a hi dde n l a y e r ( s a m e a s r e g r e s s i o n a nd bi na r y c a s e ) . – H i dde n l a y e r i s c o nne c t e d t o m ul t i pl e o ut put uni t s ( o ne f o r e a c h l a be l . ) . – W e c a n pr e di c t b y m a x i m i z i ng o ic ov e r a l l ‘ c ’ . – W e c a n c o n v e r t t o pr o ba bi l i t i e s f o r e a c h c l a s s us i ng so f tm a x t o t he o ic v a l ue s : – W e t r a i n b y m i ni m i z i ng ne g a t i v e l o g o f t hi s pr o ba bi l i t y ( so f tm a x l o s s , s um m e d a c r o s s e x a m pl e s ) . – N o t i c e t ha t w e c ha ng e d t a s k s b y o nl y c ha ng i ng l a s t l a y e r ( a nd l o s s f unc t i o n) . Neu r a l Ne t w o r k f o r M u l t i - Class Classif ic a t ion • Mu l t i - cl a s s cl a s s i f i c a t i o n a neur a l ne tw o r k : – I nput i s c o nne c t e d t o a hi dde n l a y e r ( s a m e a s r e g r e s s i o n a nd bi na r y c a s e ) . – H i dde n l a y e r i s c o nne c t e d t o m ul t i pl e o ut put uni t s ( o ne f o r e a c h l a be l . ) . – W e c a n pr e di c t b y m a x i m i z i ng o ic ov e r a l l ‘ c ’ . – W e c a n c o n v e r t t o pr o ba bi l i t i e s f o r e a c h c l a s s us i ng so f tm a x t o t he o ic v a l ue s : – W e t r a i n b y m i ni m i z i ng ne g a t i v e l o g o f t hi s pr o ba bi l i t y ( so f tm a x l o s s , s um m e d a c r o s s e x a m pl e s ) . – N o t i c e t ha t w e c ha ng e d t a s k s b y o nl y c ha ng i ng l a s t l a y e r ( a nd l o s s f unc t i o n) . aka \"cross entropy\"(good intuitive explanation here) Adding Bias Variables •We typically add a bias to each layer: Deep Learning Neural Networks 2D Non-Linear Mapping 3D neuron1 (left, x) neuron2 (right, y) 1 1 neuron3 (center) x y Left Center Right Output 1 1 1 0 0 1 0 0 Outputs 1 if >= number in node Else: 0 1 Outputs of each node Neural Networks 2D Non-Linear Mapping 3D neuron1 (left, x) neuron2 (right, y) 1 1 neuron3 (center) x y Left Center Right Output 1 1 1 1 1 0 1 0 1 0 0 1 0 1 0 0 1 1 0 0 0 0 0 0 1 Outputs of each node Outputs 1 if >= number in node Else: 0 Neural Networks 3D neuron1 (left) neuron2 (right) 1 1 neuron3 (center) Neural Networks • Point of XOR was not that you need k>d • Instead • an example of how that can make things easier • and how a transformation can make things linearly separable • Cover’s theorem: “The probability that classes are linearly separable increases when the features are nonlinearly mapped to a higher dimensional feature space.” [Coover 1965] • The output layer requires linear separability. • The purpose of the hidden layers is to make the problem linearly separable! • Multi-layer networks thus allow “non-linear regression” From: http://130.236.96.13/edu/courses/TBMI26/pdfs/lectures/le5.pdf Neural Networks • Multi-layer networks thus allow “non-linear regression” Neural Networks • Multi-layer networks thus allow “non-linear regression” • Single hidden layer (often very large): • can represent any continuous function • Two hidden layers: • can represent any discontinuous function Hierarchically composed feature representations Learning features relevant to the data Lehman, Clune, & Risi 2015 Lee et al. 2007 Learning features relevant to the data Lehman, Clune, & Risi 2015 Lee et al. 2007 • I give a whole talk on work my colleagues and I have done in visualizing deep neural networks that I think you will enjoy • Deep Learning Overview & Visualizing What Deep Neural Networks Learn • https://www.youtube.com/watch?v=3lp9eN5JE2A “Hierarchies of Parts” Motivation for Deep Learning · Each “neuron” might recognize a “part” of a digit. – “Deeper” neurons might recognize combinations of parts . – Represent complex objects as hierarchical combinations of re -useable parts (a simple “grammar”). · Watch the full video here: – https://www.youtube.com/watch?v=aircAruvnKk · Theory: – 1 big -enough hidden layer already gives universal approximation. – But some functions require exponentially -fewer parameters to approximate with more layers (can fight curse of dimensionality). Deep Learning Terminology · “layer” = number of layers of weights (numWs + V) · “hidden layer” = number of activation layers (Zs) (not including inputs, ) · Everyone: use both terms to describe this net Wh y M u l t i p l e L a y er s ? • H i s t o r i c a l l y , de e p l e a r ni ng w a s m o t i v a t e d by “ c o nne c t i o ni s t ” i de a s : – B r a i n c o n s i s t s o f n e t w o r k o f h i g h l y - co n n e c t e d si m p l e uni t s . • Sa m e u n i t s r e p e a t e d i n v a r i ou s p l a c e s. • Com p u t a t i on s ar e d o n e in pa r a l l e l . • I n f o r m a ti o n is s t o r e d in d is t r ib u t e d w a y . • Le a r n i n g c o m e s f r o m u p d a ti n g of c on n e c t i on st r e n g t h s . • On e l e a r n i n g a l g o r i t h m u s e d e v e ry w h e r e . ht t p s : / / w w w . n y t i m e s . c o m / 2 0 1 5 / 0 1 / 1 1 / m a g a z i n e / s e b a s t i a n - se u n g s - que s t - to - ma p - th e - hum a n - br a i n. h t m l Wh y M u l t i p l e L a y er s ? • A nd t he o r i e s o n t he hi e r a r c hi c a l o r g a ni z a t i o n o f t he v i s ua l s y s t e m : ht t p : / / w w w . s t r o k e n e t w o r k . o r g / n e w s l e t t e r / a r t i c l e s / v i s i o n . ht m ht t p s : / / e n . w i k i b o o k s . o r g / w i k i / S e n s o r y _ S y s t e m s / V i s u a l _ S i g n a l _ P r o c e s s i n g Wh y M u l t i p l e L a y er s ? • T he i de a o f m ul t i - l a y e r de s i g ns a ppe a r s i n e ng i ne e r i ng t o o : – D e e p h i e r a r c h i e s i n c a m e r a d e s i g n : ht t p : / / w w w . a r g m i n . n e t / 2 0 1 8 / 0 1 / 2 5 / o p t i c s / Wh y M u l t i p l e L a y er s ? • T he r e a r e a l s o m a t he m a t i c a l m o t i v a t i o ns f o r us i ng m ul t i pl e l a y e r s : – 1 l a y e r g i v e s u s a u n i v e r s a l ap p r o x im a t or . • Bu t t h i s la y e r m ig h t n e e d t o b e h u g e . – W i t h d e e p n e t w o r k s : • Som e f u n c t i on s c a n b e a p p r o x i m a t e d w i th e x p o n e n ti a l l y - fe w e r p a ra m e t e r s . – Co m p a r ed t o a n e t w o r k w i t h 1 h i d d en l a y er . • So d e e p n e t w or k s m a y n e e d f e w e r p a r a m e t e r s t h a n “ sh a l l o w b u t wi d e ” n e t w or k s. – An d h e n c e ma y n e e d l e s s d a t a t o t r a i n . • E m pi r i c a l m o t i v a t i o n f o r us i ng m ul t i pl e l a y e r s : – I n m a n y d o m a i n s d e e p n e t w o r k s h a v e l e d t o u n p r e c e d e n t e d p e r f o r m a n c e . Deep Learning Deep Learning · For 4 layers, we could write the prediction as: · For ‘m’ layers, we usually just say: https://mathwithbaddrawings.com/2016/04/27/symbols -that -math-urgently-needs-to-adopt ML and Deep Learning History · 1950 and 1960s: Initial excitement. – Perceptron : linear classifier and stochastic gradient (roughly). – “the embryo of an electronic computer that [the Navy] expects will be able to walk, talk, see, write, reproduce itself and be conscious of its existence.” New York Times (1958). · https://www.youtube.com/watch?v=IEFRtz68m-8 – Object recognition assigned to students as a summer project · Then drop in popularity: – Quickly realized limitations of linear models . https://mitpress.mit.edu/books/perceptrons/ ML and Deep Learning History · 1970 and 1980s: Connectionism (brain -inspired ML) – Want “connected networks of simple units ”. · Use parallel computation and distributed representations. – Adding hidden layers z i increases expressive power. · With 1 layer and enough sigmoid units, a universal approximator. – Success in optical character recognition. https://en.wikibooks.org/wiki/Sensory_Systems/Visual_Signal_Processing http://www.datarobot.com/blog/a -primer -on-deep-learning/ http://blog.csdn.net/strint/article/details/44163869 ML and Deep Learning History · 1990s and early -2000s: drop in popularity. – It proved really difficult to get multi - layer models working robustly. – We obtained similar performance with simpler models: · Rise in popularity of logistic regression and SVMs with regularization and kernels . – Lots of internet successes (spam filtering, web search, recommendation). – ML moved closer to other fields like numerical optimization and statistics. ML and Deep Learning History · Late 2000s: push to revive connectionism as “deep learning ”. – Canadian Institute For Advanced Research (CIFAR) NCAP program: · “Neural Computation and Adaptive Perception”. · Led by Geoff Hinton, Yann LeCun , and Yoshua Bengio · Unsupervised successes: “deep belief networks” and “autoencoders”. · Could be used to initialize deep neural networks. · https://www.youtube.com/watch?v=KuPai0ogiHk https://www.cs.toronto.edu/~hinton/science.pdf 2010s: DEEP LEARNING!!! · Bigger datasets, bigger models, parallel computing (GPUs/clusters). – And some tweaks to the models from the 1980s. · Huge improvements in automatic speech recognition (2009). – All phones now have deep learning. · Huge improvements in computer vision (2012). – Changed computer vision field almost instantly. – This is now finding its way into products. http://www.image - net.org/challenges/LSVRC/2014/ 2010s: DEEP LEARNING!!! · Media hype: – “How many computers to identify a cat? 16,000” New York Times (2012). – “Why Facebook is teaching its machines to think like humans” Wired (2013). – “What is ‘deep learning’ and why should businesses care?” Forbes (2013). – “Computer eyesight gets a lot more accurate” New York Times (2014). · 2015: huge improvement in language understanding . ImageNet Challenge · Millions of labeled images, 1000 object classes. http://www.image -net.org/challenges/LSVRC/2014/ ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. Syberian Husky Canadian Husky https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements/ http://arxiv.org/pdf/1409.0575v3.pdf http://arxiv.org/pdf/1409.4842v1.pdf ImageNet Challenge · Object detection task: – Single label per image. – Humans: ~5% error. · 2015: Won by Microsoft Asia – 3.6% error. – 152 layers, introduced “ResNets”. – Also won “localization” (finding location of objects in images). · 2016: Chinese University of Hong Kong: – Ensembles of previous winners and other existing methods. · 2017: fewer entries, organizers decided this would be last year. http://www.themtank.org/a-year-in-computer-vision https://paperswithcode.com /sota /image -classification-on-imagenet DeepViz Toolbox https://www.youtube.com/watch?v=AgkfIQ4IGaM Another video my colleagues and I made, but it will make more sense after we do convolutions Windsor Tie Anh Nguyen Summary · Neural networks learn features z i for supervised learning. · Sigmoid function avoids degeneracy by introducing non-linearity. – Universal approximator with large - enough ‘k’. · Biological motivation for (deep) neural networks. · Deep learning considers neural networks with many hidden layers. – Can more- efficiently represent some functions. · Unprecedented performance on difficult pattern recognition tasks. Multiple Word Prototypes · What about homonyms and polysemy? – The word vectors would need to account for all meanings. · More recent approaches: – Try to cluster the different contexts where words appear. – Use different vectors for different contexts . Multiple Word Prototypes http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes Why z i = Wx i ? · In PCA we had that the optimal Z = XW T (WW T ) - 1 . · If W had normalized+orthogonal rows, Z = XW T (since WW T = I). – So z i = Wx i in this normalized+orthogonal case. · Why we would use z i = Wx i in neural networks? – We didn’t enforce normalization or orthogonality. · Well, the value W T (WW T ) - 1 is just “some matrix”. – You can think of neural networks as just directly learning this matrix . (pause) Deep Learning Practicalities · Next we focus on deep learning practical issues: – Backpropagation to compute gradients. – Stochastic gradient training. – Regularization to avoid overfitting. Artificial Neural Networks · With squared loss and 1 hidden layer, our objective function is: · Usual training procedure: stochastic gradient descent (SGD) – Compute gradient of random example ‘ i ’, update both ‘v’ and ‘W’. – Highly non- convex and can be difficult to tune. · Computing the gradient is known as “backpropagation”. Backpropagation · Overview of how we compute neural network gradient: – Forward propagation : · Compute z i (1) from x i . · Compute z i (2) from z i (1) . · … · Compute \"! i from z i (m) , and use this to compute error. – Backpropagation : · Compute gradient with respect to regression weights ‘v’. · Compute gradient with respect to z i (m) weights W (m) . · Compute gradient with respect to z i (m - 1) weights W (m - 1) . · … · Compute gradient with respect to z i (1) weights W (1) . · “Backpropagation” is the chain rule plus some bookkeeping for speed. Backpropagation · Instead of the next few bonus slides, I HIGHLY recommend watching this video from former UBC master’s student Andrej Karpathy (of OpenAI, former director of AI and Autopilot Vision at Tesla) – https://www.youtube.com/watch?v =i94OvYb6noo Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. Backpropagation · Let’s illustrate backpropagation in a simple setting: – 1 training example, 3 hidden layers, 1 hidden “unit” in layer. – Only the first ‘r’ changes if you use a different loss. – With multiple hidden units, you get extra sums. · Efficient if you store the sums rather than computing from scratch. Backpropagation · We’ve marked those backprop math slides as bonus. · Do you need to know how to do this? – Exact details are probably not vital (there are many implementations). – “Automatic differentiation ” is now standard and has same cost. – But understanding basic idea helps you know what can go wrong. · Or give hints about what to do when you run out of memory. – See discussion by a neural network expert (Andrej!) - https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b Backpropagation · You should know cost of backpropagation: – Forward pass dominated by matrix multiplications by W (1) , W (2) , W (3) , and ‘v’. · If have ‘m’ layers and all z i have ‘k’ elements, cost would be O( dk + mk 2 ). – Backward pass has same cost as forward pass. Multi - class / Multi- label networks · For ‘k’ labels, replace ‘v’ by a matrix with ‘k’ columns – “Top” of a neural network is just a linear model (with learned ‘ z i ’)… – …so we can do all the same tricks we already learned – Can still do backprop the same way · Multi-class: we already learned the softmax loss! – Often called “cross entropy ” by neural network people · Multi-label: add up logistic loss (or whatever) on each output – In linear models, this was like running separate regressions – Here, we learn the ‘ z i ’ for all labels at once, so it can help to do together Deep Learning Vocabulary · “Deep learning ”: Models with many hidden layers. – Usually neural networks. · “Neuron”: node in the neural network graph. – “ Visible unit ”: feature. – “ Hidden unit ”: latent factor z ic or h(z ic ). · “Activation function”: non- linear transform. · “Activation”: h(z i ). · “Backpropagation”: compute gradient of neural network. – Sometimes “backpropagation” means “ training with SGD ”. · “ Weight decay ”: L2- regularization. · “Cross entropy ”: softmax loss. · “Learning rate”: SGD step- size. · “Learning rate decay ”: using decreasing step- sizes. · “ Vanishing/Exploding gradient ”: gradient becoming real small/big for deep net (pause) ImageNet Challenge and Optimization · ImageNet challenge: – Use millions of images to recognize 1000 objects. · ImageNet organizer visited UBC summer 2015. · “Besides huge dataset/model/cluster, what is the most important?” 1. Data augmentation Image transformations (translation, rotation, scaling, lighting, etc.). 2. Optimization. · Why would optimization be so important? – Neural network objectives are highly non- convex (and worse with depth). – Optimization has huge influence on quality of model. Stochastic Gradient Training · Standard training method is stochastic gradient (SG): – Choose a random example ‘i ’. – Use backpropagation to get gradient with respect to all parameters. – Take a small step in the negative gradient direction. · Challenging to make SG work: – Often doesn’t work as a “black box” learning algorithm. – But people have developed a lot of tricks/modifications to make it work. · Highly non- convex , so are the problem local minima? – Some empirical/theoretical evidence that local minima are not the problem. – If the network is “deep” and “wide” enough, we think all local minima are good. – But it can be hard to get SG to close to a local minimum in reasonable time. Ne w I s s u e: V a n i s h i n g G r a d i en t s • Co n s i d e r t h e s i gm o i d fu n c t i o n : • A w a y f r o m t h e o r i g i n , t h e gr a d i en t i s n ea r l y z er o . • Th e p r ob l e m g e ts w or s e wh e n y ou t a k e th e si g m o i d o f a si g m o i d : • I n d e e p n e t w o r k s , m a n y gr a d i en t s c a n b e n ea r l y z er o e v er y w h er e . – A n d n u m e ri c a l l y t h e y w i l l b e s e t t o 0 , s o SGD d oe s n ot m o v e . R e c t i f i e d L i n e a r Un i t s ( Re L U ) • Mo de r n ne t w o r k s o f t e n r e pl a c e si g m o i d w i t h pe r c e p t r o n l o ss ( Re L U ): • J u s t se t s ne g a t i v e v a l ue s z ic t o z e r o . – Re d u c e s va n i s h i n g g r a d i e n t p r o b l e m ( p o s i t i v e r e g i o n i s n e v e r f l a t ) . – G i v e s s p a r s e r a c t i v a t i o n s . – St i l l gi v e s a u n i v e r s a l ap p r o x im a t o r i f s i z e o f h i d d e n l a y e r s g r o w s w i t h ‘n ’ . “Swish” Activiation · Recent work searched for “best” activation: · Found that z ic /(1+exp( -z ic )) worked best (“swish” function). – A bit weird because it allows negative values and is non- monotonic. – But basically the same as ReLU when not close to 0. Sk ip Con n e c t ion s De e p L e ar n in g • S k i p c o nne c t i o ns c a n a l s o r e duc e v a ni s hi ng g r a di e n t pr o bl e m : • Ma k e s “ s ho r t c ut s ” be t w e e n l a y e r s ( s o f e w e r t r a ns f o r m a t i o ns ) . – M a n y v a r i a t i o n s e x i s t o n s k i p c o n n e c t i o n s e x i s t . Re s N e t “B l o c k s ” • Re s i d u a l n e t w o r k s ( Re s N e t s ) ar e a v ar ian t o n s k ip c o n n e c t io n s . – C o n s i s t o f r e p e a t e d “ b l o c k s ” , fi r s t m e t h o d s t h a t s u c c es s fu l l y u s ed 1 0 0 + la y e r s . • Us u a l c o m p u t a t i o n o f a ct i v a t i o n b a s e d o n p r e v i o u s 2 l a y e r s : • Re s N e t “ bl o c k ”: – Ad d s a ct i v a t i on s f r om “2 l a y e r s a g o” . • Di f f e r e n ce s f r o m u s u a l s k i p c o n n e ct i o n s : – Act i v a t i on s v e ct or s a l an d a l+ 2 mu s t h a v e th e sa m e si z e . – No w e ig h t s o n a l , s o W l an d W l+ 1 mu s t f o c u s o n “ u p d a ti n g ” a l (f i t “r e s i d u a l ”). • I f y o u us e Re L U , t he n W l = 0 i m pl i e s a l+ 2 =a l . ht t p s : / / e n . w ik ip e d ia. o r g / w ik i/ R e s id u al_ n e u r al_ n e t w o r k ht t p s : / / t o w a r d s d a t a s c i e n c e . c o m / a n - ov e r v i e w - of - re s n e t - an d - it s - va r i a n t s - 5281e 2f 56035 Parameter Initialization · Parameter initialization is crucial: – Can’t initialize weights in same layer to same value, or units will stay the same. · Architecture is symmetric, so gradient would be the same for every hidden unit in the layer, so they’d all just always stay doing the exact same thing. – Can’t initialize weights too large, it will take too long to learn. · A traditional random initialization: – Initialize bias variables to 0. – Sample from standard normal, divided by 10 5 (0.00001* randn). · w = .00001* randn(k,1) – Performing multiple initializations does not seem to be important (except maybe with very small networks). Parameter Initialization · Parameter initialization is crucial: – Can’t initialize weights in same layer to same value, or they will stay same. – Can’t initialize weights too large, it will take too long to learn. · Also common to transform data in various ways: – Subtract mean, divide by standard deviation, “whiten”, standardize y i . · More recent initializations try to standardize initial z i : – Use different initialization in each layer. – Try to make variance of z i the same across layers. · Popular approach is to sample from standard normal, divide by sqrt(2* nInputs ). – Use samples from uniform distribution on [ - b,b], where Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · Common approach: manual “babysitting” of the step-size. – Run SG for a while with a fixed step- size. – Occasionally measure error and plot progress: – If error is not decreasing, decrease step- size. Setting the Step - Size · Stochastic gradient is very sensitive to the step size in deep models. · Bias step -size multiplier : use bigger step-size for the bias variables. · Momentum (stochastic version of “heavy-ball” algorithm): – Add term that moves in previous direction: – Usually β t = 0.9. Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Gradient Descent vs. Heavy- Ball Method Good demo to check out: https://distill.pub /2017/momentum/ Setting the Step - Size · Automatic method to set step size is Bottou trick : 1. Grab a small set of training examples (maybe 5% of total). 2. Do a binary search for a step size that works well on them. 3. Use this step size for a long time (or slowly decrease it from there). · Several recent methods using a step size for each variable : – AdaGrad, RMSprop, Adam (often work better “out of the box”). – Some controversy versus plain stochastic gradient (often with momentum). · SGD can often get lower test error, even though it takes longer and requires more tuning of step- size. · Batch size (number of random examples) also influences results. – Bigger batch sizes often give faster convergence but maybe to worse solutions? · Another recent trick is batch normalization: – Try to “standardize” the hidden units within the random samples as we go. – Held as example of deep learning “alchemy ” (blog post here about deep learning claims). · Sounds science - ey and often works, but little theoretical understanding. Com m on De e p L e ar n in g T r ic k s • Da t a st a n d a r d i z a t i o n ( “ c e n t e r i n g ” a n d “ w h i t e n i n g ”) . • Pa r a m e t e r in it ializ a t io n : “ sm a l l but di f f e r e n t “. – I f w e i n i t i a l i z e a l l p a r a m e t e r s i n t h e l a y e r t o s a m e v a l u e , t h e y s t a y t h e s a m e . – Al s o c om m on t o u s e i n i t i a l i z a t i on s t h a t a r e st a n d a r d i z e d wi th i n l a y e r s . • St e p - si z e se l e c t i o n: “ ba b y si t t i ng “. – U s e b i g g e r st e p - s i z e f o r t h e b i a s v a ri a b l e s , o r d i f f e r e n t f o r e a c h l a y e r . – M e t h o d s t h a t u s e a s t e p s i z e f o r e ac h c o o r d in a t e ( Ad a Gr a d , RM S p r op , Ad a m ). • E a r l y s t op p i n g of th e op ti mi z a ti on b a s e d on v a l i d a ti on a c c u r a c y . • Mo m e n t um : ad d s w e ig h t e d s u m o f p r e vio u s S GD d ir e c t io n s . • Ba t c h n o r m a l i z a t i o n : a d a p ti v e s t a n d a r d i z i n g w i th i n l a y e r s . – O f t e n a l l o w s s i g m o i d a c t i v a t i o n s i n d e e p n e t w o rk s . Com m on De e p L e ar n in g T r ic k s • L2 - re g u l a r i z a t i o n or L1 - re g u l a r i z a t i o n ( “ w e i g h t d e c a y ” ) . – Som e t i m e s wi t h d i f f e r e n t ! fo r e a c h l a y e r . – R e c e n t w o r k s h o w s th i s c a n i n tr o d u c e b a d l o c a l o p ti m a . • Dr o p o u t : r a n d o m l y z e r o e s a c t i v a t i o n s ‘z ’ v a l u e s t o d i s c o u r a g e d e p e n d e n c e . • Re c t i f i e d l i n e a r u n i t s ( Re L U ) a s n o n - l i n e a r t r a n s f o r m a t i o n . – Ma k e s o bj e ct i v e no n - d i f f e r e n t i a b l e , b u t w e n o w k n o w S G D s t i l l c o n v e r g e s i n t h i s s e t t i n g . • R e s i d u a l / s k i p c o n n e c t i o n s : c o n n e c t l a y e r s t o m u l t i p l e p r e v i o u s l a y e r s . – We n o w k n o w t h a t s u c h c o n n e c t i o n s m a k e i t m o r e l i k e l y t o c o n v e r g e t o g o o d m i n i m a . • Ne u r a l a r c h i t e c t u r e s e a r c h : t r y t o c l e v e r l y s e a r c h sp a c e o f h y p e r - pa r a me t e r s . – Th i s g e t s e x p en s i v e! • S o m e o f t h e s e tr ic k s a r e e x p l o r e d i n b o n u s s l i d e s . Mi s s i n g T h e o r y B e h i n d T r a i n i n g D e e p N e tw o rk s • Un f o r t u n a t el y , we d o n o t u n d e r s t a n d m a n y o f t h e s e t r i c k s v e r y w e l l . – La r g e p o r ti o n o f th e o r y i s o n d e g e n e r a t e c a s e o f l i n e a r n e u r a l n e tw o r k s . • Or o t h e r w e i r d c a s e s l i k e “ 1 h i d d e n u n i t p e r l a y e r ” . – A l o t o f r e s e a r c h i s p e r f o r m e d u s i n g “ gr a d s tu d e n t d e sc e n t ”. • S e v e r a l v a r i a t i o n s a r e t r i e d , on e s th a t p e r f o r m w e l l e m p i r i c a l l y a r e ke p t ( p o s s i b l y o v e r f i t t i n g ) . • P o p u l a r E x a m p l e s : – Ba t c h n or m a l i z a t i on or i g i n a l l y p r op ose d t o fi x “ i n t e r n a l c o v a r i a t e sh i ft ” . • I n t e r n a l c o v a r i a t e s h i f t n o t d e f i n e d i n o r i g i n a l p a p e r , a n d b a t c h n o r m d o e s s e e m t o r e d u c e i t . – O f t e n s i ng l e d o ut a s a n e x a m pl e o f pr o bl e m s w i t h m a c hi ne l e a r ni ng s c ho l a r s hi p . • L i k e m a n y h e u r i s t i c s , p e o p l e u s e b a t c h n o rm b e c a u s e th e y f o u n d th a t i t o f t e n h e l p s . – M a n y pe o pl e ha v e w o r k e d o n be t t e r e x pl a na t i o ns . – Ad a m o p ti m i z e r is a n ic e c o m b in a t io n s o f id e as f r o m s e v e r al e x is t in g alg o r it h m s . • Suc h as “ m o m e n t um ” and “ Ad a G r a d ” , b o t h o f w h i c h a r e w e l l - unde r s t o o d t he o r e t ic ally . – B ut t he o r y i n t he o r i g i na l pa pe r w a s i nc o r r e c t , a nd A da m f a i l s a t s o l v i ng s o m e v e r y - s i m pl e o p t i m i z a t i o n pr o bl e m s . • Bu t i s A d a m i s o f t en u s ed b ec a u s e i t i s a m a z i n g a t t r a i n i n g s o m e n e t w o r k s . – It ha s be e n h y po t he s i z e d t ha t w e “ c o n v e r g e d” t o w a r ds ne t w o r k s t ha t a r e e a s i e r f o r c ur r e n t S G D m e t ho ds l i k e A da m . Summary · Unprecedented performance on difficult pattern recognition tasks. · Backpropagation computes neural network gradient via chain rule. · Parameter initialization is crucial to neural net performance. · Optimization and step size are crucial to neural net performance. – “Babysitting”, momentum. · ReLU avoid “vanishing gradients”. · Next: The most important idea in computer vision? Autoencoders · Autoencoders are an unsupervised deep learning model: – Use the inputs as the output of the neural network. – Middle layer could be latent features in non - linear latent-factor model. · Can do outlier detection, data compression, visualization, etc. – A non - linear generalization of PCA. · Equivalent to PCA if you don’t have non- linearities . http://inspirehep.net/record/1252540/plots Autoencoders https://www.cs.toronto.edu/~hinton/science.pdf Denoising Autoencoder · Denoising autoencoders add noise to the input: – Learns a model that can remove the noise. http://inspirehep.net/record/1252540/plots","libVersion":"0.2.1","langs":""}