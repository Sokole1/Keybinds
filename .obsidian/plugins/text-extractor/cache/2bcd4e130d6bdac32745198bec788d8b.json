{"path":".obsidian/plugins/text-extractor/cache/2bcd4e130d6bdac32745198bec788d8b.json","text":"CPSC 340 Assignment 4 (Due 2023-11-10 at 11:59pm) Important: Submission Format [5 points] Please make sure to follow the submission instructions posted on the course website. We will deduct marks if the submission format is incorrect, or if you’re not using LATEX and your submission is at all diﬃcult to read – at least these 5 points, more for egregious issues. 1 Convex Functions [15 points] Recall that convex loss functions are typically easier to minimize than non-convex functions, so it’s important to be able to identify whether a function is convex. Show that the following functions are convex: 1. f (w) = αw2 − βw + γ with w ∈ R, α ≥ 0, β ∈ R, γ ∈ R (1D quadratic). 2. f (w) = − log(αw) with α > 0 and w > 0 (“negative logarithm”) 3. f (w) = ∥Xw − y∥1 + λ 2 ∥w∥1 with w ∈ Rd, λ ≥ 0 (L1-regularized robust regression). 4. f (w) = ∑n i=1 log(1 + exp(−yiwT xi)) with w ∈ Rd (logistic regression). 5. f (w) = ∑n i=1[max{0, |wT xi − yi|} − ϵ] + λ 2 ∥w∥ 2 2 with w ∈ Rd, ϵ ≥ 0, λ ≥ 0 (support vector regression). General hint: for the ﬁrst two you can check that the second derivative is non-negative since they are one- dimensional. For the last 3, it’s easier to use some of the results regarding how combining convex functions can yield convex functions; which can be found in the lecture slides. Hint for part 4 (logistic regression): this function may at ﬁrst seem non-convex since it contains log(z) and log is concave, but note that log(exp(z)) = z is convex despite containing a log. To show convexity, you can reduce the problem to showing that log(1 + exp(z)) is convex, which can be done by computing the second derivative. It may simplify matters to note that exp(z) 1+exp(z) = 1 1+exp(−z) . 1 2 Logistic Regression with Sparse Regularization [30 points] If you run python main.py 2, it will: 1. Load a binary classiﬁcation dataset containing a training and a validation set. 2. Standardize the columns of X, and add a bias variable (in utils.load_dataset). 3. Apply the same transformation to Xvalidate (in utils.load_dataset). 4. Fit a logistic regression model. 5. Report the number of features selected by the model (number of non-zero regression weights). 6. Report the error on the validation set. Logistic regression does reasonably well on this dataset, but it uses all the features (even though only the prime-numbered features are relevant) and the validation error is above the minimum achievable for this model (which is 1 percent, if you have enough data and know which features are relevant). In this question, you will modify this demo to use diﬀerent forms of regularization to improve on these aspects. Note: your results may vary slightly, depending on your software versions, the exact order you do ﬂoating- point operations in, and so on. 2.1 L2-Regularization [5 points] In linear_models.py, you will ﬁnd a class named LinearClassifier that deﬁnes the ﬁtting and prediction behaviour of a logistic regression classiﬁer. As with ordinary least squares linear regression, the particular choice of a function object (fun_obj) and an optimizer (optimizer) will determine the properties of your output model. Your task is to implement a logistic regression classiﬁer that uses L2-regularization on its weights. Go to fun_obj.py and complete the LogisticRegressionLossL2 class. This class’ constructor takes an input parameter λ, the L2 regularization weight. Speciﬁcally, while LogisticRegressionLoss computes f (w) = n∑ i=1 log(1 + exp(−yiwT xi)), your new class LogisticRegressionLossL2 should compute f (w) = n∑ i=1 [ log(1 + exp(−yiwT xi))] + λ 2 ∥w∥2. and its gradient. Submit your function object code. Using this new code with λ = 1, report how the following quantities change: (1) the training (classiﬁcation) error, (2) the validation (classiﬁcation) error, (3) the number of features used, and (4) the number of gradient descent iterations. Note: as you may have noticed, lambda is a special keyword in Python, so we can’t use it as a variable name. Some alternative options: lammy (what Mike’s niece calls her toy stuﬀed lamb), lamda, reg_wt, λ if you feel like typing it, the sheep emoji 1, . . . . 2.2 L1-Regularization and Regularization Path [5 points] L1-regularized logistic regression classiﬁer has the following objective function: f (w) = n∑ i=1 [ log(1 + exp(−yiwT xi))] + λ∥w∥1. 1Harder to insert in LATEX than you’d like; turns out there are some drawbacks to using software written in 1978. 2 Because the L1 norm isn’t diﬀerentiable when any elements of w are 0 – and that’s exactly what we want to get – standard gradient descent isn’t going to work well on this objective. There is, though, a similar approach called proximal gradient descent that does work here. 2 This is implemented for you in the GradientDescentLineSearchProxL1 class inside optimizers.py. Note that to use it, you don’t include the L1 penalty in your loss function object; the optimizer handles that itself. Write and submit code to instantiate LinearClassifier with the correct function object and optimizer for L1-regularization. Using this linear model, obtain solutions for L1-regularized logistic regression with λ = 0.01, λ = 0.1, λ = 1, λ = 10. Report the following quantities per each value of λ: (1) the training error, (2) the validation error, (3) the number of features used, and (4) the number of gradient descent iterations. 2.3 L0 Regularization [8 points] The class LogisticRegressionLossL0 in fun_obj.py contains part of the code needed to implement the forward selection algorithm, which approximates the solution with L0-regularization, f (w) = n∑ i=1 [ log(1 + exp(−yiwT xi))] + λ∥w∥0. The class LinearClassifierForwardSel in linear_models.py will use a loss function object and an opti- mizer to perform a forward selection to approximate the best feature set. The for loop in its fit() method is missing the part where we ﬁt the model using the subset selected_new, then compute the score and updates the min_loss and best_feature. Modify the for loop in this code so that it ﬁts the model using only the features selected_new, computes the score above using these features, and updates the variables min_loss and best_feature, as well as self.total_evals. Hand in your updated code. Using this new code with λ = 1, report the training error, validation error, number of features selected, and total optimization steps. 2Here’s an explanation, as bonus content you don’t need to understand. (Feel free to delete this overly long footnote from your answers ﬁle, if you want.) For the explanation to make sense, it’ll help to ﬁrst re-frame gradient descent in the following way: to take a step from wt while trying to minimize an objective function f , we ﬁrst make a quadratic approximation to f around the point wt of the form ˜f t(w) = f (wt) + [∇f (wt)]T (w − wt) + 1 2αt ∥w − wt∥ 2. This is like taking a Taylor expansion of f , but instead of using the expensive-to-compute Hessian ∇2f , we just use 1 αt I. Then we minimize that approximation to ﬁnd our next step: wt+1 = arg minw ˜f t(w), which if you do out the math ends up being exactly our old friend wt+1 = w − αt∇f (wt).3 In proximal gradient descent, our objective f (w) is of the form g(w) + h(w), where g is a smooth function (e.g. the logistic regression loss) but h might not be diﬀerentiable. Then the idea of proximal gradient descent is that we do the quadratic approximation for g but just leave h alone: wt+1 = arg min w g(wt) + [∇g(wt)] T (w − wt) + 1 2αt ∥w − wt∥ 2 + h(w) = arg min w 1 2αt ∥w − (wt − αt∇g(w))∥ 2 + h(w), (prox) an optimization problem trying to trade oﬀ being close to the gradient descent update (ﬁrst term) with keeping h small (second). As long as you can compute ∇g(w), this problem otherwise doesn’t depend on g at all: you can just run the gradient descent update based on g then plug that into the “prox update” (prox). For many important functions h, this is available in closed form. For L1 regularization we have h(w) = λ∥w∥1, and it turns out that the solution is the “soft-thresholding” function, given elementwise by [ arg min w 1 2α ∥w − z∥2 + λ∥w∥1 ] i =  | | zi − αλ if zi > αλ 0 if |zi| ≤ αλ zi + αλ if zi < −αλ . 3Incidentally, using the real Hessian here is called Newton’s method. This is a much better approximation to f , and so the update steps it takes can be much better than gradient descent, causing it to converge in many fewer iterations. But each of these iterations is much more computationally expensive, since we need to compute and solve a linear system with the d × d Hessian. In ML settings it’s often too computationally expensive to run.4 3 Note that the code diﬀers slightly from what we discussed in class, since we’re hard-coding that we include the ﬁrst (bias) variable. Also, note that for this particular case using the L0-norm with λ = 1 is using the Akaike Information Criterion (AIC) for variable selection. Also note that, for numerical reasons, your answers may vary depending on exactly what system and package versions you are using. That is ﬁne. 2.4 Discussion [4 points] In a short paragraph, brieﬂy discuss your results from the above. How do the diﬀerent forms of regularization compare with each other? Can you provide some intuition for your results? No need to write a long essay, please! 2.5 L 1 2 regularization [8 points] Previously we’ve considered L2- and L1- regularization which use the L2 and L1 norms respectively. Now consider least squares linear regression with “L 1 2 regularization” (in quotation marks because the “L 1 2 norm” is not a true norm): f (w) = 1 2 n∑ i=1(wT xi − yi) 2 + λ d∑ j=1 |wj|1/2 . Let’s consider the case of d = 1 and assume there is no intercept term being used, so the loss simpliﬁes to f (w) = 1 2 n∑ i=1(wxi − yi) 2 + λ√ |w| . Finally, let’s assume the very special case of n = 2, where our 2 data points are (x1, y1) = (1, 2) and (x2, y2) = (0, 1). 1. Plug in the dataset values and write the loss in a simpliﬁed form, without a ∑ . 2. If λ = 0, what is the solution, i.e. arg minw f (w)? 3. If λ → ∞, what is the solution, i.e., arg minw f (w)? 4. Plot f (w) when λ = 1. What is arg minw f (w) when λ = 1? Answer to one decimal place if appropriate. (For the plotting questions, you can use matplotlib or any graphing software, such as https://www. desmos.com.) 5. Plot f (w) when λ = 10. What is arg minw f (w) when λ = 10? Answer to one decimal place if appropriate. 6. Does L 1 2 regularization behave more like L1 regularization or L2 regularization when it comes to performing feature selection? Brieﬂy justify your answer. 7. Is least squares with L 1 2 regularization a convex optimization problem? Brieﬂy justify your answer. 4 3 Multi-Class Logistic Regression [32 points] If you run python main.py 3 the code loads a multi-class classiﬁcation dataset with yi ∈ {0, 1, 2, 3, 4} and ﬁts a “one-vs-all” classiﬁcation model using least squares, then reports the validation error and shows a plot of the data/classiﬁer. The performance on the validation set is ok, but could be much better. For example, this classiﬁer never even predicts that examples will be in classes 0 or 4. 3.1 Softmax Classiﬁcation, toy example [4 points] Linear classiﬁers make their decisions by ﬁnding the class label c maximizing the quantity wT c xi, so we want to train the model to make wT yixi larger than wT c′xi for all the classes c ′ that are not yi. Here c ′ is a possible label and wc′ is row c ′ of W . Similarly, yi is the training label, wyi is row yi of W , and in this setting we are assuming a discrete label yi ∈ {1, 2, . . . , k}. Before we move on to implementing the softmax classiﬁer to ﬁx the issues raised in the introduction, let’s work through a toy example: Consider the dataset below, which has n = 10 training examples, d = 2 features, and k = 3 classes: X =                 0 1 1 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0                 , y =                 1 1 1 2 2 2 2 3 3 3                 . Suppose that you want to classify the following test example: ˜x = [ 1 1 ] . Suppose we ﬁt a multi-class linear classiﬁer using the softmax loss, and we obtain the following weight matrix: W =   +2 −1 +2 −2 +3 −1   Under this model, what class label would we assign to the test example? (Show your work.) 3.2 One-vs-all Logistic Regression [7 points] Using the squared error on this problem hurts performance because it has “bad errors” (the model gets penal- ized if it classiﬁes examples “too correctly”). In linear_models.py, complete the class named LinearClassifierOneVsAll that replaces the squared loss in the one-vs-all model with the logistic loss. Hand in the code and report the validation error. 3.3 Softmax Classiﬁer Gradient [7 points] Using a one-vs-all classiﬁer can hurt performance because the classiﬁers are ﬁt independently, so there is no attempt to calibrate the columns of the matrix W . As we discussed in lecture, an alternative to this independent model is to use the softmax loss, which is given by f (W ) = n∑ i=1 [ −wT yixi + log ( k∑ c′=1 exp(wT c′xi) )] , 5 Show that the partial derivatives of this function, which make up its gradient, are given by the following expression: ∂f ∂Wcj = n∑ i=1 xij[p(yi = c | W, xi) − 1 (yi = c)] , where... • 1 (yi = c) is the indicator function (it is 1 when yi = c and 0 otherwise) • p(yi = c | W, xi) is the predicted probability of example i being class c, deﬁned as p(yi = c | W, xi) = exp(wT c xi) ∑k c′=1 exp(wT c′xi) 3.4 Softmax Classiﬁer Implementation [8 points] Inside linear_models.py, you will ﬁnd the class MulticlassLinearClassifier, which ﬁts W using the softmax loss from the previous section instead of ﬁtting k independent classiﬁers. As with other linear models, you must implement a function object class in fun_obj.py. Find the class named SoftmaxLoss. Complete these classes and their methods. Submit your code and report the validation error. Hint: You may want to use check_correctness() to check that your implementation of the gradient is correct. Hint: With softmax classiﬁcation, our parameters live in a matrix W instead of a vector w. However, most optimization routines (like scipy.optimize.minimize or our optimizers.py) are set up to optimize with respect to a vector of parameters. The standard approach is to “ﬂatten” the matrix W into a vector (of length kd, in this case) before passing it into the optimizer. On the other hand, it’s inconvenient to work with the ﬂattened form everywhere in the code; intuitively, we think of it as a matrix W and our code will be more readable if the data structure reﬂects our thinking. Thus, the approach we recommend is to reshape the parameters back and forth as needed. The skeleton code of SoftmaxLoss already has lines reshaping the input vector w into a k × d matrix using np.reshape. You can then compute the gradient using sane, readable code with the W matrix inside evaluate(). You’ll end up with a gradient that’s also a matrix: one partial derivative per element of W . Right at the end of evaluate(), you can ﬂatten this gradient matrix into a vector using g.reshape(-1). If you do this, the optimizer will be sending in a vector of parameters to SoftmaxLoss, and receiving a gradient vector back out, which is the interface it wants – and your SoftmaxLoss code will be much more readable, too. You may need to do a bit more reshaping elsewhere, but this is the key piece. Hint: A na¨ıve implementation of SoftmaxLoss.evaluate() might involve many for-loops, which is ﬁne as long as the function and gradient calculations are correct. However, this method might take a very long time! This speed bottleneck is one of Python’s shortcomings, which can be addressed by employing pre-computing and lots of vectorized operations. However, it can be diﬃcult to convert your written solutions of f and g into vectorized forms, so you should prioritize getting the implementation to work correctly ﬁrst. One reasonable path is to ﬁrst make a correct function and gradient implementation with lots of loops, then (if you want) pulling bits out of the loops into meaningful variables, and then thinking about how you can compute each of the variables in a vectorized way. Our solution code doesn’t contain any loops, but the solution code for previous instances of the course actually did; it’s totally okay for this course to not be allergic to Python for loops the way Danica is. 5 5Reading the old solution with loops probably isn’t why I was sick the last week. . . . 6 3.5 Comparison with scikit-learn [2 points] Compare your results (training error and validation error for both one-vs-all and softmax) with scikit-learn’s LogisticRegression, which can also handle multi-class problems. For one-vs-all, set multi_class='ovr'; for softmax, set multi_class='multinomial'. Since your comparison code above isn’t using regularization, set penalty='none'. Again, set fit_intercept to False for the same reason as above (there is already a column of 1’s added to the data set). 3.6 Cost of Multi-Class Logistic Regression [4 points] Assume that we have • n training examples. • d features. • k classes. • t testing examples. • T iterations of gradient descent for training. Also assume that we take X and form new features Z using Gaussian RBFs as a non-linear feature trans- formation. 1. In O() notation, what is the cost of training the softmax classiﬁer with gradient descent? 2. What is the cost of classifying the t test examples? Hint: you’ll need to take into account the cost of forming the basis at training (Z) and test ( ˜Z) time. It will be helpful to think of the dimensions of all the various matrices. 7 4 Very-Short Answer Questions [18 points] Answer each of the following questions in a sentence or two. 1. Suppose that a client wants you to identify the set of “relevant” factors that help prediction. Should you promise them that you can do this? 2. What is a setting where you would use the L1-loss, and what is a setting where you would use L1- regularization? 3. Among L0-regularization, L1-regularization, and L2-regularization: which yield convex objectives? Which yield unique solutions? Which yield sparse solutions? 4. What is the eﬀect of λ in L1-regularization on the sparsity level of the solution? What is the eﬀect of λ on the two parts of the fundamental trade-oﬀ? 5. Suppose you have a feature selection method that tends not to generate false positives, but has many false negatives (it misses relevant variables). Describe an ensemble method for feature selection that could improve the performance of this method. 6. Suppose a binary classiﬁcation dataset has 3 features. If this dataset is “linearly separable”, what does this precisely mean in three-dimensional space? 7. When searching for a good w for a linear classiﬁer, why do we use the logistic loss instead of just minimizing the number of classiﬁcation errors? 8. What is a disadvantage of using the perceptron algorithm to ﬁt a linear classiﬁer? 9. How does the hyper-parameter σ aﬀect the shape of the Gaussian RBFs bumps? How does it aﬀect the fundamental tradeoﬀ? 8","libVersion":"0.2.1","langs":""}