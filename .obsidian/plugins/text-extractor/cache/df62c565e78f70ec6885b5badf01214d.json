{"path":".obsidian/plugins/text-extractor/cache/df62c565e78f70ec6885b5badf01214d.json","text":"Solution of practice questions, CPSC 302, 2023 1. Machine rounding unit is a bit larger than 10−16, and therefore for n larger than or equal to 1016 the number 1+ 1 n is evaluated to 1.0 on the ﬂoating point system . At that point, for the number 1.0, no matter what power is taken - the ﬁnal result is 1.0. For smaller numbers there is no such issue and therefore a much more accurate result is obtained. As long as we do not hit the dangerous zone of roundoﬀ errors, the larger n is the more accurate we would expect the result to be. 2. (a) We solve f (x) = 2 x − n = 0. Newton’s scheme reads xk+1 = xk − 2xk − n 2xk ln(2) . Simplifying, we get xk+1 = xk − 1 0.6931... + n 0.6931... · 2xk . (b) We have f ′(x) = 2 x ln(2) ̸= 0 for all x including x∗, and since we may assume that the ini- tial guess is suﬃciently close to the solution, we have quadrartic convergence. (c) In this case evaluating the derivative is almost identical to the evaluation of the function, and secant will also involve computa- tion of 2xk as the main cost, and at the same time its convergence speed is lower (superlinear), so there is no point to consider it. 3. Guidance: ﬁrst show explicitly that H T H = (I − 2uu T )T (I − 2uu T ) = I, which means H is orthogonal, and then since the eigenvalues of H T H are all 1s because it is equal to the identity matrix, it means that all singular values of H are equal to 1, and therefore its condition number is equal to 1 1 = 1. 4. (a) Since U ΣV T x = b, we have that x = V Σ−1U T b. Notice that in this case, since A is square all matrices involved are square and Σ is square as well. Since A is nonsingular and U and V are orthogonal, we are guaranteed that Σ is nonsingular. (b) The inversion of Σ requires O(n) operations, whereas the matrix- vector products with U and V T require O(n2) operations each. Altogether, then, it takes O(n2) operations if the SVD is already given. 5. (a) Advantage: more numerically stable and can deal better with ill- conditioned matrices. Disadvantage: requires approximately twice more ﬂoating point operations when the number of rows is much larger than the number of columns. (b) Advantage: signiﬁcantly cheaper single iterations because this method is based on matrix-vector products whereas inverse power requires a decomposition plus backward and forward solves. Disadvantage: may be signiﬁcantly slower to converge, especially if we have an eﬀective shift for the inverse power method. 6. (a) If Ax = λx deﬁnes an eigenpair of A, then (A + αI)x = (λ + α)x is a corresponding eigenpair of A + αI. Since all eigenvalues of A are positive, so are the eigenvalues of A + αI. For the latter, the largest eigenvalue is λmax(A) + α and the smallest eigenvalue is λmin(A) + α. The condition number is κ2(A + αI) = λmax(A) + α λmin(A) + α . The inequality κ2(A + αI) ≤ κ2(A) can be veriﬁed easily and we have already shown that in Assignment 4 question 3. (b) If all the diagonal elements of A are equal to 1, then all the diagonal elements of A + αI are equal to 1 + α. We have that the diagonal of A + αI is thus D = (1 + α)I. Therefore, the Jacobi iteration matrix is T = I − 1 1 + α (A + αI). The eigenvalues of T are µi = 1 − λi + α 1 + α . We want ρ(T ) < 1. Therefore, we require −1 < 1 − λi + α 1 + α < 1. The inequality on the right is always satisﬁed because the eigen- values λi and α are positive. As for the inequality on the left, we have λi + α 1 + α < 2, from which it follows that α > λi − 2 > 0. We want this to be true for all eigenvalues λi, so it is enough to require that α > λmax(A) − 2 > 0.","libVersion":"0.2.1","langs":""}