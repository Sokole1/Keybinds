{"path":".obsidian/plugins/text-extractor/cache/44fd8e4b4cfefc0dfadc0ab20880f365.json","text":"CPSC 302, Fall 2023, Assignment 1 Solutions Released September 25, 2023 1. In the lecture we considered the approximation of the ﬁrst derivative of a function f (x): f ′(x) ≈ f (x + h) − f (x) h , and we showed that the truncation (or discretization) error for the approximation formula on the right-hand side is ﬁrst-order accurate, namely the error is O(h). (a) Matlab question: Download the script that generates Figure 1.3 in the textbook; it is provided on the assignment page on Canvas. i. The script does not contain any loops. Explain what Matlab features are used to accomplish this. Answer: The variable i contains a vector of values by using separating colons. For h and err element-wise operations are used. In general, commands that include a dot before an arithmetic operation are used element-wise operations on arrays. ii. State what the variables i, h, and err will contain by the end of running the script. You may run the script to ﬁgure this out, but please do not output the values of those variables; just state brieﬂy what those variables are intended to compute, using words and formulas. Answer: The vector i contains values from −20 to 0 in jumps of 0.5. The variable h contains powers of the elements of the vector i. The vector err contains the absolute error of the approximation for all values of h at once. (b) Use Taylor expansions to derive a second-order approximation of the derivative f ′(x0) using the values f (x0), f (x0 + h), and f (x0 + 2h), i.e., with a discretization error of O(h2). Show all your steps. Answer: We write f (x0 + h) = f (x0) + hf ′(x0) + h2 2 f ′′(x0) + h3 6 f ′′′(x0) + O(h4); f (x0 + 2h) = f (x0) + 2hf ′(x0) + (2h)2 2 f ′′(x0) + (2h)3 6 f ′′′(x0) + O(h4). We will eventually want to get an approximation of f ′(x0), so in order to minimize the error we multiply the ﬁrst equation by 4 and subtract the second equation from it. This will eliminate the h2-term associated with f ′′(x0): 4f (x0 + h) − f (x0 + 2h) = 3f (x0) + 2hf ′(x0) − 2h3 3 f ′′′(x0) + O(h 4). 1 Dividing by 2h and rearranging gives us the desired result: f ′(x0) = −3f (x0) + 4f (x0 + h) − f (x0 + 2h) 2h + h2 3 f ′′′(x0) + O(h3). We therefore obtain the approximation f ′(x0) ≈ −3f (x0) + 4f (x0 + h) − f (x0 + 2h) 2h , with an O(h2) discretization error, referring to the term associated to f ′′′. Note that we could have also written the original equations a little diﬀerently with a ﬁnite error term f (x0 + h) = f (x0) + hf ′(x0) + h2 2 f ′′(x0) + h3 6 f ′′′(ξ1); f (x0 + 2h) = f (x0) + 2hf ′(x0) + (2h)2 2 f ′′(x0) + (2h)3 6 f ′′′(ξ2), with x0 ≤ ξ1 ≤ x0 + h and x0 ≤ ξ2 ≤ x0 + 2h, and obtain the second-order discretization error. (c) Generate a graph similar to Figure 1.3 in the textbook for the same function and the same value of x0, and compare the two graphs. To generate your code for this question, edit the provided script (see (a)). Answer: We change the script in two places: the formula for err and the formula for d err. The latter is the discretization error, and we can approximate it by − h2 3 f ′′′(x0) = h2 6 cos(x0) (in the same spirit as done in the example in the book but with a diﬀerent discretization error here). x0 = 1 . 2 ; f 0 = s i n ( x0 ) ; f p = c o s ( x0 ) ; i = = 2 0 : 0 . 5 : 0 ; h = 1 0 . ˆ i ; e r r = abs ( f p = (= 3* f 0 +4* s i n ( x0+h)= s i n ( x0+2* h ) ) . / ( 2 * h ) ) ; d e r r = f p /3* h . ˆ 2 ; l o g l o g ( h , e r r , ’=* ’ ) ; h o l d on l o g l o g ( h , d e r r , ’ r = . ’ ) ; x l a b e l ( ’ h ’ ) y l a b e l ( ’ A b s o l u t e e r r o r ’ ) 10 -20 10 -15 10 -10 10 -5 10 0 h 10 -50 10 -40 10 -30 10 -20 10 -10 10 0Absolute error 2 (d) Explain the meaning of your results in comparison with Example 1.2 in the textbook. In particular, make sure to address the following points, and add any other relevant observations: i. Explain the reason for the value of the error going down as h decreases until a certain point and then going up again. Answer: As long as h is large enough, the discretization errors dominate and they coincide with the expected error based on the Taylor expansion and plotted by the red broken line. ii. Explain why the graph is ﬂat at certain points. Answer: For extremely small values below machine rounding unit it is impossi- ble for the computer to distinguish between f (x0), f (x0 + h), and f (x0 + 2h), and therefore the value computed is zero and the error is simply the derivative value cos(1.2) = 0.362..., which is very bad (i.e., an unacceptably large error). iii. Explain why the value of the minimum of the error is obtained at a diﬀerent location than the minimum for the ﬁrst-order approximation discussed in the lecture. Answer: If you answer Question 1.31(a) in the textbook you will get a good insight on what is going on here. For answering this question it was enough here to say that the discretization error goes down like h2 here rather than h, and therefore we will get more quickly to the range (of h) where roundoﬀ errors start dominating, and near h = 10−5. Note, however, that at that point the combined error is in the order of 10−10, which is in fact pretty good (i.e., the approximation of the derivative is quite accurate, with approximately ten correct decimal digits). 2. Consider the function f (x) = 1 − cos(x) sin(x) , x ≈ 0. (a) Using Taylor expansions or other means, state what you expect the value of f (x) to be approximately equal to if it were to be approximated by a linear polynomial in x, namely p(x) = a0 + a1x. That is, determine what a0 and a1 for this approximation to be valid for a small (in magnitude) value of x. Answer: There are several ways to tackle this. We can, for example, do this: 1 − cos(x) sin(x) = 1 − (1 − x2 2 + o(x2) ) x + o(x) = x 2 + o(x). If you just developed the Taylor expansion of the entire function at once, that was absolutely ﬁne too. In any case, we get a0 = 0 and a1 = 1 2 . (b) Write a short Matlab script that computes f (x) for x = 0.01, x = 0.001 and x = 0.0001, in single precision and in double precision. The default of Matlab is double precision, and for single precision use the Matlab command single. You should compute six values in total (three values for each of the two precisions). Explain the diﬀerence in the quality of the results between the two precisions, based on analyzing the error and the rounding units of the corresponding ﬂoating point systems. Answer: Here is a simple script that does this: x1 = 0 . 0 1 ; x1s=s i n g l e ( x1 ) ; x2 = 0 . 0 0 1 ; x2s=s i n g l e ( x2 ) ; x3 = 0 . 0 0 0 1 ; x3s=s i n g l e ( x3 ) ; 3 f p r i n t f ( ’ x1 = d o u b l e : %f \\n ’ ,((1 = c o s ( x1 ) ) / s i n ( x1 ) ) ) f p r i n t f ( ’ x1 = s i n g l e : %f \\n ’ ,((1 = c o s ( x1s ) ) / s i n ( x1s ) ) ) f p r i n t f ( ’ x2 = d o u b l e : %f \\n ’ ,((1 = c o s ( x2 ) ) / s i n ( x2 ) ) ) f p r i n t f ( ’ x2 = s i n g l e : %f \\n ’ ,((1 = c o s ( x2s ) ) / s i n ( x2s ) ) ) f p r i n t f ( ’ x3 = d o u b l e : %f \\n ’ ,((1 = c o s ( x3 ) ) / s i n ( x3 ) ) ) f p r i n t f ( ’ x3 = s i n g l e : %f \\n ’ ,((1 = c o s ( x3s ) ) / s i n ( x3s ) ) ) (We note that this script could be written much more nicely and economically with the use of functions, but we go for the simplest implementation.) We run this script and obtain: x1 - double: 0.005000 x1 - single: 0.005001 x2 - double: 0.000500 x2 - single: 0.000477 x3 - double: 0.000050 x3 - single: 0.000000 For x = 0.01 both computations are ﬁne, but for x = 0.01 we are starting to lose accuracy in single precision, and for x = 0.001 it is a complete disaster for single precision and we get 0, which is completely wrong. The computations for double precision are accurate. The problem is mainly cancellation error: subtraction of two very close numbers. For x = 0.001 we have that cos(x) = 0.999999500000042, and for x = 0.0001 we have that cos(x) = 0.999999955, i.e., very close to 1 in both cases. In particular, for x = 0.0001 we have 1 − cos(x) ≈ 5 · 10−8. The error is thus close to machine rounding unit in the single precision system and the error is magniﬁed by the division by a very small number, namely sin(x). In single precision we have only 32 bits, and 24 are reserved for the signiﬁcand (23 are explicitly stored). This means that we have only approximately eight correct decimal digits for our computations (rounding unit is larger than 10−8 and smaller than 10−7). Therefore, in this computation, we have at most one correct decimal digit in single precision, and the computed result is inaccurate. For double precision, which has 64 bits, with 53 bits in the signiﬁcand (52 stored explicitly), the result of the subtraction is not close to the rounding unit and therefore the result is accurate. The Taylor expansion and the linear polynomial approximation we obtained gives us a bit of a sanity check here. It is expected to be fairly accurate for small values of x, because the error is quadratic in x and the derivatives are bounded. Here we do not have a harmful subtraction of two close numbers to worry about. (c) Suggest a formula that is more suitable for numerical computation of the above quanti- ties, and explain why this formula is expected to be better. Repeat your calculations of the same six quantities with this formula to show that your new formula indeed produces either the same quality or more accurate results, depending on the value of x and the ﬂoating point system used. Answer: As implied from (a), a Taylor expansion for very small values of x is legitimate. Alternatively, and better still, just multiply and divide by the conjugate. The formula f (x) = sin(x) 1 + cos(x) should be much more accurate, because it gets rid of the cancellation error. We run the following script: 4 x1 = 0 . 0 1 ; x1s=s i n g l e ( x1 ) ; x2 = 0 . 0 0 1 ; x2s=s i n g l e ( x2 ) ; x3 = 0 . 0 0 0 1 ; x3s=s i n g l e ( x3 ) ; f p r i n t f ( ’ x1 = d o u b l e : %f \\n ’ , s i n ( x1 )/(1+ c o s ( x1 ) ) ) f p r i n t f ( ’ x1 = s i n g l e : %f \\n ’ , s i n ( x1s )/(1+ c o s ( x1s ) ) ) f p r i n t f ( ’ x2 = d o u b l e : %f \\n ’ , s i n ( x2 )/(1+ c o s ( x2 ) ) ) f p r i n t f ( ’ x2 = s i n g l e : %f \\n ’ , s i n ( x2s )/(1+ c o s ( x2s ) ) ) f p r i n t f ( ’ x3 = d o u b l e : %f \\n ’ , s i n ( x3 )/(1+ c o s ( x3 ) ) ) f p r i n t f ( ’ x3 = s i n g l e : %f \\n ’ , s i n ( x3s )/(1+ c o s ( x3s ) ) ) We obtain perfect results in both precisions: x1 - double: 0.005000 x1 - single: 0.005000 x2 - double: 0.000500 x2 - single: 0.000500 x3 - double: 0.000050 x3 - single: 0.000050 3. For the following well-known inﬁnite series it is known that s = ∞∑ j=1 1 j2 = π2 6 = 1.6449340668... Write a short Matlab script that computes the ﬁnite sum of the ﬁrst one billion (109) terms of this series in single precision, once in increasing order from the smallest to largest number in the (ﬁnite) series, and once in decreasing order, from largest to smallest number. Include your Matlab code. s=p i ˆ 2 / 6 ; % a c t u a l i n f i n i t e sum % i n c r e a s i n g o r d e r from s m a l l e s t t o l a r g e s t e l e m e n t o f t h e s e r i e s s i n c=s i n g l e ( 0 ) ; f o r j=s i n g l e ( 1 e9 ) : = 1 : s i n g l e ( 1 ) , s i n c=s i n c +1/( j ˆ 2 ) ; end % d e s c r e a s i n g o r d e r from l a r g e s t t o s m a l l e s t e l e m e n t o f t h e s e r i e s s d e c=s i n g l e ( 0 ) ; f o r j=s i n g l e ( 1 ) : 4 0 9 6 , s d e c=s d e c +1/( j ˆ 2 ) ; end f p r i n t f ( ’ a c t u a l i n f i n i t e sum : %f \\n ’ , s ) f p r i n t f ( ’ sum = d e c r e a s i n g o r d e r : %f \\n ’ , s d e c ) f p r i n t f ( ’ sum = i n c r e a s i n g o r d e r : %f \\n ’ , s i n c ) f p r i n t f ( ’ a b s o l u t e e r r o r = d e c r e a s i n g o r d e r : %e \\n ’ , abs ( s d e c = s ) ) f p r i n t f ( ’ r e l a t i v e e r r o r = d e c r e a s i n g o r d e r : %e \\n ’ , abs ( ( s d e c = s ) / s ) ) f p r i n t f ( ’ a b s o l u t e e r r o r = i n c r e a s i n g o r d e r : %e \\n ’ , abs ( s i n c = s ) ) f p r i n t f ( ’ r e l a t i v e e r r o r = i n c r e a s i n g o r d e r : %e \\n ’ , abs ( ( s i n c = s ) / s ) ) (a) Print out the values you obtain for the approximation and the absolute and relative errors for each of the two calculations. Answer: We run the above script and get actual infinite sum: 1.644934 sum - decreasing order: 1.644725 sum - increasing order: 1.644934 absolute error - decreasing order: 2.087441e-04 relative error - decreasing order: 1.269012e-04 absolute error - increasing order: 8.658835e-09 relative error - increasing order: 5.263940e-09 5 (b) Which of the two formulas gives you a more accurate result? Explain why. Answer: The formula with the increasing order is much more accurate. The formula with the decreasing order is only accurate to about four decimal digits. What is shocking about this (and you were not expected to think about that) is that this accuracy is already attained after j = 4096 terms, because the size of the mantissa for single precision (24 bits), and if we reach s ≈ 1.6, when we add to s the number 1/j2 we add 1/(40962) = 2−24 in this ﬂoating point system and stay with s. When we sum the small numbers ﬁrst, we are able to accumulate them and account for the least-signiﬁcant digits, and by the time we get to the larger numbers, we already have a meaningful nonzero sum. But when we start with largest numbers, there is no room for the least-signiﬁcant bits of the smallest numbers. 4. Consider the ﬂoating point system given by (β, t, L, U ) = (10, 3, −9, 10). (a) What is η, the rounding unit? Answer: η = 1 2 β1−t = 1 2 · 101−3 = 0.005. (b) What is the smallest positive number in this system? Answer: The smallest positive number is βL = 10−9. (c) What is the smallest positive number larger than 1? Answer: 1.01. (d) What is the largest number in this system? Answer: The largest psotive number is 9.99 · 1010 ≈ 1011. (e) How many distinct normalized positive numbers can be represented in this ﬂoating point system? Answer: There are 900 · 20 = 18, 000 distinct normalized numbers in this system. 6","libVersion":"0.2.1","langs":""}