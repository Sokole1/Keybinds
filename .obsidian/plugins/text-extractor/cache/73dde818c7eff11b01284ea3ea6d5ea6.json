{"path":".obsidian/plugins/text-extractor/cache/73dde818c7eff11b01284ea3ea6d5ea6.json","text":"CPSC 340: Machine Learning and Data Mining Non-Parametric Models Admin â€¢ Welcome to the course! â€“ If you have remaining forms, bring them to me after class and good luck. â€¢ Assignment 1: â€“ 1 late day to hand in tonight, 2 for Friday. â€¢ Assignment 2 is out. â€“ Due Friday of next week. It is long so start early. Last Time: E-mail Spam Filtering â€¢ Want a build a system that filters spam e-mails: â€¢ We formulated as supervised learning: â€“ (yi = 1) if e-mail â€˜iâ€™ is spam, (yi = 0) if e-mail is not spam. â€“ (xij = 1) if word/phrase â€˜jâ€™ is in e-mail â€˜iâ€™, (xij = 0) if it is not. $ Hi CPSC 340 Vicodin Offer â€¦ 1 1 0 0 1 0 â€¦ 0 0 0 0 1 1 â€¦ 0 1 1 1 0 0 â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ Spam? 1 1 0 â€¦ Last Time: NaÃ¯ve Bayes â€¢ We considered spam filtering methods based on Bayes rule: â€¢ NaÃ¯ve Bayes uses practical conditional independence assumption: â€¢ Predict â€œspamâ€ if p(yi = â€œspamâ€ | xi1, xi2,â€¦, xid) > p(yi = â€œnot spamâ€ | xi1, xi2,â€¦, xid). â€“ We do not need p(xi1, xi2,â€¦, xid) to test this. NaÃ¯ve Bayes â€¢ p(â€œvicodinâ€ = 1 | â€œspamâ€ = 1) is probability of seeing â€œvicodinâ€ in spam. ALL POSSIBLE E-MAILS (including duplicates)SPAM NOT SPAM â€¢ Easy to estimate: Vicodin Again, this is a â€œmaximum likelihood estimateâ€ (MLE). We will cover how to derive this later. NaÃ¯ve Bayes â€¢ Comparing ğ‘ ğ‘¥ğ‘–ğ‘— = 1 ğ‘¦ğ‘– = ğ‘) for c=â€œspamâ€ and c=â€œnot spamâ€: â€¢ Even though independence is not true, these values may be enough to distinguish the classes. NaÃ¯ve Bayes â€¢ NaÃ¯ve Bayes formally: â€¢ Post-lecture slides: how to train/test by hand on a simple example. Laplace Smoothing â€¢ Our estimate of p(â€˜lactaseâ€™ = 1| yi = â€˜spamâ€™) is: â€“ But there is a problem if you have no spam messages with lactase: â€¢ p(â€˜lactaseâ€™ | â€˜spamâ€™) = 0, so spam messages with lactase automatically get through. â€“ Common fix is Laplace smoothing estimate: â€¢ Add 1 to numerator, and add 2 to denominator (for binary features). â€“ Acts like a â€œfakeâ€ spam example that has lactase, and a â€œfakeâ€ spam example that doesnâ€™t. Laplace Smoothing â€¢ Laplace smoothing: â€“ Typically you do this for all features. â€¢ Helps against overfitting by biasing towards the uniform distribution. â€¢ A common variation is to use a real number Î² rather than 1. â€“ Add â€˜Î²kâ€™ to denominator if feature has â€˜kâ€™ possible values (so it sums to 1). This is a â€œmaximum a posterioriâ€ (MAP) estimate of the probabiliy. Weâ€™ll discuss MAP and how to derive this formula later. Decision Theory â€¢ Are we equally concerned about â€œspamâ€ vs. â€œnot spamâ€? â€¢ True positives, false positives, false negatives, true negatives: â€¢ The costs of mistakes might be different: â€“ Letting a spam message through (false negative) is not a big deal. â€“ Filtering a not spam (false positive) message will make users mad. Predict / True True â€˜spamâ€™ True â€˜not spamâ€™ Predict â€˜spamâ€™ True Positive False Positive Predict â€˜not spamâ€™ False Negative True Negative Decision Theory â€¢ We can give a cost to each scenario, such as: â€¢ Instead of most probable label, take à·œğ‘¦i minimizing expected cost: â€¢ Even if â€œspamâ€ has a higher probability, predicting â€œspamâ€ might have a expected higher cost. Predict / True True â€˜spamâ€™ True â€˜not spamâ€™ Predict â€˜spamâ€™ 0 100 Predict â€˜not spamâ€™ 10 0 Decision Theory Example â€¢ Consider a test example we have p( à·¤ğ‘¦i = â€œspamâ€ | à·¤ğ‘¥i) = 0.6, then: â€¢ Even though â€œspamâ€ is more likely, we should predict â€œnot spamâ€. â€“ With above costs, only classify as â€œspamâ€ if p( à·¤ğ‘¦i = â€œspamâ€ | à·¤ğ‘¥i) â‰¥ 0.91. Predict / True True â€˜spamâ€™ True â€˜not spamâ€™ Predict â€˜spamâ€™ 0 100 Predict â€˜not spamâ€™ 10 0 Decision Theory Discussion â€¢ In other applications, the costs could be different. â€“ In cancer screening, maybe false positives are ok, but donâ€™t want to have false negatives. â€¢ Decision theory and â€œdartsâ€: â€“ http://www.datagenetics.com/blog/january12012/index.html â€¢ Decision theory and video poker: â€“ http://datagenetics.com/blog/july32019/index.html Decision Theory and Basketball â€¢ â€œHow Mapping Shots In The NBA Changed It Foreverâ€ https://fivethirtyeight.com/features/how-mapping-shots-in-the-nba-changed-it-forever/ Unbalanced Class Labels â€¢ A related idea is that of â€œunbalancedâ€ class labels. â€“ If 99% of the e-mails are spam, you can get 99% accuracy by always predicting spam. â€¢ There are a variety of other performance measures available: â€“ Weighted classification error. â€“ Jaccard similarity. â€“ Precision and recall. â€“ False positive and false negative rate. â€“ ROC curves. â€¢ See the post-lecture bonus slides for additional details. Decision Theory and â€œDebugging by TAâ€ â€¢ Here is one way to write a complicated program: 1. Write the entire function at once. 2. Try it out to â€œsee if it worksâ€. 3. Spend hours fiddling with commands, to find magic working combination. 4. Send code to the TA, asking â€œwhat is wrong?â€ â€¢ Decision theory: â€“ If you are lucky, Step 2 works and you are done! â€“ If you are not lucky, takes way longer than principled coding methods. â€“ E[time(â€œsee if it worksâ€)] â‰¥ E[time(â€œcarefully implementâ€)). â€¢ The above is also a great way to introduce bugs into your code. â€¢ So E[bugs(â€œsee if it worksâ€)] â‰¥ E[bugs(â€œcarefully implementationâ€)). â€¢ And you will not be able to do Step 4 when you graduate. Digression: Debugging 101 â€¢ What strategies could we use to debug an ML implementation? â€“ Use â€œprintâ€ statements to see what is happening at each step of the code. â€¢ Or use a debugger. â€“ Develop one or more simple â€œtest casesâ€, were you worked out the result by hand. â€¢ Maybe one of the functions you are using does not work the way you think it does. â€“ Check if the â€œpredictâ€ functionality works correctly on its own. â€¢ Maybe the training works but the prediction does not. â€“ Check if the â€œtrainingâ€ functionality works correctly on its own. â€¢ Maybe the prediction works but the training does not. â€“ Try the implementation with only one training example or only one feature. â€¢ Maybe there is an indexing problem, or things are not being aggregated properly. â€“ Try the implementation with only two features so you can visualize the decision surface. â€¢ May be able to see obvious problems. â€“ Make a â€œbrute forceâ€ implementation to compare to your â€œfast/cleverâ€ implementation. â€¢ Maybe you made a mistake when trying to be fast/clever. â€¢ With these strategies, you should be able to diagnose locations of problems. Next Topic: Non-Parametric Models Decision Trees vs. NaÃ¯ve Bayes â€¢ Decision trees: 1. Sequence of rules based on 1 feature. 2. Training: 1 pass over data per depth. 3. Greedy splitting as approximation. 4. Testing: just look at features in rules. 5. New data: might need to change tree. 6. Accuracy: good if simple rules based on individual features work (â€œsymptomsâ€). 7. Interpretability: easy to see how decisions are made. â€¢ NaÃ¯ve Bayes: 1. Simultaneously combine all features. 2. Training: 1 pass over data to count. 3. Conditional independence assumption. 4. Testing: look at all features. 5. New data: just update counts. 6. Accuracy: good if features almost independent given label (bag of words). 7. Interpretability: can see how each feature influences decision. Geometric Motivation for K-Nearest Neigbours â€¢ Do you think the green example should be orange or blue? test example Feature space Geometric Motivation for K-Nearest Neigbours â€¢ Do you think the green example should be orange or blue? â€“ In the feature space, it is close to examples labeled orange (â€œneighboursâ€). test example Feature space K-Nearest Neighbours (KNN) â€¢ An old/simple classifier: k-nearest neighbours (KNN). â€¢ To classify an example à·¤ğ‘¥i: 1. Find the â€˜kâ€™ training examples xi that are â€œnearestâ€ to à·¤ğ‘¥i. 2. Classify using the most common label of â€œnearestâ€ training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 â€¦ â€¦ Label O + + O + â€¦ K-Nearest Neighbours (KNN) â€¢ An old/simple classifier: k-nearest neighbours (KNN). â€¢ To classify an example à·¤ğ‘¥i: 1. Find the â€˜kâ€™ training examples xi that are â€œnearestâ€ to à·¤ğ‘¥i. 2. Classify using the most common label of â€œnearestâ€ training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 â€¦ â€¦ Label O + + O + â€¦ K-Nearest Neighbours (KNN) â€¢ An old/simple classifier: k-nearest neighbours (KNN). â€¢ To classify an example à·¤ğ‘¥i: 1. Find the â€˜kâ€™ training examples xi that are â€œnearestâ€ to à·¤ğ‘¥i. 2. Classify using the most common label of â€œnearestâ€ training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 â€¦ â€¦ Label O + + O + â€¦ K-Nearest Neighbours (KNN) â€¢ An old/simple classifier: k-nearest neighbours (KNN). â€¢ To classify an example à·¤ğ‘¥i: 1. Find the â€˜kâ€™ training examples xi that are â€œnearestâ€ to à·¤ğ‘¥i. 2. Classify using the most common label of â€œnearestâ€ training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 â€¦ â€¦ Label O + + O + â€¦ K-Nearest Neighbours (KNN) â€¢ An old/simple classifier: k-nearest neighbours (KNN). â€¢ To classify an example à·¤ğ‘¥i: 1. Find the â€˜kâ€™ training examples xi that are â€œnearestâ€ to à·¤ğ‘¥i. 2. Classify using the most common label of â€œnearestâ€ training examples. Egg Milk Fish 0 0.7 0 0.4 0.6 0 0 0 0 0.3 0.5 1.2 0.4 0 1.2 Sick? 1 1 0 1 1 Egg Milk Fish 0.3 0.6 0.8 Sick? ? K-Nearest Neighbours (KNN) â€¢ Assumption: â€“ Examples with similar features are likely to have similar labels. â€¢ Seems strong, but all good classifiers basically rely on this assumption. â€“ If not true there may be nothing to learn and you are in â€œno free lunchâ€ territory. â€“ Methods just differ in how you define â€œsimilarityâ€. â€¢ Most common distance function is Euclidean distance: â€“ xi is features of training example â€˜iâ€™, and à·¤ğ‘¥ Çğ‘– is features of test example â€˜ Çğ‘–â€™. â€“ Costs O(d) to calculate for a pair of examples. Effect of â€˜kâ€™ in KNN. â€¢ With large â€˜kâ€™ (hyper-parameter), KNN model will be very simple. â€“ With k=n, you just predict the mode of the labels. â€“ Model gets more complicated as â€˜kâ€™ decreases. â€“ With k=1 it is very sensitive to data (can fit data better but can overfit better too). â€¢ Effect of â€˜kâ€™ on fundamental trade-off: â€“ As â€˜kâ€™ grows, training error tends to increase. â€“ As â€˜kâ€™ grows, generalization gap tends to decrease. KNN Implementation â€¢ There is no training phase in KNN (â€œlazyâ€ learning). â€“ You just store the training data. â€“ Costs O(1) if you use a pointer. â€¢ But predictions are expensive: O(nd) to classify 1 test example. â€“ Need to do an O(d) distance calculation for all â€˜nâ€™ training examples. â€“ So prediction time grows with number of training examples. â€¢ Tons of work on reducing this cost (for example, â€œcondensed nearest neighborâ€). â€¢ And storage is expensive: needs O(nd) memory to store â€˜Xâ€™ and â€˜yâ€™. â€“ So memory grows with number of training examples. â€“ When storage depends on â€˜nâ€™, we call it a non-parametric model. Parametric vs. Non-Parametric â€¢ Parametric models: â€“ Have fixed number of parameters: trained â€œmodelâ€ size is O(1) in terms â€˜nâ€™. â€¢ E.g., naÃ¯ve Bayes just stores counts. â€¢ E.g., fixed-depth decision tree just stores rules for that depth. â€“ You can estimate the fixed parameters more accurately with more data. â€“ But eventually more data does not help: model is too simple. â€¢ Non-parametric models: â€“ Number of parameters grows with â€˜nâ€™: size of â€œmodelâ€ depends on â€˜nâ€™. â€“ Model gets more complicated as you get more data. â€¢ E.g., KNN stores all the training data, so size of â€œmodelâ€ is O(nd). â€¢ E.g., decision tree whose depth depends on number of training examples. Parametric vs. Non-Parametric Models â€¢ Parametric models have bounded memory. â€¢ Non-parametric models can have unbounded memory. Effect of â€˜nâ€™ in KNN. â€¢ With a small â€˜nâ€™, KNN model will be very simple. â€¢ Model gets more complicated as â€˜nâ€™ increases. â€“ Requires more memory, but detects subtle differences between examples. Consistency of KNN (â€˜nâ€™ going to â€˜âˆâ€™) â€¢ KNN has appealing consistency properties: â€“ As â€˜nâ€™ goes to âˆ, KNN test error is less than twice best possible error. â€¢ For fixed â€˜kâ€™ and binary labels (under mild assumptions). â€¢ Stoneâ€™s Theorem: KNN is â€œuniversally consistentâ€. â€“ If k/n goes to zero and â€˜kâ€™ goes to âˆ, converges to the best possible error. â€¢ For example, k = log(n). â€¢ First algorithm shown to have this property. â€¢ Does Stoneâ€™s Theorem violate the no free lunch theorem? â€“ No: it requires a continuity assumption on the labels. â€“ Consistency says nothing about finite â€˜nâ€™ (see \"Dont Trust Asymptoticsâ€). â€¢ The â€œspeedâ€ at which universal consistency happens is exponential in the dimension â€˜dâ€™. Parametric vs. Non-Parametric Models â€¢ With parametric models, there is an accuracy limit. â€“ Even with infinite â€˜nâ€™, may not be able to achieve optimal error (Ebest). Parametric vs. Non-Parametric Models â€¢ With parametric models, there is an accuracy limit. â€“ Even with infinite â€˜nâ€™, may not be able to achieve optimal error (Ebest). â€¢ Many non-parametric models (like KNN) converge to optimal error. â€“ Though may also converge to needing infinite memory. Summary â€¢ Decision theory allows us to consider costs of predictions. â€¢ Debugging 101: ideas to find bugs and write code with fewer bugs. â€¢ K-Nearest Neighbours: use most common label of nearest examples. â€¢ Often works surprisingly well. â€¢ Suffers from high prediction and memory cost. â€¢ Canonical example of a â€œnon-parametricâ€ model. â€¢ Non-parametric models grow with number of training examples. â€“ Can have appealing â€œconsistencyâ€ properties. â€¢ Next Time: â€¢ Fighting the fundamental trade-off and Microsoft Kinect. NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Training Phase â€¢ Training a naÃ¯ve Bayes model: NaÃ¯ve Bayes Prediction Phase â€¢ Prediction in a naÃ¯ve Bayes model: NaÃ¯ve Bayes Prediction Phase â€¢ Prediction in a naÃ¯ve Bayes model: NaÃ¯ve Bayes Prediction Phase â€¢ Prediction in a naÃ¯ve Bayes model: NaÃ¯ve Bayes Prediction Phase â€¢ Prediction in a naÃ¯ve Bayes model: NaÃ¯ve Bayes Prediction Phase â€¢ Prediction in a naÃ¯ve Bayes model: â€œProportional toâ€ for Probabilities â€¢ When we say â€œp(y) âˆ exp(-y2)â€ for a function â€˜pâ€™, we mean: â€¢ However, if â€˜pâ€™ is a probability then it must sum to 1. â€“ If ğ‘¦ âˆˆ 1,2,3,4 then â€¢ Using this fact, we can find Î²: Probability of Paying Back a Loan and Ethics â€¢ Article discussing predicting â€œwhether someone will pay back a loanâ€: â€“ https://www.thecut.com/2017/05/what-the-words-you-use-in-a-loan- application-reveal.html â€¢ Words that increase probability of paying back the most: â€“ debt-free, lower interest rate, after-tax, minimum payment, graduate. â€¢ Words that decrease probability of paying back the most: â€“ God, promise, will pay, thank you, hospital. â€¢ Article also discusses an important issue: are all these features ethical? â€“ Should you deny a loan because of religion or a family member in the hospital? â€“ ICBC is limited in the features it is allowed to use for prediction. Avoiding Underflow â€¢ During the prediction, the probability can underflow: â€¢ Standard fix is to (equivalently) maximize the logarithm of the probability: Less-NaÃ¯ve Bayes â€¢ Given features {x1,x2,x3,â€¦,xd}, naÃ¯ve Bayes approximates p(y|x) as: â€¢ The assumption is very strong, and there are â€œless naÃ¯veâ€ versions: â€“ Assume independence of all variables except up to â€˜kâ€™ largest â€˜jâ€™ where j < i. â€¢ E.g., naÃ¯ve Bayes has k=0 and with k=2 we would have: â€¢ Fewer independence assumptions so more flexible, but hard to estimate for large â€˜kâ€™. â€“ Another practical variation is â€œtree-augmentedâ€ naÃ¯ve Bayes. Computing p(xi) under naÃ¯ve Bayes â€¢ Generative models donâ€™t need p(xi) to make decisions. â€¢ However, itâ€™s easy to calculate under the naÃ¯ve Bayes assumption: Gaussian Discriminant Analysis â€¢ Classifiers based on Bayes rule are called generative classifier: â€“ They often work well when you have tons of features. â€“ But they need to know p(xi | yi), probability of features given the class. â€¢ How to â€œgenerateâ€ features, based on the class label. â€¢ To fit generative models, usually make BIG assumptions: â€“ NaÃ¯ve Bayes (NB) for discrete xi: â€¢ Assume that each variables in xi is independent of the others in xi given yi. â€“ Gaussian discriminant analysis (GDA) for continuous xi. â€¢ Assume that p(xi | yi) follows a multivariate normal distribution. â€¢ If all classes have same covariance, itâ€™s called â€œlinear discriminant analysisâ€. Other Performance Measures â€¢ Classification error might be wrong measure: â€“ Use weighted classification error if have different costs. â€“ Might want to use things like Jaccard measure: TP/(TP + FP + FN). â€¢ Often, we report precision and recall (want both to be high): â€“ Precision: â€œif I classify as spam, what is the probability it actually is spam?â€ â€¢ Precision = TP/(TP + FP). â€¢ High precision means the filtered messages are likely to really be spam. â€“ Recall: â€œif a message is spam, what is probability it is classified as spam?â€ â€¢ Recall = TP/(TP + FN) â€¢ High recall means that most spam messages are filtered. Precision-Recall Curve â€¢ Consider the rule p(yi = â€˜spamâ€™ | xi) > t, for threshold â€˜tâ€™. â€¢ Precision-recall (PR) curve plots precision vs. recall as â€˜tâ€™ varies. http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf ROC Curve â€¢ Receiver operating characteristic (ROC) curve: â€“ Plot true positive rate (recall) vs. false positive rate (FP/FP+TN). (negative examples classified as positive) â€“ Diagonal is random, perfect classifier would be in upper left. â€“ Sometimes papers report area under curve (AUC). â€¢ Reflects performance for different possible thresholds on the probability. http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf More on Unbalanced Classes â€¢ With unbalanced classes, there are many alternatives to accuracy as a measure of performance: â€“ Two common ones are the Jaccard coefficient and the F-score. â€¢ Some machine learning models donâ€™t work well with unbalanced data. Some common heuristics to improve performance are: â€“ Under-sample the majority class (only take 5% of the spam messages). â€¢ https://www.jair.org/media/953/live-953-2037-jair.pdf â€“ Re-weight the examples in the accuracy measure (multiply training error of getting non-spam messages wrong by 10). â€“ Some notes on this issue are here. More on Weirdness of High Dimensions â€¢ In high dimensions: â€“ Distances become less meaningful: â€¢ All vectors may have similar distances. â€“ Emergence of â€œhubsâ€ (even with random data): â€¢ Some datapoints are neighbours to many more points than average. â€“ Visualizing high dimensions and sphere-packing Vectorized Distance Calculation â€¢ To classify â€˜tâ€™ test examples based on KNN, cost is O(ndt). â€“ Need to compare â€˜nâ€™ training examples to â€˜tâ€™ test examples, and computing a distance between two examples costs O(d). â€¢ You can do this slightly faster using fast matrix multiplication: â€“ Let D be a matrix such that Dij contains: where â€˜iâ€™ is a training example and â€˜jâ€™ is a test example. â€“ We can compute D in Julia using: â€“ And you get an extra boost because Julia uses multiple cores. Condensed Nearest Neighbours â€¢ Disadvantage of KNN is slow prediction time (depending on â€˜nâ€™). â€¢ Condensed nearest neighbours: â€“ Identify a set of â€˜mâ€™ â€œprototypeâ€ training examples. â€“ Make predictions by using these â€œprototypesâ€ as the training data. â€¢ Reduces runtime from O(nd) down to O(md). Condensed Nearest Neighbours â€¢ Classic condensed nearest neighbours: â€“ Start with no examples among prototypes. â€“ Loop through the non-prototype examples â€˜iâ€™ in some order: â€¢ Classify xi based on the current prototypes. â€¢ If prediction is not the true yi, add it to the prototypes. â€“ Repeat the above loop until all examples are classified correctly. â€¢ Some variants first remove points from the original data, if a full-data KNN classifier classifies them incorrectly (â€œoutliersâ€™). Condensed Nearest Neighbours â€¢ Classic condensed nearest neighbours: â€¢ Recent work shows that finding optimal compression is NP-hard. â€“ An approximation algorithm algorithm was published in 2018: â€¢ â€œNear optimal sample compression for nearest neighborsâ€ https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm Refined Fundamental Trade-Off â€¢ Let Ebest be the irreducible error (lowest possible error for any model). â€“ For example, irreducible error for predicting coin flips is 0.5. â€¢ Some learning theory results use Ebest to further decompose Etest: â€¢ Egap measures how sensitive we are to training data. â€¢ Emodel measures if our model is complicated enough to fit data. â€¢ Ebest measures how low can any model make test error. â€“ Ebest does not depend on what model you choose. Consistency and Universal Consistency â€¢ A model is consistent for a particular learning problem if: â€“ Etest converges to Ebest as â€˜nâ€™ goes to infinity, for that particular problem. â€¢ A model is universally consistent for a class of learning problems if: â€“ Etest converges to Ebest as â€˜nâ€™ goes to infinity, for all problems in the class. â€¢ Class of learning problems will usually be â€œall problems satisfyingâ€: â€“ A continuity assumption on the labels yi as a function of xi. â€¢ E.g., if xi is close to xj then they are likely to receive the same label. â€“ A boundedness assumption of the set of xi. Consistency of KNN (Discrete/Deterministic Case) â€¢ Letâ€™s show universal consistency of KNN in a simplified setting. â€“ The xi and yi are binary, and yi being a deterministic function of xi. â€¢ Deterministic yi implies that Ebest is 0. â€¢ Consider KNN with k=1: â€“ After we observe an xi, KNN makes right test prediction for that vector. â€“ As â€˜nâ€™ goes to âˆ, each feature vectors with non-zero probability is observed. â€“ We have Etest = 0 once weâ€™ve seen all feature vectors with non-zero probability. â€¢ Notes: â€“ â€œNo free lunchâ€ isnâ€™t relevant as â€˜nâ€™ goes to âˆ: we eventually see everything. â€¢ But there are 2d possible feature vectors, so might need a huge number of training examples. â€“ Itâ€™s more complicated if labels arenâ€™t deterministic and features are continuous. Consistency of Non-Parametric Models â€¢ Universal consistency can be been shown for many models weâ€™ll cover: â€“ Linear models with polynomial basis. â€“ Linear models with Gaussian RBFs. â€“ Neural networks with one hidden layer and standard activations. â€¢ Sigmoid, tanh, ReLU, etc. â€¢ But itâ€™s always the non-parametric versions that are consistent: â€“ Where size of model is a function of â€˜nâ€™. â€“ Examples: â€¢ KNN needs to store all â€˜nâ€™ training examples. â€¢ Degree of polynomial must grow with â€˜nâ€™ (not true for fixed polynomial). â€¢ Number of hidden units must grow with â€˜nâ€™ (not true for fixed neural network).","libVersion":"0.2.1","langs":""}