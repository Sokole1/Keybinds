{"path":".obsidian/plugins/text-extractor/cache/73dde818c7eff11b01284ea3ea6d5ea6.json","text":"CPSC 340: Machine Learning and Data Mining Non-Parametric Models Admin • Welcome to the course! – If you have remaining forms, bring them to me after class and good luck. • Assignment 1: – 1 late day to hand in tonight, 2 for Friday. • Assignment 2 is out. – Due Friday of next week. It is long so start early. Last Time: E-mail Spam Filtering • Want a build a system that filters spam e-mails: • We formulated as supervised learning: – (yi = 1) if e-mail ‘i’ is spam, (yi = 0) if e-mail is not spam. – (xij = 1) if word/phrase ‘j’ is in e-mail ‘i’, (xij = 0) if it is not. $ Hi CPSC 340 Vicodin Offer … 1 1 0 0 1 0 … 0 0 0 0 1 1 … 0 1 1 1 0 0 … … … … … … … … Spam? 1 1 0 … Last Time: Naïve Bayes • We considered spam filtering methods based on Bayes rule: • Naïve Bayes uses practical conditional independence assumption: • Predict “spam” if p(yi = “spam” | xi1, xi2,…, xid) > p(yi = “not spam” | xi1, xi2,…, xid). – We do not need p(xi1, xi2,…, xid) to test this. Naïve Bayes • p(“vicodin” = 1 | “spam” = 1) is probability of seeing “vicodin” in spam. ALL POSSIBLE E-MAILS (including duplicates)SPAM NOT SPAM • Easy to estimate: Vicodin Again, this is a “maximum likelihood estimate” (MLE). We will cover how to derive this later. Naïve Bayes • Comparing 𝑝 𝑥𝑖𝑗 = 1 𝑦𝑖 = 𝑐) for c=“spam” and c=“not spam”: • Even though independence is not true, these values may be enough to distinguish the classes. Naïve Bayes • Naïve Bayes formally: • Post-lecture slides: how to train/test by hand on a simple example. Laplace Smoothing • Our estimate of p(‘lactase’ = 1| yi = ‘spam’) is: – But there is a problem if you have no spam messages with lactase: • p(‘lactase’ | ‘spam’) = 0, so spam messages with lactase automatically get through. – Common fix is Laplace smoothing estimate: • Add 1 to numerator, and add 2 to denominator (for binary features). – Acts like a “fake” spam example that has lactase, and a “fake” spam example that doesn’t. Laplace Smoothing • Laplace smoothing: – Typically you do this for all features. • Helps against overfitting by biasing towards the uniform distribution. • A common variation is to use a real number β rather than 1. – Add ‘βk’ to denominator if feature has ‘k’ possible values (so it sums to 1). This is a “maximum a posteriori” (MAP) estimate of the probabiliy. We’ll discuss MAP and how to derive this formula later. Decision Theory • Are we equally concerned about “spam” vs. “not spam”? • True positives, false positives, false negatives, true negatives: • The costs of mistakes might be different: – Letting a spam message through (false negative) is not a big deal. – Filtering a not spam (false positive) message will make users mad. Predict / True True ‘spam’ True ‘not spam’ Predict ‘spam’ True Positive False Positive Predict ‘not spam’ False Negative True Negative Decision Theory • We can give a cost to each scenario, such as: • Instead of most probable label, take ො𝑦i minimizing expected cost: • Even if “spam” has a higher probability, predicting “spam” might have a expected higher cost. Predict / True True ‘spam’ True ‘not spam’ Predict ‘spam’ 0 100 Predict ‘not spam’ 10 0 Decision Theory Example • Consider a test example we have p( ෤𝑦i = “spam” | ෤𝑥i) = 0.6, then: • Even though “spam” is more likely, we should predict “not spam”. – With above costs, only classify as “spam” if p( ෤𝑦i = “spam” | ෤𝑥i) ≥ 0.91. Predict / True True ‘spam’ True ‘not spam’ Predict ‘spam’ 0 100 Predict ‘not spam’ 10 0 Decision Theory Discussion • In other applications, the costs could be different. – In cancer screening, maybe false positives are ok, but don’t want to have false negatives. • Decision theory and “darts”: – http://www.datagenetics.com/blog/january12012/index.html • Decision theory and video poker: – http://datagenetics.com/blog/july32019/index.html Decision Theory and Basketball • “How Mapping Shots In The NBA Changed It Forever” https://fivethirtyeight.com/features/how-mapping-shots-in-the-nba-changed-it-forever/ Unbalanced Class Labels • A related idea is that of “unbalanced” class labels. – If 99% of the e-mails are spam, you can get 99% accuracy by always predicting spam. • There are a variety of other performance measures available: – Weighted classification error. – Jaccard similarity. – Precision and recall. – False positive and false negative rate. – ROC curves. • See the post-lecture bonus slides for additional details. Decision Theory and “Debugging by TA” • Here is one way to write a complicated program: 1. Write the entire function at once. 2. Try it out to “see if it works”. 3. Spend hours fiddling with commands, to find magic working combination. 4. Send code to the TA, asking “what is wrong?” • Decision theory: – If you are lucky, Step 2 works and you are done! – If you are not lucky, takes way longer than principled coding methods. – E[time(“see if it works”)] ≥ E[time(“carefully implement”)). • The above is also a great way to introduce bugs into your code. • So E[bugs(“see if it works”)] ≥ E[bugs(“carefully implementation”)). • And you will not be able to do Step 4 when you graduate. Digression: Debugging 101 • What strategies could we use to debug an ML implementation? – Use “print” statements to see what is happening at each step of the code. • Or use a debugger. – Develop one or more simple “test cases”, were you worked out the result by hand. • Maybe one of the functions you are using does not work the way you think it does. – Check if the “predict” functionality works correctly on its own. • Maybe the training works but the prediction does not. – Check if the “training” functionality works correctly on its own. • Maybe the prediction works but the training does not. – Try the implementation with only one training example or only one feature. • Maybe there is an indexing problem, or things are not being aggregated properly. – Try the implementation with only two features so you can visualize the decision surface. • May be able to see obvious problems. – Make a “brute force” implementation to compare to your “fast/clever” implementation. • Maybe you made a mistake when trying to be fast/clever. • With these strategies, you should be able to diagnose locations of problems. Next Topic: Non-Parametric Models Decision Trees vs. Naïve Bayes • Decision trees: 1. Sequence of rules based on 1 feature. 2. Training: 1 pass over data per depth. 3. Greedy splitting as approximation. 4. Testing: just look at features in rules. 5. New data: might need to change tree. 6. Accuracy: good if simple rules based on individual features work (“symptoms”). 7. Interpretability: easy to see how decisions are made. • Naïve Bayes: 1. Simultaneously combine all features. 2. Training: 1 pass over data to count. 3. Conditional independence assumption. 4. Testing: look at all features. 5. New data: just update counts. 6. Accuracy: good if features almost independent given label (bag of words). 7. Interpretability: can see how each feature influences decision. Geometric Motivation for K-Nearest Neigbours • Do you think the green example should be orange or blue? test example Feature space Geometric Motivation for K-Nearest Neigbours • Do you think the green example should be orange or blue? – In the feature space, it is close to examples labeled orange (“neighbours”). test example Feature space K-Nearest Neighbours (KNN) • An old/simple classifier: k-nearest neighbours (KNN). • To classify an example ෤𝑥i: 1. Find the ‘k’ training examples xi that are “nearest” to ෤𝑥i. 2. Classify using the most common label of “nearest” training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 … … Label O + + O + … K-Nearest Neighbours (KNN) • An old/simple classifier: k-nearest neighbours (KNN). • To classify an example ෤𝑥i: 1. Find the ‘k’ training examples xi that are “nearest” to ෤𝑥i. 2. Classify using the most common label of “nearest” training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 … … Label O + + O + … K-Nearest Neighbours (KNN) • An old/simple classifier: k-nearest neighbours (KNN). • To classify an example ෤𝑥i: 1. Find the ‘k’ training examples xi that are “nearest” to ෤𝑥i. 2. Classify using the most common label of “nearest” training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 … … Label O + + O + … K-Nearest Neighbours (KNN) • An old/simple classifier: k-nearest neighbours (KNN). • To classify an example ෤𝑥i: 1. Find the ‘k’ training examples xi that are “nearest” to ෤𝑥i. 2. Classify using the most common label of “nearest” training examples. F1 F2 1 3 2 3 3 2 2.5 1 3.5 1 … … Label O + + O + … K-Nearest Neighbours (KNN) • An old/simple classifier: k-nearest neighbours (KNN). • To classify an example ෤𝑥i: 1. Find the ‘k’ training examples xi that are “nearest” to ෤𝑥i. 2. Classify using the most common label of “nearest” training examples. Egg Milk Fish 0 0.7 0 0.4 0.6 0 0 0 0 0.3 0.5 1.2 0.4 0 1.2 Sick? 1 1 0 1 1 Egg Milk Fish 0.3 0.6 0.8 Sick? ? K-Nearest Neighbours (KNN) • Assumption: – Examples with similar features are likely to have similar labels. • Seems strong, but all good classifiers basically rely on this assumption. – If not true there may be nothing to learn and you are in “no free lunch” territory. – Methods just differ in how you define “similarity”. • Most common distance function is Euclidean distance: – xi is features of training example ‘i’, and ෤𝑥 ǁ𝑖 is features of test example ‘ ǁ𝑖’. – Costs O(d) to calculate for a pair of examples. Effect of ‘k’ in KNN. • With large ‘k’ (hyper-parameter), KNN model will be very simple. – With k=n, you just predict the mode of the labels. – Model gets more complicated as ‘k’ decreases. – With k=1 it is very sensitive to data (can fit data better but can overfit better too). • Effect of ‘k’ on fundamental trade-off: – As ‘k’ grows, training error tends to increase. – As ‘k’ grows, generalization gap tends to decrease. KNN Implementation • There is no training phase in KNN (“lazy” learning). – You just store the training data. – Costs O(1) if you use a pointer. • But predictions are expensive: O(nd) to classify 1 test example. – Need to do an O(d) distance calculation for all ‘n’ training examples. – So prediction time grows with number of training examples. • Tons of work on reducing this cost (for example, “condensed nearest neighbor”). • And storage is expensive: needs O(nd) memory to store ‘X’ and ‘y’. – So memory grows with number of training examples. – When storage depends on ‘n’, we call it a non-parametric model. Parametric vs. Non-Parametric • Parametric models: – Have fixed number of parameters: trained “model” size is O(1) in terms ‘n’. • E.g., naïve Bayes just stores counts. • E.g., fixed-depth decision tree just stores rules for that depth. – You can estimate the fixed parameters more accurately with more data. – But eventually more data does not help: model is too simple. • Non-parametric models: – Number of parameters grows with ‘n’: size of “model” depends on ‘n’. – Model gets more complicated as you get more data. • E.g., KNN stores all the training data, so size of “model” is O(nd). • E.g., decision tree whose depth depends on number of training examples. Parametric vs. Non-Parametric Models • Parametric models have bounded memory. • Non-parametric models can have unbounded memory. Effect of ‘n’ in KNN. • With a small ‘n’, KNN model will be very simple. • Model gets more complicated as ‘n’ increases. – Requires more memory, but detects subtle differences between examples. Consistency of KNN (‘n’ going to ‘∞’) • KNN has appealing consistency properties: – As ‘n’ goes to ∞, KNN test error is less than twice best possible error. • For fixed ‘k’ and binary labels (under mild assumptions). • Stone’s Theorem: KNN is “universally consistent”. – If k/n goes to zero and ‘k’ goes to ∞, converges to the best possible error. • For example, k = log(n). • First algorithm shown to have this property. • Does Stone’s Theorem violate the no free lunch theorem? – No: it requires a continuity assumption on the labels. – Consistency says nothing about finite ‘n’ (see \"Dont Trust Asymptotics”). • The “speed” at which universal consistency happens is exponential in the dimension ‘d’. Parametric vs. Non-Parametric Models • With parametric models, there is an accuracy limit. – Even with infinite ‘n’, may not be able to achieve optimal error (Ebest). Parametric vs. Non-Parametric Models • With parametric models, there is an accuracy limit. – Even with infinite ‘n’, may not be able to achieve optimal error (Ebest). • Many non-parametric models (like KNN) converge to optimal error. – Though may also converge to needing infinite memory. Summary • Decision theory allows us to consider costs of predictions. • Debugging 101: ideas to find bugs and write code with fewer bugs. • K-Nearest Neighbours: use most common label of nearest examples. • Often works surprisingly well. • Suffers from high prediction and memory cost. • Canonical example of a “non-parametric” model. • Non-parametric models grow with number of training examples. – Can have appealing “consistency” properties. • Next Time: • Fighting the fundamental trade-off and Microsoft Kinect. Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Training Phase • Training a naïve Bayes model: Naïve Bayes Prediction Phase • Prediction in a naïve Bayes model: Naïve Bayes Prediction Phase • Prediction in a naïve Bayes model: Naïve Bayes Prediction Phase • Prediction in a naïve Bayes model: Naïve Bayes Prediction Phase • Prediction in a naïve Bayes model: Naïve Bayes Prediction Phase • Prediction in a naïve Bayes model: “Proportional to” for Probabilities • When we say “p(y) ∝ exp(-y2)” for a function ‘p’, we mean: • However, if ‘p’ is a probability then it must sum to 1. – If 𝑦 ∈ 1,2,3,4 then • Using this fact, we can find β: Probability of Paying Back a Loan and Ethics • Article discussing predicting “whether someone will pay back a loan”: – https://www.thecut.com/2017/05/what-the-words-you-use-in-a-loan- application-reveal.html • Words that increase probability of paying back the most: – debt-free, lower interest rate, after-tax, minimum payment, graduate. • Words that decrease probability of paying back the most: – God, promise, will pay, thank you, hospital. • Article also discusses an important issue: are all these features ethical? – Should you deny a loan because of religion or a family member in the hospital? – ICBC is limited in the features it is allowed to use for prediction. Avoiding Underflow • During the prediction, the probability can underflow: • Standard fix is to (equivalently) maximize the logarithm of the probability: Less-Naïve Bayes • Given features {x1,x2,x3,…,xd}, naïve Bayes approximates p(y|x) as: • The assumption is very strong, and there are “less naïve” versions: – Assume independence of all variables except up to ‘k’ largest ‘j’ where j < i. • E.g., naïve Bayes has k=0 and with k=2 we would have: • Fewer independence assumptions so more flexible, but hard to estimate for large ‘k’. – Another practical variation is “tree-augmented” naïve Bayes. Computing p(xi) under naïve Bayes • Generative models don’t need p(xi) to make decisions. • However, it’s easy to calculate under the naïve Bayes assumption: Gaussian Discriminant Analysis • Classifiers based on Bayes rule are called generative classifier: – They often work well when you have tons of features. – But they need to know p(xi | yi), probability of features given the class. • How to “generate” features, based on the class label. • To fit generative models, usually make BIG assumptions: – Naïve Bayes (NB) for discrete xi: • Assume that each variables in xi is independent of the others in xi given yi. – Gaussian discriminant analysis (GDA) for continuous xi. • Assume that p(xi | yi) follows a multivariate normal distribution. • If all classes have same covariance, it’s called “linear discriminant analysis”. Other Performance Measures • Classification error might be wrong measure: – Use weighted classification error if have different costs. – Might want to use things like Jaccard measure: TP/(TP + FP + FN). • Often, we report precision and recall (want both to be high): – Precision: “if I classify as spam, what is the probability it actually is spam?” • Precision = TP/(TP + FP). • High precision means the filtered messages are likely to really be spam. – Recall: “if a message is spam, what is probability it is classified as spam?” • Recall = TP/(TP + FN) • High recall means that most spam messages are filtered. Precision-Recall Curve • Consider the rule p(yi = ‘spam’ | xi) > t, for threshold ‘t’. • Precision-recall (PR) curve plots precision vs. recall as ‘t’ varies. http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf ROC Curve • Receiver operating characteristic (ROC) curve: – Plot true positive rate (recall) vs. false positive rate (FP/FP+TN). (negative examples classified as positive) – Diagonal is random, perfect classifier would be in upper left. – Sometimes papers report area under curve (AUC). • Reflects performance for different possible thresholds on the probability. http://pages.cs.wisc.edu/~jdavis/davisgoadrichcamera2.pdf More on Unbalanced Classes • With unbalanced classes, there are many alternatives to accuracy as a measure of performance: – Two common ones are the Jaccard coefficient and the F-score. • Some machine learning models don’t work well with unbalanced data. Some common heuristics to improve performance are: – Under-sample the majority class (only take 5% of the spam messages). • https://www.jair.org/media/953/live-953-2037-jair.pdf – Re-weight the examples in the accuracy measure (multiply training error of getting non-spam messages wrong by 10). – Some notes on this issue are here. More on Weirdness of High Dimensions • In high dimensions: – Distances become less meaningful: • All vectors may have similar distances. – Emergence of “hubs” (even with random data): • Some datapoints are neighbours to many more points than average. – Visualizing high dimensions and sphere-packing Vectorized Distance Calculation • To classify ‘t’ test examples based on KNN, cost is O(ndt). – Need to compare ‘n’ training examples to ‘t’ test examples, and computing a distance between two examples costs O(d). • You can do this slightly faster using fast matrix multiplication: – Let D be a matrix such that Dij contains: where ‘i’ is a training example and ‘j’ is a test example. – We can compute D in Julia using: – And you get an extra boost because Julia uses multiple cores. Condensed Nearest Neighbours • Disadvantage of KNN is slow prediction time (depending on ‘n’). • Condensed nearest neighbours: – Identify a set of ‘m’ “prototype” training examples. – Make predictions by using these “prototypes” as the training data. • Reduces runtime from O(nd) down to O(md). Condensed Nearest Neighbours • Classic condensed nearest neighbours: – Start with no examples among prototypes. – Loop through the non-prototype examples ‘i’ in some order: • Classify xi based on the current prototypes. • If prediction is not the true yi, add it to the prototypes. – Repeat the above loop until all examples are classified correctly. • Some variants first remove points from the original data, if a full-data KNN classifier classifies them incorrectly (“outliers’). Condensed Nearest Neighbours • Classic condensed nearest neighbours: • Recent work shows that finding optimal compression is NP-hard. – An approximation algorithm algorithm was published in 2018: • “Near optimal sample compression for nearest neighbors” https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm Refined Fundamental Trade-Off • Let Ebest be the irreducible error (lowest possible error for any model). – For example, irreducible error for predicting coin flips is 0.5. • Some learning theory results use Ebest to further decompose Etest: • Egap measures how sensitive we are to training data. • Emodel measures if our model is complicated enough to fit data. • Ebest measures how low can any model make test error. – Ebest does not depend on what model you choose. Consistency and Universal Consistency • A model is consistent for a particular learning problem if: – Etest converges to Ebest as ‘n’ goes to infinity, for that particular problem. • A model is universally consistent for a class of learning problems if: – Etest converges to Ebest as ‘n’ goes to infinity, for all problems in the class. • Class of learning problems will usually be “all problems satisfying”: – A continuity assumption on the labels yi as a function of xi. • E.g., if xi is close to xj then they are likely to receive the same label. – A boundedness assumption of the set of xi. Consistency of KNN (Discrete/Deterministic Case) • Let’s show universal consistency of KNN in a simplified setting. – The xi and yi are binary, and yi being a deterministic function of xi. • Deterministic yi implies that Ebest is 0. • Consider KNN with k=1: – After we observe an xi, KNN makes right test prediction for that vector. – As ‘n’ goes to ∞, each feature vectors with non-zero probability is observed. – We have Etest = 0 once we’ve seen all feature vectors with non-zero probability. • Notes: – “No free lunch” isn’t relevant as ‘n’ goes to ∞: we eventually see everything. • But there are 2d possible feature vectors, so might need a huge number of training examples. – It’s more complicated if labels aren’t deterministic and features are continuous. Consistency of Non-Parametric Models • Universal consistency can be been shown for many models we’ll cover: – Linear models with polynomial basis. – Linear models with Gaussian RBFs. – Neural networks with one hidden layer and standard activations. • Sigmoid, tanh, ReLU, etc. • But it’s always the non-parametric versions that are consistent: – Where size of model is a function of ‘n’. – Examples: • KNN needs to store all ‘n’ training examples. • Degree of polynomial must grow with ‘n’ (not true for fixed polynomial). • Number of hidden units must grow with ‘n’ (not true for fixed neural network).","libVersion":"0.2.1","langs":""}