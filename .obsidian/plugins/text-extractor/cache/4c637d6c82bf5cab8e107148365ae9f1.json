{"path":".obsidian/plugins/text-extractor/cache/4c637d6c82bf5cab8e107148365ae9f1.json","text":"2.2 Smooth Approximation to the L1-Norm [8 points] Unfortunately, we typically do not know the identities of the outliers. In situations where we suspect that there are outliers, but we do not know which examples are outliers, it makes sense to use a loss function that is more robust to outliers. In class, we discussed using the sum of absolute values objective, n T fw) =\" [w\"z; — yil. i=1 This is less sensitive to outliers than least squares, but it is non-differentiable and harder to optimize. Nevertheless, there are various smooth approximations to the absolute value function that are easy to optimize. One possible approximation is to use the log-sum-exp approximation of the max functionﬂ |r| = max{r, —r} ~ log(exp(r) + exp(—)). Using this approximation, we obtain an objective of the form n flw)= Z log (exp(w”z; — y;) + exp(y; — w”'z;)) . i=1 which is smooth but less sensitive to outliers than the squared error. Derive the gradient V f of this function with respect to w. You should show your work but you do not have to express the final result in matrix notation. 1Other possibilities are the Huber loss, or |r| &~ v/rZ + € for some small e.","libVersion":"0.2.1","langs":"eng"}