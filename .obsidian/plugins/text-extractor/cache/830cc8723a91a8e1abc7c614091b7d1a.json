{"path":".obsidian/plugins/text-extractor/cache/830cc8723a91a8e1abc7c614091b7d1a.json","text":"Introduction to Partial Diﬀerential Equations Math 257/316 University of British Columbia Draft and still in progress Year: 2024-2025 Some materials are adapted from lecture notes by Prof. Peirce (available here) and Prof. Rahmani. Contents 1 Review of techniques to solve Ordinary Diﬀerential Equations 3 1.1 Separable equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Linear ﬁrst-order equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.3 Second-order linear homogeneous equations with constant coeﬃcients . . . . . . . . . 6 1.4 Second-order Euler equations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2 Series solutions of variable coeﬃcient ordinary diﬀerential equations 12 2.1 Power series method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.2 Power series solution of general variable coeﬃcient linear ODE . . . . . . . . . . . . . 19 2.2.1 Homogeneous case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.2.2 Non-homogeneous case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 2.2.3 Series solutions at singular points . . . . . . . . . . . . . . . . . . . . . . . . . 28 2.3 Bessel functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 2.3.1 Bessel’s function of order ν /∈ {. . . , −2, −1, 0, 1, 2 . . .} . . . . . . . . . . . . . . 37 2.3.2 Bessel’s function of order ν = 0: repeated roots . . . . . . . . . . . . . . . . . 38 2.3.3 Bessel’s Function of Order ν = 1 2 : . . . . . . . . . . . . . . . . . . . . . . . . . 40 3 Introduction to partial diﬀerential equations 45 3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.2 Classiﬁcation of PDEs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.3 A one dimensional conservation law . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.4 The heat/diﬀusion equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 3.5 The Wave Equation: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 3.6 Laplace’s equation: Flow in porous media . . . . . . . . . . . . . . . . . . . . . . . . 52 4 Introduction to numerical methods for partial diﬀerential equations 54 4.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.2 Approximating the derivatives of a function by ﬁnite diﬀerences . . . . . . . . . . . . 54 4.3 Solving the heat equation using the method of ﬁnite diﬀerences . . . . . . . . . . . . 56 4.3.1 Dirichlet boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 56 4.3.2 Neumann boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.4 Solving the Wave equation using the method of ﬁnite diﬀerences . . . . . . . . . . . . 58 4.4.1 Dirichlet boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 58 4.4.2 Neumann Boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . 59 4.4.3 Initial conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60 4.5 Solving the Laplace’s equation using the method of ﬁnite diﬀerences . . . . . . . . . . 61 4.5.1 Dirichlet boundary conditions . . . . . . . . . . . . . . . . . . . . . . . . . . . 61 1 5 Fourier series and separation of variables 63 5.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63 5.2 Solution to the heat equation by separation of variables . . . . . . . . . . . . . . . . . 64 5.2.1 Dirichlet boundary conditions: Fourier sine series . . . . . . . . . . . . . . . . 64 5.2.2 Neumann boundary conditions: Fourier cosine series . . . . . . . . . . . . . . . 70 5.2.3 Heat equation on a circular ring - Full Fourier series . . . . . . . . . . . . . . . 75 5.3 Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 5.3.1 Half range Fourier Series: even and odd functions . . . . . . . . . . . . . . . . 84 5.3.2 Half-range expansions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 5.3.3 Convergence of Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . . 89 5.3.4 Complex form of Fourier Series . . . . . . . . . . . . . . . . . . . . . . . . . . 91 5.4 Bessel’s inequality and Parseval Identity . . . . . . . . . . . . . . . . . . . . . . . . . 91 5.5 Heat conduction problems with time-independent inhomogeneous boundary conditions 96 5.6 Solving the non-homgeneous heat equation with non-homogeneous time dependent BC 113 6 The Wave Equation 117 6.1 Solution of the Wave Equation by separation of variables . . . . . . . . . . . . . . . . 117 6.2 Non-homogeneous Wave Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121 6.3 d’Alembert’s solution of the Wave Equation . . . . . . . . . . . . . . . . . . . . . . . 124 6.4 Interpretation of the Fourier series solution in terms of d’Alembert’s solution . . . . . 127 7 The Laplace Equation 130 7.1 Rectangular domain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 130 7.2 Laplace’s Equation on a disk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140 7.3 Semi-inﬁnite strip problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145 8 Sturm-Liouville boundary value problems 149 8.1 Properties of SL problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152 8.2 Application: Solving the heat equation with Robin boundary conditions . . . . . . . . 155 2 Chapter 1 Review of techniques to solve Ordinary Diﬀerential Equations The aim of this ﬁrst lecture is to provide you with a warm-up on some techniques for solving Ordinary Diﬀerential Equations (ODEs). You are likely already familiar with these techniques, as they were introduced in Math 215, 255, and 256. We will brieﬂy review some of them. 1.1 Separable equations Deﬁnition 1.1.1 We say that a ﬁrst-order ODE is separable when it can be written on the form: dy dx = g(x)h(y), (1.1) where g is a function of x only and h is a function of y only. Method 1.1.1 To solve the ODE (1.1), we proceed as follows: • Bring all terms involving x to one side, and all terms involving y and dy to the other side. • Integrate both sides with respect to their respective variables (dont forget an arbitrary constant). • Solve the resulting equation for y(x). When it is clear from the context, we may sometimes write y′ to denote dy dx. Example 1.1.1 Solve the following equation (x ̸= 0): x3y′ = e3y. 3 We can separate the variables because y′ = dy dx; this gives e−3y dy = 1 x3 dx. By integrating, we obtain e −3y 3 = 1 2x2 + C. Thus, y = −1 3 ln \f \f \f \f 3 2x2 + ˜C\f \f \f \f , where ˜C ∈ R. Example 1.1.2 Solve the diﬀerential equation: dy dx = 2x(y2 + 1). We rewrite the equation to separate the variables x and y: 1 y2 + 1 dy = 2x dx. Now, if we integrate both sides of the latter equation, we obtain arctan(y) = x2 + C, where C ∈ R. To solve for y, we take the tangent of both sides to obtain y = tan(x2 + C), where C ∈ R is an arbitrary constant. 1.2 Linear ﬁrst-order equations First-order linear diﬀerential equations are the most interesting because we encounter them in physics (electricity, mechanics, etc.). These are equations where y and y′ are of the ﬁrst degree. Deﬁnition 1.2.1: First-order linear diﬀerential equations The general form is: a(x)y′ + b(x)y = c(x), (1.2) where a, b, and c are functions of x. When a(x) ̸= 0, then equation (1.2) is said to be normalized. In this case, (1.2) reduces to the equation: 4 y′ + p(x)y = q(x), (1.3) where p(x) = b(x) a(x) and q(x) = c(x) a(x) . If c(x) = 0 (equivalently q(x) = 0), then the equation obtained will be homogeneous, called a homogeneous linear equation. Method 1.2.1 To solve (1.3) using the integrator factor method, we proceed as follows: • We compute the integrating factor: µ(x) = e ∫ p(x) dx (Don’t worry about arbitrary constants here). • Multiply the equation (1.3) by the integrating factor. • Observe that, by the product rule, the left-hand side of the resulting equation can be rewritten as d dx [µ(x)y(x)], thus giving you the equation: d dx [µ(x)y(x)] = µ(x)q(x). • Integrate both sides of your last equation with respect to x, and solve for y(x). Don’t forget the arbitrary constant. Note that to obtain the expression for the integrating factor µ(x), we use the requirement that: d dx [µ(x)y(x)] = µ dy dx + dµ dx y = µ dy dx + µp(x)y. This means that µ must satisfy the simple diﬀerential equation: dµ dx = µp(x). Example 1.2.1 Solve the following equation: x dy dx + 2y = 10x2. For x ̸= 0, we rewrite the equation in standard linear form: dy dx + 2 x y = 10x. (1.4) This is now in the form: dy dx + p(x)y = q(x), 5 where p(x) = 2 x and q(x) = 10x. The integrating factor µ(x) is given by: µ(x) = e∫ p(x) dx = e∫ 2 x dx = x2. Now, we multiply the diﬀerential equation (1.4) by µ(x) = x2 to obtain: x2 dy dx + 2xy = 10x3. The left-hand side can be written as the derivative of a product: d dx (x2y) = 10x3. Now, if we integrate both sides with respect to x: ∫ d dx ( x2y) dx = ∫ 10x3 dx, we obtain x2y = 5x4 2 + C, where C ∈ R. That is y = 5x2 2 + C x2 , x ̸= 0 where C ∈ R is an arbitrary constant. 1.3 Second-order linear homogeneous equations with con- stant coeﬃcients Deﬁnition 1.3.1: Homogeneous second-order ODE We call second-order linear homogeneous diﬀerential equation a ODE of the form: Ly := ay′′ + by′ + cy = 0, where a, b, and c are real constants. To solve this type of equation, we assume a solution of the form: y(x, r) = erx, where r is a constant to be determined. Substituting this expression for y(x) into the diﬀerential equation gives the corresponding characteristic equation: ar2 + br + c = 0. Solving this quadratic equation yields two possible values for r: r1 = −b + √b2 − 4ac 2a , r2 = −b − √ b2 − 4ac 2a . 6 Depending on the discriminant ∆ = b2 − 4ac, we consider the following cases: (a) Distinct real roots: If r1 and r2 are two distinct real values, then the general solution to the diﬀerential equation is: y(x) = c1er1x + c2er2x, where c1 and c2 are arbitrary constants. (b) Repeated real roots: If r1 = r2, then the roots are real and equal, and the general solution is: y(x) = c1er1x + c2xer1x. Wait a moment! In this case, we initially obtain only one solution, y(x) = e r1x. But how do we derive the second term, c2xer1x? To understand this, consider the function: y(r, x) = e rx. When the linear diﬀerential operator L is applied to y(r, x), we have: Ly(r, x) = a(r − r1)2e rx. Now, take the partial derivative of y(r, x) with respect to r, and evaluate it at r = r1: L [∂y ∂r (r, x) ] r=r1 = [ 2a(r − r1)e rx + a(r − r1) 2xe rx] r=r1 . At r = r1, this expression simpliﬁes to: L [ ∂y ∂r (r, x)] r=r1 = 0. This demonstrates that the derivative: [∂y ∂r (r, x) ] r=r1 = xe r1x is also a valid solution to the diﬀerential equation. (c) Complex roots: If r1 and r2 are complex conjugates, i.e., r1 = α + iβ and r2 = α − iβ, where α and β are real constants, the general solution to the diﬀerential equation can be written as: y(x) = c1e(α+iβ)x + c2e(α−iβ)x. Using Euler’s formula e (α±iβ)x = e αx[cos(βx) ± i sin(βx)], the solution can also be expressed as: y(x) = C1eαx cos(βx) + C2e αx sin(βx), where C1 and C2 are arbitrary constants. This form is typically preferred in practice since it involves real-valued functions. 7 Example 1.3.1 Solve the following diﬀerential equations: 1. y′′ − 9y = 0 a) If we let y = erx, we obtain the characteristic equation: r2 − 9 = 0 b) Solving for r: r2 = 9 ⇒ r = ±3 c) The general solution is: y(x) = c1e 3x + c2e −3x, where c1 and c2 are arbitrary constants. 2. y′′ + 9y = 0 a) If we let y = erx, we obtain the characteristic equation: r2 + 9 = 0 b) Solving for r: r2 = −9 ⇒ r = ±3i c) The general solution is: y(x) = c1 cos(3x) + c2 sin(3x), where c1 and c2 are arbitrary constants. 3. y′′ + 6y′ + 9y = 0 a) If we let y = erx, we obtain the characteristic equation: r2 + 6r + 9 = 0 b) Factorizing: (r + 3)2 = 0 ⇒ r = −3 c) The general solution is: y(x) = c1e −3x + c2xe−3x, where c1 and c2 are arbitrary constants. 4. y′′ + 6y′ − 9y = 0 a) If we let y = erx, we obtain the characteristic equation: r2 + 6r − 9 = 0 b) Solving using the quadratic formula: r = −6 ± √ 62 − 4(1)(−9) 2(1) = −3 ± 3 √2 c) The general solution is: y(x) = c1e(−3+3 √2)x + c2e (−3−3 √2)x, where c1 and c2 are arbitrary constants. 8 1.4 Second-order Euler equations Deﬁnition 1.4.1 A second-order Euler equation is a type of diﬀerential equation written as: x2y′′ + αxy′ + βy = 0, where α, and β are constants. To solve this equation, we guess a solution of the form: y(x) = xr, where r is a number we need to ﬁnd. Then, we substitute this guess into the equation and solve for r. We obtain: {r(r − 1) + αr + β}xr = 0. This simpliﬁes to the characteristic equation: f (r) = r2 + (α − 1)r + β = 0. The solutions for r are: r± = 1 − α ± √ (α − 1)2 − 4β 2 . Case 1: Distinct real roots (∆ > 0) If ∆ = (α − 1) 2 − 4β > 0, the roots r1 and r2 are distinct real numbers. The general solution is: y(x) = c1xr1 + c2xr2. If either r1 or r2 is negative, the solution |y| → ∞ as x → 0. Case 2: Repeated roots (∆ = 0) If ∆ = 0, there is only one root r1. In this case, the solution is: y(x) = c1xr1. To ﬁnd a second independent solution, we proceed as above. We write L[xr] = (r − r1) 2er ln x. If we diﬀerentiate with respect to the parameter r, and evaluate it at r = r1, we obtain: [ L [ ∂ ∂r xr]] r=r1 = [ ∂ ∂r L[xr] ] r=r1 = 0. So, [ ∂ ∂r (xr) ] r=r1 = xr1 ln x is also a solution. This leads to the general solution: y(x) = (c1 + c2 ln x)xr1, where c1 and c2 are arbitrary constants. Finally, using the Wronskian test, we can prove that the two solutions are linearly independent. 9 Case 3: Complex roots (∆ < 0) If ∆ = (α − 1) 2 − 4β < 0, the roots are complex: r± = 1 − α 2 ± i √ 4β − (α − 1)2 2 . Let λ = 1−α 2 and µ = √ 4β−(α−1)2 2 . The solution becomes: y(x) = xλ (A1 cos(µ ln x) + A2 sin(µ ln x)) . If x < 0, replace x with |x|. Example 1.4.1 Solve the following diﬀerential equations: 1. x2y′′ + xy′ − 9y = 0. We assume the solution in the form y = xr, and plug it into the diﬀerential equation: x2 d2 dx2 (xr) + x d dx (xr) − 9 (xr) = 0. Now, applying the derivatives, we get: x2 [r(r − 1)xr−2] + x [ rxr−1] − 9xr = 0. Simplifying further: r2xr − rxr + rxr − 9xr = 0. This simpliﬁes to: r2 − 9 = 0. Solving for r, we get: r = ±3. Thus, the general solution to the diﬀerential equation is: y(x) = c1x3 + c2x−3, where c1 and c2 are arbitrary constants. 2. x2y′′ − 3xy′ + 4y = 0, y(1) = 1, y′(1) = 0. Assume a solution of the form y = xr. Substituting into the diﬀerential equation: r(r − 1)xr−2 − 3rxr−1 + 4xr = 0. 10 Multiplying through by x2, we get: r(r − 1) − 3r + 4 = 0, which simpliﬁes to: r2 − 4r + 4 = 0. Factoring, we get: (r − 2) 2 = 0. Thus, r = 2 is a double root. The general solution to the diﬀerential equation is: y(x) = c1x2 + c2x2 ln x. Now, using the initial conditions y(1) = 1 and y′(1) = 0: From y(1) = 1, we get: c1 · 1 2 + c2 · 1 2 ln(1) = 1 ⇒ c1 = 1. To ﬁnd c2, we compute y′(x): y′(x) = d dx ( c1x2 + c2x2 ln x) = 2c1x + c2 (2x ln x + x) . Evaluating at x = 1: y′(1) = 2c1 + c2 (2 · 1 · ln(1) + 1) = 2c1 + c2 = 0. Substituting c1 = 1, we get: 2 + c2 = 0 ⇒ c2 = −2. Thus, the solution is: y(x) = x2 − 2x2 ln x. If x < 0, we replace x by |x|. 11 Chapter 2 Series solutions of variable coeﬃcient ordinary diﬀerential equations In this part, we introduce a method to solve linear diﬀerential equations with variable coeﬃcients, which frequently arise in physical problems. They do not usually admit solutions expressible in terms of elementary functions. Such equations can often be solved using numerical methods, but in many cases, it is easier to ﬁnd solutions in the form of an inﬁnite series. We introduce the concepts of ordinary points about which Taylor series solutions are obtained and singular points about which more general solutions are required. 2.1 Power series method The power series method is the standard method for solving linear ODEs with variable coeﬃcients. It gives solutions in the form of power series. These series can be used for computing values, graphing curves, proving formulas, and exploring properties of solutions, as we shall see. In this section we begin by explaining the idea of the power series method. Deﬁnition 2.1.1: Power series A power series (in powers of x − x0 ) is an inﬁnite series of the form ∞∑ n=0 an (x − x0) n = a0 + a1 (x − x0) + a2 (x − x0) 2 + · · · (2.1) Here, x is a variable. a0, a1, a2, · · · are constants, called the coeﬃcients of the series. x0 is a constant, called the center of the series. In particular, if x0 = 0, we obtain a power series in powers of x ∞∑ n=0 anxn = a0 + a1x + a2x2 + a3x3 + · · · (2.2) We shall assume that all variables and constants are real. 12 Example 2.1.1 Familiar examples of power series are the Taylor-Maclaurin series 1 1 − x = ∞∑ m=0 xm = 1 + x + x2 + · · · (|x| < 1, geometric series ) ex = ∞∑ m=0 xm m! = 1 + x + x2 2! + x3 3! + · · · cos x = ∞∑ m=0 (−1) mx2m (2m)! = 1 − x2 2! + x4 4! − + · · · sin x = ∞∑ m=0 (−1) mx2m+1 (2m + 1)! = x − x3 3! + x5 5! − + · · · . More generally, if we know all the derivatives of a function f (x) at a single point x0, then we have the Taylor approximation: f (x) = ∞∑ k=0 f (k) (x0) k! (x − x0) k , for x near x0. We note that the term ”power series” usually refers to a series of the form (2.1) [or (2.2)] but does not include series of negative or fractional powers of x. We use m as the summation letter. Before moving further, we review relevant properties of power series. Deﬁnition 2.1.2: Convergence of a power series The power series (2.1) is said to converge for a given x if the limit lim N →∞ N∑ n=0 an(x − x0) n exists. Otherwise, the series diverges for the given x. For any power series (2.1), exactly one of the following statements is true: (a) The power series converges only for x = x0. (b) The power series converges for all values of x. (c) There is a positive number R such that the power series converges if |x − x0| < R and diverges if |x − x0| > R. In case (c), R is called the radius of convergence of the power series. For convenience, we include the other two cases in this deﬁnition by setting R = 0 in case (a) and R = ∞ in case (b). The open interval of convergence is deﬁned as: (x0 − R, x0 + R) if 0 < R < ∞, or (−∞, ∞) if R = ∞. If R is ﬁnite, no general statement can be made about convergence at the endpoints x = x0 ± R. The series may converge at one or both endpoints, or diverge at both. 13 Theorem 2.1.1: Ratio test Consider the series ∑∞ n=0 cn and suppose: lim n→∞ \f \f \f \fcn+1 cn \f \f \f \f = L. (a) If L < 1, then ∑∞ n=0 cn converges. (b) If L > 1 or the limit approaches ∞, then ∑∞ n=0 cn diverges. (c) If L = 1, the ratio test is inconclusive, and another test must be used. Example 2.1.2 1. Find the radius of convergence for the following series: ∞∑ n=0 n!xn. Here, cn = n!xn. Using the ratio test, lim n→∞ \f \f \f \fcn+1 cn \f \f \f \f = lim n→∞ \f \f \f \f(n + 1)!xn+1 n!xn \f \f \f \f = lim n→∞(n + 1)|x| = ∞ for all x ̸= 0. Thus, the series converges only at x = 0, so R = 0. 2. Find the radius of convergence for the following series: ∞∑ n=1 (−1)nxn n! . Here, cn = (−1)nxn n! . Using the ratio test, lim n→∞ \f \f \f \f cn+1 cn \f \f \f \f = lim n→∞ \f \f \f \f xn+1/(n + 1)! xn/n! \f \f \f \f = |x| lim n→∞ 1 n + 1 = 0 for all x. Hence, R = ∞. 3. Find the radius of convergence for the following series: ∞∑ n=0 2 n n2 (x − 1) n. Here, cn = 2n n2 (x − 1)n. Using the ratio test, lim n→∞ \f \f \f \fcn+1 cn \f \f \f \f = lim n→∞ \f \f \f \f2 n+1(x − 1) n+1/(n + 1)2 2n(x − 1)n/n2 \f \f \f \f = 2|x − 1| lim n→∞ n2 (n + 1)2 = 2|x − 1|. The series converges when 2|x − 1| < 1, or |x − 1| < 1 2. Thus, R = 1 2. 14 4. Find the radius of convergence for the following series: ∞∑ m=0 (−1)m 8m x3m. Here, cn = (−1)m 8m x3m. Using the ratio test, lim n→∞ \f \f \f \fcn+1 cn \f \f \f \f = |x| 3 8 . The series converges when |x|3 8 < 1, or |x| < 2. Thus, R = 2. Idea of the power series method The idea of the power series method for solving ODEs is simple and natural. We describe the practical procedure and illustrate it for two ODEs whose solution we know, so that we can see what is going on. Important! For a given ODE P (x)y′′ + Q(x)y′ + R(x)y = 0, (2.3) we ﬁrst represent P (x), Q(x) and R(x) by power series in powers of x (or of x − x0 if solutions in powers of x − x0 are wanted). Next we assume a solution in the form of a power series with unknown coeﬃcients, y = ∞∑ m=0 amxm = a0 + a1x + a2x2 + a3x3 + · · · (2.4) and insert this series and the series obtained by termwise diﬀerentiation, y′ = ∞∑ m=1 mamxm−1 = a1 + 2a2x + 3a3x2 + · · · (2.5) y′′ = ∞∑ m=2 m(m − 1)amxm−2 = 2a2 + 3 · 2a3x + 4 · 3a4x2 + · · · (2.6) into the ODE. Then we collect like powers of x and equate the sum of the coeﬃcients of each occurring power of x to zero, starting with the constant terms, then taking the terms containing x, then the terms in x2, and so on. This gives equations from which we can determine the unknown coeﬃcients of (2.4) successively. Remark 2.1.1 Note that if the power series (2.4) has a positive radius of convergence R, then the radius of convergence of all its successive derivative series is also R. 15 Let us show this for three simple ODEs that can also be solved by elementary methods, so that we would not need power series. Example 2.1.3 1. Solve the following ODE by power series: y′ + 2y = 0. By substituting (2.4) and (2.5) into the ODE, we get: ∞∑ m=1 mamxm−1 + 2 ∞∑ m=0 amxm = 0. To obtain the same general power on both series, we let s = m − 1 (i.e. m = s + 1) in the ﬁrst series and s = m in the second. This gives . This gives: ∞∑ s=0 (s + 1)as+1xs + 2 ∞∑ s=0 asxs = 0. Combining terms, we have: ∞∑ s=0 [(s + 1)as+1 + 2as] xs = 0. For this equation to hold, we must have: (s + 1)as+1 + 2as = 0. Thus, we derive the recurrence relation: as+1 = − 2as s + 1 , s = 0, 1, 2, . . . Starting with a0 as arbitrary, we calculate the successive coeﬃcients: a1 = −2a0 1 , a2 = −2a1 2 = 2 2a0 2! , a3 = −2a2 3 = −2 3a0 3! . In general, the coeﬃcients are given by: as = (−1) s 2 sa0 s! . Substituting back into the power series, the solution becomes: y(x) = ∞∑ s=0 (−1) s 2 sa0 s! xs = a0 ∞∑ s=0 (−2x)s s! . 16 Recognizing the series for the exponential function, we have: y(x) = a0e−2x, where a0 is an arbitrary constant. 2. Solve the following ODE by power series: y′′ + y = 0. By inserting (2.4) and (2.6) into the ODE we have ∞∑ m=2 m(m − 1)amxm−2 + ∞∑ m=0 amxm = 0 To obtain the same general power on both series, we let s = m − 2 in the ﬁrst series and s = m in the second. This gives ∞∑ s=0 (s + 2)(s + 1)as+2xs + ∞∑ s=0 asxs = 0. Therefore, ∞∑ s=0 [(s + 2)(s + 1)as+2 + as] xs = 0. Hence (s + 2)(s + 1)as+2 + as = 0. This gives the recursion formula as+2 = − as (s + 2)(s + 1) (s = 0, 1, · · · ) We thus obtain successively a2 = − a0 2·1 = − a0 2! , a3 = − a1 3·2 = − a1 3! a4 = − a2 4·3 = a0 4! , a5 = − a3 5·4 = a1 5! and so on. a0 and a1 remain arbitrary. With these coeﬃcients the series becomes y = a0 + a1x − a0 2! x2 − a1 3! x3 + a0 4! x4 + a1 5! x5 + · · · . Reordering terms (which is permissible for a power series), we can write this in the form y = a0 ( 1 − x2 2! + x4 4! − + · · · ) + a1 ( x − x3 3! + x5 5! − + · · · ) 17 and we recognize the familiar general solution y = a0 cos x + a1 sin x, a0, a1 ∈ R. 2. Solve the following ODE by power series: y′ = 2xy We insert (2.4) and (2.5) into the given ODE, obtaining 1 · a1x0 + ∞∑ m=2 mamxm−1 = 2x ∞∑ m=0 amxm = ∞∑ m=0 2amxm+1 Now, to get the same general power on both sides, we make a ”shift of index” on the left by setting m = s + 2, thus m − 1 = s + 1. Then am becomes as+2 and xm−1 becomes xs+1. Also the summation, which started with m = 2, now starts with s = 0 because s = m − 2. On the right we simply make a change of notation m = s, hence am = as and xm+1 = xs+1; also the summation now starts with s = 0. This altogether gives a1 + ∞∑ s=0 (s + 2)as+2xs+1 = ∞∑ s=0 2asxs+1. Every occurring power of x must have the same coeﬃcient on both sides; hence a1 = 0 and (s + 2)as+2 = 2as or as+2 = 2 s + 2 as For s = 0, 1, 2, · · · we thus have a1 = 0, a2 = (2/2)a0, a3 = (2/3)a1 = 0, a4 = (2/4)a2, · · · . Hence a3 = 0, a5 = 0, · · · and for the coeﬃcients with even subscripts, a2 = a0, a4 = a2 2 = a0 2! , a6 = a4 3 = a0 3! , · · · ; a0 remains arbitrary. Hence, y = a0 ( 1 + x2 + x4 2! + x6 3! + x8 4! + · · · ) = a0e x2, a0 ∈ R. Remark 2.1.2 Note that we do not need power series method for these or similar ODEs? We used them just for explaining the idea of the method. What happens if we apply the method to an ODE not of the kind considered so far, even to an innocent-looking one such as y′′ + xy = 0 (”Airy’s equation”)? We most likely end up with new special functions given by power series. 18 2.2 Power series solution of general variable coeﬃcient lin- ear ODE In the last section we saw that the power series method gives solutions of ODEs in the form of power series. 2.2.1 Homogeneous case We consider solving the following variable linear ODE of the form P (x)y′′ + Q(x)y′ + R(x)y = 0. (2.7) If we divide the latter equation by P (x), we obtain Ly := y′′ + p(x)y′ + q(x)y = 0, (2.8) where p(x) = Q(x)/P (x) and q(x) = R(x)/P (x). In order to calculate the higher derivatives of y(x) to substitute into Taylors formula, we rewrite (2.8) as follows: y′′ = −p(x)y′ − q(x)y If y(x0) and y′(x0) are given, then y′′(x0) can be obtained directly from the ODE. Higher derivatives of y can, in turn, be obtained by diﬀerentiating the ODE repeatedly. This process will be successful provided p(x) and q(x) are inﬁnitely diﬀerentiable at x = x0. In this case, p(x) and q(x) are said to be analytic at x0 and have Taylor expansions of the form: p(x) = p0 + p1(x − x0) + · · · = ∞∑ k=0 pk(x − x0) k q(x) = q0 + q1(x − x0) + · · · = ∞∑ k=0 qk(x − x0)k Note that in the power series method we can diﬀerentiate, add, and multiply power series, in a ”suitable sense”. For example: consider two power series ∞∑ m=0 am (x − x0) m and ∞∑ m=0 bm (x − x0)m . Then the series obtained by multiplying each term of the ﬁrst series by each term of the second series and collecting like powers of x − x0 is ∞∑ m=0 (a0bm + a1bm−1 + · · · + amb0) (x − x0)m = a0b0 + (a0b1 + a1b0) (x − x0) + (a0b2 + a1b1 + a2b0) (x − x0)2 + · · · . Ordinary points and singular points 19 Deﬁnition 2.2.1: Ordinary points and singular points The expansion point x0 is said to be an ordinary point of (2.8) if p(x) = Q(x)/P (x) and q(x) = R(x)/P (x) are analytic at x0. Otherwise, x0 is a singular point. Theorem 2.2.1 If x0 is an ordinary point, it is possible to obtain power series expansions of the solution y(x) of the form: y(x) = ∞∑ n=0 cn(x − x0) n (2.9) and to substitute the latter expansion (2.8) and solve for the unknown coeﬃcients cn in order to determine a solution. In addition, (2.9) converges at least on the open interval (x0 − R, x0 + R), where R is the the distance from x0 to the nearest singular point of (2.8). More precisely, the radius of convergence of (2.9) is at least as large as the radius of convergence of each of the series expansions for p(x) = Q(x)/P (x) and q(x) = R(x)/P (x), i.e., up to the closest singularity to x0. Example 2.2.1: Singular point An example of singular point: when P , Q, and R are polynomials and P (x0) = 0 while Q(x0) ̸= 0 or R(x0) ̸= 0, then x0 is a singular point. Another example is when: p(x) = √ x, q(x) = 2, then x0 = 0 is a singular point because p(x) is not diﬀerentiable at x = 0. Observations • If P , Q, and R are polynomials, then a point x0 such that P (x0) ̸= 0 is an ordinary point. • If x0 = 0 is an ordinary point, then we assume: y = ∞∑ n=0 cnxn, y′ = ∞∑ n=1 cnnx n−1, y′′ = ∞∑ n=2 cnn(n − 1)xn−2. The substitution into the ODE 0 = Ly gives: 0 = ∞∑ n=2 cnn(n − 1)xn−2 + ( ∞∑ n=0 pnxn) ( ∞∑ n=1 ncnxn−1) + ( ∞∑ n=0 qnxn) ( ∞∑ n=0 cnxn) . Re-indexing the powers of x, this results in: ∞∑ m=0 {(m + 2)(m + 1)cm+2 + (p0(m + 1)cm+1 + · · · + pmc1) + (q0cm + · · · + qmc0)} xm = 0. This yields a non-degenerate recursion formula for the cm. At an ordinary point x0, we can obtain two linearly independent solutions of the form (2.9). 20 Example 2.2.2 Find the power series in x for the general solution of (1 + 2x2)y′′ + 6xy′ + 2y = 0 (2.10) The functions p(x) = 6x 1+2x2 and q(x) = 2 1+2x2 are analytic at x = 0. Hence, x0 = 0 is an ordinary point of (2.10). Next, we rewrite the given equation as: (1 + 2x2)y′′ + 6xy′ + 2y = y′′ + 2x2y′′ + 6xy′ + 2y = 0. Since x0 = 0 is an ordinary point of (2.10), we assume the general solution in the form of a power series: y = ∞∑ n=0 anxn, which will generate two linearly independent solutions. Now, diﬀerentiating y: y′ = ∞∑ n=1 nanxn−1, y′′ = ∞∑ n=2 n(n − 1)anxn−2. Substituting these into the diﬀerential equation: ∞∑ n=2 n(n − 1)anxn−2 + 2x2 ∞∑ n=2 n(n − 1)anxn−2 + 6x ∞∑ n=1 nanxn−1 + 2 ∞∑ n=0 anxn = 0, we can express the equation as: ∞∑ n=2 n(n − 1)anxn−2 + ∞∑ n=2 2n(n − 1)anxn + ∞∑ n=1 6nanxn + ∞∑ n=0 2anxn = 0. Let m = n − 2 in the ﬁrst sum then, n = m + 2 and when n = 2, m = 0. We thus obtain after taking n = m in the other sums: ∞∑ m=0 (m + 2)(m + 1)am+2xm + ∞∑ m=2 2m(m − 1)amxm + ∞∑ m=1 6mamxm + ∞∑ m=0 2amxm = 0. Reindexing: 2a2 + 6a3x + ∞∑ m=2 (m + 2)(m + 1)am+2xm + ∞∑ m=2 2m(m − 1)amxm + 6a1x + ∞∑ m=2 6mamxm +2a0 + 2a1x + ∞∑ m=2 2amxm = 0. Next, we collect like terms: (2a2 + 2a0) + (6a3 + 8a1)x + ∞∑ m=2 [(m + 2)(m + 1)am+2 + (2m 2 + 4m + 2)am] xm = 0. 21 Equating the coeﬃcients of powers of x, we get the recurrence relations for am: a2 = −a0, a3 = −4 3a1, am+2 = − 2(m + 1)2 (m + 1)(m + 2) am = −2(m + 1) (m + 2) am, m = 2, 3, 4, . . . . (2.11) Substituting m = 2, 3, 4, . . . into the recurrence relations: a4 = −3 2 a2 = 3 2a0, a5 = −8 5 a3 = 32 15a1, a6 = −5 3 a4 = −5 2a0, a7 = −12 7 a5 = −128 35 a1, . . . Thus, the general solution is: y = a0 (1 − x2 + 3 2 x4 − 5 2x6 + · · · ) + a1 ( x − 4 3x3 + 32 15x5 − 128 35 x7 + · · · ) . (2.12) This is the power series in x for the general solution of equation (2.10). Now, let us compute the radius of convergence of this solution. We use the ratio test and the recurrence relation (2.11): lim m→∞ \f \f \f \f am+2xm+2 amxm \f \f \f \f = |x| 2 lim m→∞ \f \f \f \f−2(m + 1) (m + 2) \f \f \f \f = 2|x| 2. Therefore, the series converges if 2|x| 2 < 1 i.e., |x| < √2 2 . Hence, R = √2 2 . Another way to determine the radius of convergence is to ﬁnd the nearest singular point of (2.10) (which could be a complex number) and compute the distance from x0 = 0 to this nearest singular point. Since P is a polynomial, P (x) = 1 + 2x2, the singular points are the roots of P (x) = 0, i.e., 1 + 2x2 = 0 =⇒ x2 = −1 2 =⇒ x = ±i 1 √2. The singular points are therefore x = ±i √2 2 . To compute the distance from x0 = 0 to the nearest singular point, we calculate the modulus of x = i √2 2 (or of x = −i √2 2 ): |x| = \f \f \f \f \fi √2 2 \f \f \f \f \f = √2 2 . Hence according to Theorem 2.2.1, the series (2.12) converges at least on the open interval (− √2 2 , √2 2 ). Example 2.2.3: The Airy equation: Consider the Airy equation, which arises in Quantum Mechanics: Ly = y′′ − xy = 0 We observe that x = 0 is an ordinary point. 22 y = ∞∑ n=0 cnxn, y′ = ∞∑ n=1 cnnxn−1, y′′ = ∞∑ n=2 cnn(n − 1)xn−2. ∞∑ n=2 cnn(n − 1)xn−2 − ∞∑ n=0 cnxn+1 = 0. In the ﬁrst sum, let m + 1 = n − 2 n = m + 3 n = 2 ⇒ m = −1 c22x0 + ∞∑ m=0 [cm+3(m + 3)(m + 2) − cm] xm+1 = 0 c2 = 0, cm+3 = cm (m + 3)(m + 2) m = 0, 1, . . . (1) c0 → c3 → c6. c3 = c0 3.2 , c6 = c3 6.5 = c0 6.5.3.2, c9 = c0 9.8.6.5.3.2 c3n = c0 (3n)(3n − 1)(3n − 3)(3n − 4) . . . 9.8.6.5.3.2 y0(x) = 1 + x3 3.2 + x6 6.5.3.2 + · · · + x3n (3n)(3n − 1) . . . 3.2 + . . . (2) c1 → c4 → c7 →. c4 = c1 4.3 c7 = c1 7.6.4.3 c10 = c1 (10.9)(7.6)(4.3) c3n+1 = c1 (3n + 1)(3n)(3n − 2)(3n − 3) . . . (7.6)(4.3) y1(x) = x + x4 4.3 + x7 7.6.4.3 + · · · + x3n+1 (3n + 1)(3n) . . . 4.3 y(x) = c0y0(x) + c1y1(x) Radius of Convergence: lim m→∞ \f \f \f \fcm+3 cm x3\f \f \f \f = lim m→∞ |x| 3 (m + 3)(m + 2) = 0 < 1. R = ∞. Solutions near other points When looking for solutions near an ordinary point x0 ̸= 0, the calculations are generally simpler if we translate x0 to the origin by the change of variable t = x − x0. The solution to the resulting diﬀerential equation can be obtained by the method of power series near t = 0. The solution to the original equation is then obtained simply by performing the inverse translation. Example 2.2.4 To ﬁnd the solution of the diﬀerential equation in powers of (x − 1) of the ODE: y′′ − xy = 0 (2.13) 23 The point x0 = 1 is an ordinary point of the equation. The solution of the equation is written in the form: y = ∞∑ n=0 bn(x − 1)n (2.14) Let t = x − 1 and dt = dx. (2.15) Then the equation (2.13) becomes (we add t on the superscript of the second derivative to indicate that we derive with respect to the new variable t) y′′ t − (t + 1)y = 0, (2.16) and (2.16) becomes y = ∞∑ n=0 bnt n (2.17) Now y′ = ∞∑ n=1 nbnt n−1 and y′′ = ∞∑ n=2 n(n − 1)bnt n−2. Substituting into the equation: We obtain ∞∑ n=2 n(n − 1)bnt n−2 − (t + 1) ∞∑ n=0 bntn = 0 ∞∑ n=0(n + 1)(n + 2)bn+2tn − ∞∑ n=0 bntn − ∞∑ n=1 bn−1t n = 0. Then: (2 · 1b2 − b0) + ∞∑ n=1 [(n + 2)(n + 1)bn+2 − bn−1 − bn] tn = 0. Thus, { b2 = b0 2·1 bn+2 = 1 (n+2)(n+1) (bn−1 + bn) n ≥ 1. For n = 1 b3 = 1 3 · 2 (b0 + b1) = 1 3 · 2 b0 + 1 3 · 2b1 For n = 2 b4 = 1 4 · 3 (b1 + b2) = 1 4 · 3 · 2b0 + 1 4 · 3 b1 For n = 3 b5 = 1 5 · 4 (b2 + b3) = 4 5 · 4 · 3 · 2 · 1b0 + 1 5 · 4 · 3 · 2b1 24 Thus, y = ∞∑ n=0 bnt n = b0 + b1t + b2t 2 + b3t 3 + b4t 4 + b5t5 + . . . y = b0 + b1t + b2t 2 + ( 1 3 · 2b0 + 1 3 · 2 b1 ) t 3 + . . . y = b0 ( 1 + 1 2t 2 + 1 3 · 2 t 3 + . . .) + b1 ( t + 1 3 · 2t3 + . . .) Since t = x − 1, we get: y = b0 ( 1 + 1 2(x − 1) 2 + 1 3 · 2(x − 1) 3 + . . .) + b1 ( (x − 1) + 1 3 · 2 (x − 1) 3 + . . .) . 2.2.2 Non-homogeneous case Deﬁnition 2.2.2: Ordinary point and singular point Let y′′ + p(x)y′ + q(x)y = f (x). (2.18) We say that x0 is an ordinary point of equation (2.18) if p(x), q(x), and f (x) are all analytic at x0. Otherwise, x0 is a singular point. Theorem 2.2.2 Let x0 be an ordinary point of (2.18). Let R be the distance from x0 to the nearest singular point of (2.18). Then every solution of (2.18) can be represented by a power series: y = ∞∑ n=0 an(x − x0)n (2.19) that converges at least on the open interval (x0 − R, x0 + R). Furthermore, (2.19) will generate the two linearly independent solutions of the homogeneous part of (2.18) and a particular solution to the nonhomogeneous part of (2.18). Example 2.2.5 Find the power series in x for the general solution of y′′ + xy′ + y = 1 1 − x. (2.20) The functions p(x) = x, q(x) = 1 and f (x) = 1 1−x are analytic at x = 0. The series for 1 1−x is given by: 1 1 − x = ∞∑ n=0 xn, 25 which converges on (−1, 1). Since x = 0 is an ordinary point, Theorem 2.2.2 says we will generate both the homogeneous and the non-homogeneous solutions of (2.20) from: y = ∞∑ n=0 anxn, y′ = ∞∑ n=1 nanxn−1, y′′ = ∞∑ n=2 n(n − 1)anxn−2. Substituting these into (2.20), we get: ∞∑ n=2 n(n − 1)anxn−2 + x ∞∑ n=1 nanxn−1 + ∞∑ n=0 anxn = ∞∑ n=0 xn. Reindexing the sums, we obtain: 2a2 + ∞∑ n=1(n + 2)(n + 1)an+2xn + ∞∑ n=1 nanxn + a0 + ∞∑ n=1 anxn = 1 + ∞∑ n=1 xn. Collecting like terms: (2a2 + a0) + ∞∑ n=1 [(n + 2)(n + 1)an+2 + (n + 1)an] xn = 1 + ∞∑ n=1 xn. Equating coeﬃcients, we ﬁnd: a2 = 1 2(1 − a0), an+2 = − an n + 2 + 1 (n + 2)(n + 1) , n = 1, 2, 3, . . . Substituting n = 1, 2, 3, . . ., we ﬁnd: a3 = −1 3a1 + 1 6 , a4 = −1 4 a2 + 1 12 = 1 8a0 − 1 24, a5 = −1 5 a3 + 1 20 = 1 15a1 + 1 60, . . . Thus, the general solution is: y = a0 ( 1 − 1 2x2 + 1 8x4 + · · · ) +a1 ( x − 1 3x3 + 1 15x5 + · · · ) + ( 1 2x2 + 1 6 x3 − 1 24x4 + 1 60x5 + · · · ) . This is the power series in x for the general solution of Equation (2.20), deﬁned on (−1, 1). Example 2.2.6 Find a solution in series for the equation: y′′ − xy′ = e−x (2.21) We have: p(x) = −x and q(x) = 0 and f (x) = e−x, which are analytic at x0 = 0. Therefore, x0 = 0 is an ordinary point. Let 26 y = ∞∑ n=0 anxn y′ = ∞∑ n=1 nanxn−1 y′′ = ∞∑ n=2 n(n − 1)anxn−2 and since e −x = ∞∑ n=0 (−1)nxn n! . By substituting the series into equation (2.21), we obtain: ∞∑ n=2 n(n − 1)anxn−2 − ∞∑ n=1 nanxn = ∞∑ n=0 (−1) nxn n! . Thus, ∞∑ n=0(n + 2)(n + 1)an+2xn − ∞∑ n=1 nanxn = ∞∑ n=0 (−1) nxn n! . Now, 2 · 1a2x0 + ∞∑ n=1(n + 2)(n + 1)an+2xn − ∞∑ n=1 nanxn = 1 + ∞∑ n=1 (−1) nxn n! . We conclude that { a2 = 1 2 an+2 = (−1)n (n+2)! + n (n+2)(n+1) an, ∀n ≥ 1 For n = 1, a3 = − 1 3! + 1 3 · 2 a1, a4 = 1 4! + 2 4 · 3 · 2 = 3 4! For n = 3, a5 = − 1 5! + 3 5 · 4 a3 = − 1 5! + 3 5 · 4 [− 1 3! + 1 3 · 2a1 ] = − 4 5! + 3 5!a1 For n = 4, a6 = 13 6! For n = 5, a7 = −21 7! + 15 7! a1 27 Thus, y = ∞∑ n=0 anxn = a0 + a1x + a2x2 + a3x3 + a4x4 + a5x5 + . . . By substituting the values of an into y, we obtain the general solution of (2.21): y = a0 · 1 + a1 [ x + 1 3!x3 + 3 5!x5 + 15 7! x7 + . . .] + [1 2x2 − 1 3!x3 + 3 4!x4 − 4 5!x5 + . . .] . You can now check using the recurrence formula that R = ∞. 2.2.3 Series solutions at singular points In the last section, we saw how to deal with power series solutions at ordinary points. But the question is, how can we proceed at a singular point? Let us consider the homogeneous ODE: P (x)y′′ + Q(x)y′ + R(x)y = 0. (2.22) We observe the following: Remark 2.2.1 If in (2.22), P, Q and R are polynomials without common factors then singular points are points x0 at which P (x0) = 0. At singular points the solution is not necessarily analytic. Now, we deﬁne a regular singular Point about which a Taylor series will not work. Deﬁnition 2.2.3: Regular singular points (polynomial coeﬃcients) Consider the equation (2.22). If P, Q and R are polynomials and suppose P (x0) = 0 then x0 is a regular singular point if lim x→x0 (x − x0) Q(x) P (x) and lim x→x0 (x − x0) 2 R(x) P (x) are ﬁnite. Otherwise, x0 is an irregular singular point. Example 2.2.7 (a) Considering (1 − x2) y′′ − 2xy′ + 4y = 0, we have P (x) = 1 − x2, P (±1) = 0, Q(x) = −2x, R(x) = 4 and lim x→1 (x − 1) (−2x) (1 − x)(1 + x) = 1, lim x→1 (x − 1) 2 4 (1 + x)(1 − x) = 0. Hence, x = 1 is a regular singular point. (similarly for x = −1 ). (b) Considering the ODE: x3y′′ − y = 0, P (x) = x3 Q = 0 R = −1 and lim x→0 x2 ( −1 x3 ) = ∞. 28 Thus x = 0 is an irregular singular point. (c) Consider the ODE: 2(x − 2) 2xy′′ + 3xy′ + (x − 2)y = 0. We have singular points at x = 0 and at x = 2. x = 0 is a regular singular point and x = 2 is an irregular singular point. In the rest of the chapter, we will only consider regular singular points at x0 = 0, and in all other cases, a change of variable t = x − x0 will be applied, which will translate x0 to the origin. More general deﬁnition of a regular singular point Deﬁnition 2.2.4: Regular singular points (general case) If P, Q, and R are not limited to polynomials then consider P (x)y′′ + Q(x)y′ + R(x)y = 0, (2.23) or x2y′′ + x ( xQ(x) P (x) ) y′ + (x2R(x) P (x) ) y = 0 (2.24) x = 0 is a regular singular point if p(x) = ( xQ(x) P (x) ) and q(x) = ( x2R(x) P (x) ) are analytic at x = 0, i.e., p(x) = xQ(x) P (x) = p0 + p1x + · · · and q(x) = x2R(x) P (x) = q0 + q1x + · · · Example 2.2.8 Consider the diﬀerential equation x2y′′ + 2 (e x − 1) y′ + e−x cos xy = 0. Here, P (x) = x2, Q(x) = 2 (e x − 1) , R(x) = e−x cos x. x = 0 is a singular point. lim x→0 xQ P = lim x→0 x2 (e x − 1) x2 = lim x→0 2 (e x − 1) x 0 0= lim x→0 2e x 1 = 2 (L’Hopital) lim x→0 x2R P = lim x→0 x2 e −x cos x x2 = 1 Since the quotient functions p = xQ(x)/P (x) and q = x2R(x)/P (x) have Taylor Expansions about x = 0, x = 0 is a regular singular point. We note from Deﬁnition 2.2.3 that Ly = x2y′′ + xp0y′ + q0y + small as x→0 z }| { x {p1xy′ + q1y + · · · } = 0. (3.16) Then as x → 0, x2y′′ + xp0y′ + q0y ≈ 0 which is an Euler Equation which has solutions of the form y = xλ. Thus about a regular singular point we look for solutions of the form y = xλ ∞∑ n=0 anxn. 29 Important! If x0 = 0 is a regular singular point of the diﬀerential equation (2.23), then the equation has at least one solution of the form y = xλ ∞∑ n=0 anxn = ∞∑ n=0 anxn+λ, (2.25) where λ and an, (n = 0, 1, . . .) are constants. This solution is valid on any interval 0 < x < R, R is the radius of convergence of the series and is greater than or equal to the distance between x0 = 0 and the nearest singular point in the complex plane. In that case, our task is to determine: (a) λ (b) the coeﬃcients an (c) the radius of convergence R. Solution in power series near regular singular points (Frobenius method) In this section, I will provide an outline of the diﬀerent steps of the Frobenius method. For a complete analysis, refer to Chapter 5 of Boyce and DiPrima. Theorem 2.2.3: Fuchs’ Theorem Consider the ODE: P (x)y′′ + Q(x)y′ + R(x)y = 0, (2.26) Assume that x0 = 0 is a regular singular point. Step 1: Indicial equation λ(λ − 1) + bλ + c = 0, (2.27) where b = lim x→0 xQ(x) P (x) and c = lim x→0 x2 Q(x) P (x) . Step 2: First solution If λ1 > λ2 are roots of the indicial equation (2.48) then a solution of (2.45) is y1(x) = ∞∑ n=0 anxn+λ1 where a0 ̸= 0 ( and can be chosen to be equal to 1). To obtain an, we proceed as in the method of power series. The power series y(x) = ∞∑ n=0 anxn+λ = a0xλ + a1xλ+1 + a2xλ+2 + . . . + anxn+λ + . . . (2.28) and its derivatives given by y′(x) = ∞∑ n=0 an(n + λ)xn+λ−1 = λa0xλ−1 + (λ + 1)a1xλ + . . . + (λ + n)anxn+λ−1 + . . . (2.29) 30 and y′′(x) = ∞∑ n=0 an(n + λ)(n + λ − 1)xn+λ−2 = λ(λ − 1)a0xλ−2 + (λ + 1)λa1xλ−1 + . . . (2.30) are substituted into equation (2.45). The Frobenius method always gives a solution y1(x) = ∞∑ n=0 anxn+λ1, (2.31) and the general solution is y(x) = c1y1(x) + c2y2(x). (2.32) The method for obtaining the second solution depends on the relations between the two roots of the indicial equation. Step 3: Second solution The second linearly independent solution y2 is of the form: Case 1: If λ1 − λ2 is neither 0 nor a positive integer: y2(x) = ∞∑ n=0 bnxn+λ2 where we can choose b0 = 1 The coeﬃcients bn are obtained exactly as an but with λ = λ2. Case 2: If λ1 − λ2 = 0 : y2(x) = y1(x) ln x + ∞∑ n=1 bnxn+λ2 for some b1, b2... where y2 is obtained by assuming y1 = f (x, λ) and y2 = ∂f (x,λ) ∂λ \f \f \fλ=λ1. Case 3: If λ1 − λ2 is a positive integer: y2(x) = ay1(x) ln x + ∞∑ n=0 bnxn+λ2 where we can choose b0 = 1. To calculate this solution, ﬁrst try the Frobenius method with λ2. If we obtain a second solution, then this solution is y2(x) from the previous equation with a = 0. Otherwise, we get inﬁnite coeﬃcients. For this case, we proceed to calculate y2(x) using the following formula: y2 = ∂ ∂λ [(λ − λ2)y(λ, x)] \f \f \f \fλ=λ2 . Note The computations presented previously are tedious and heavy, and they may feel overwhelming. Just relax and move forward with the example to see how easy it is to implement. 31 Example 2.2.9 Solve the following diﬀerential equation: 4xy′′ + 2y′ + y = 0. (2.33) Let P (x) = 4x, Q(x) = 2 and R(x) = 1. Then P (0) = 0, and x0 = 0 is a singular point. Moreover, lim x→0 (x − 0) Q(x) P (x) = lim x→0 x 2 4x = 1 2 and lim x→0 (x − 0) 2 R(x) P (x) = lim x→0 x2 1 4x = lim x→0 1 4 x = 0. Therefore, x0 = 0 is a regular singular point. Next, we assume: y(x) = ∞∑ n=0 anxn+λ, y′(x) = ∞∑ n=0 an(n + λ)xn+λ−1, y′′(x) = ∞∑ n=0 an(n + λ)(n + λ − 1)xn+λ−2. Substituting these values into (2.33), we obtain: ∞∑ n=0 4an(n + λ)(n + λ − 1)xn+λ−1 + ∞∑ n=0 2an(n + λ)xn+λ−1 + ∞∑ n=0 anxn+λ = 0, i.e., ∞∑ n=0 2(2n + 2λ − 1)(n + λ)anxn+λ−1 + ∞∑ n=0 anxn+λ = 0. Let us consider in the ﬁrst sum, m + λ = n + λ − 1, i.e., m = n − 1 and n = m + 1. When n = 0, m = −1. In the second sum we just take n = m and we obtain ∞∑ m=−1 2(2m + 2λ + 1)(m + λ + 1)am+1xm+λ + ∞∑ m=0 amxm+λ = 0. We can now expand the ﬁrst term of the ﬁrst sum to match with the second sum. This way, we obtain 2(2λ − 1)λa0xλ−1 + ∞∑ m=0 [2(2m + 2λ + 1)(m + λ + 1)am+1 + am] xm+λ = 0. Hence, 2(2λ − 1)λa0 = 0 and 2(2m + 2λ + 1)(m + λ + 1)am+1 + am = 0, m = 0, 1, 2, . . .. Since a0 ̸= 0, we deduce that 2(2λ − 1)λ = 0, this implies λ = 0 or λ = 1 2. We set λ1 = 1 2 and λ2 = 0. On the other hand, we have the recurrence formula am+1 = −1 2(2m + 2λ + 1)(m + λ + 1) am, m = 0, 1, 2, . . . (2.34) Here, δ = λ1 − λ2 = 1 2 not an integer, so we are in the ﬁrst case. For λ = 1 2, we have: am+1 = −1 2(m + 1)(2m + 3) am, m = 0, 1, 2, . . . 32 For m = 0, a1 = − 1 3 × 2 a0 = − 1 3!a0 For m = 1, a2 = − 1 5 × 4 a1 = 1 5 × 4 × 3 × 2 a0 = 1 5!a0 For m = 2, a3 = − 1 7 × 6 a2 = − 1 7 × 6 × 5!a0 = − 1 7!a0 For m = 3, a4 = − 1 9 × 8 a3 = 1 9!a0 Thus, y1(x) = ∞∑ n=0 anxn+1/2 = a0x 1 2 + a1x 3 2 + a2x 5 2 + a3x 7 2 + a4x 9 2 + . . . By substituting the values of an into y1, we obtain the ﬁrst solution of (2.33): y1(x) = a0 [ x 1 2 − 1 3!x 3 2 + 1 5!x 5 2 − 1 7! x 7 2 + 1 9!x 9 2 + . . .] . For λ = 0, we have: am+1 = −1 2(2m + 1)(m + 1) am, m = 0, 1, 2, . . . For m = 0, a1 = −1 2a0 = − 1 2!a0 For m = 1, a2 = − 1 4 × 3a1 = 1 4 × 3 × 2!a0 = 1 4!a0 For m = 2, a3 = − 1 6 × 5 a2 = − 1 6 × 5 × 4!a0 = − 1 6!a0 For m = 3, a4 = − 1 8 × 7 a3 = 1 8!a0 Thus, 33 y2(x) = ∞∑ n=0 anxn+0 = a0 + a1x1 + a2x2 + a3x3 + a4x4 + . . . By substituting the values of an into y2, we obtain the second solution of (2.33): y2(x) = a0 [ 1 − 1 2!x + 1 4!x2 − 1 6! x3 + 1 8! x4 + . . .] . Therefore, the general solution of (2.33) is a linear combination of y1 and y2. Hence, the general solution of (2.33) is y(x) = c1y1(x) + c2y2(x), with c1 and c2constants. From the recurrence relation (2.34), we can deduce that R = ∞. We can further see that y(x) = c1 sin( √x) + c2 cos( √x) x > 0, where c1 and c2 are constants. Example 2.2.10 Solve the following diﬀerential equation: x2y′′ − xy′ + y = 0. (2.35) Let P (x) = x2, Q(x) = −x and R(x) = 1. Then P (0) = 0, and x0 = 0 is a singular point. Moreover, lim x→0 (x − 0) Q(x) P (x) = lim x→0 x(−x) x2 = −1 and lim x→0 (x − 0)2 R(x) P (x) = lim x→0 x2 1 x2 = 1. Therefore, x0 = 0 is a regular singular point. Next, we assume: y(x) = ∞∑ n=0 anxn+λ, y′(x) = ∞∑ n=0 an(n + λ)xn+λ−1, y′′(x) = ∞∑ n=0 an(n + λ)(n + λ − 1)xn+λ−2. Substituting these values into (2.35), we obtain: ∞∑ n=0 an(λ + n)(λ + n − 1)xλ+n − ∞∑ n=0(λ + n)anxλ+n + ∞∑ n=0 anxn+λ = 0, i.e., ∞∑ n=0 an(n + λ − 1) 2xn+λ = 0. Hence, an(n + λ − 1) 2 = 0, n = 0, 1, 2, . . . (2.36) For n = 0: (λ − 1)2a0 = 0 with a0 ̸= 0 34 Since a0 ̸= 0, we obtain: (λ − 1) 2 = 0. (2.37) The equation (2.37) is called the indicial equation. The roots of the equation are: λ1 = λ2 = 1. We see from the formula (2.36) that an = 0, n = 1, 2, . . . Here, δ = 0, so we are in the second case. Thus, the second solution will be of the form: y2(x) = y1(x) ln x + xλ1 ∞∑ n=0 bn (λ1) xn. We have: y1(x) = xλ1 ∞∑ n=0 anxn = a0xλ1 = a0x In particular, if we take a0 = 1, then y1(x) = x. To ﬁnd y2, we use the following formula: y2 = ∂y(x, λ) ∂λ \f \f \f \fλ=λ1=1 We have: y(x, λ) = xλ = eλ ln(x) Thus: ∂y(x, λ) ∂λ = ln(x)e λ ln x Therefore: y2 = x ln x Thus: y(x) = c1x + c2x ln x. Example 2.2.11 Solve the following diﬀerential equation: x2y′′ + x(1 − x)y′ − y = 0. (2.38) Let P (x) = x2, Q(x) = x(1 − x) and R(x) = −1. Then P (0) = 0, and x0 = 0 is a singular point. Moreover, lim x→0 (x − 0) Q(x) P (x) = lim x→0 x x(1 − x) x2 = 1 and lim x→0 (x − 0) 2 R(x) P (x) = lim x→0 x2 −1 x2 = −1. 35 Therefore, x0 = 0 is a regular singular point. Next, we assume: y(x) = ∞∑ n=0 anxn+λ, y′(x) = ∞∑ n=0 an(n + λ)xn+λ−1, y′′(x) = ∞∑ n=0 an(n + λ)(n + λ − 1)xn+λ−2. Substituting these values into (2.38), we get: ∞∑ n=0 an(n + λ)(n + λ − 1)xn+λ + ∞∑ n=0 an(n + λ)xn+λ − ∞∑ n=0 an(n + λ)xn+λ+1 − ∞∑ n=0 anxn+λ = 0, i.e., ∞∑ n=0[(n + λ)2 − 1]anxn+λ − ∞∑ n=0 an(n + λ)xn+λ+1 = 0. Let us consider in the ﬁrst sum, m + λ + 1 = n + λ, i.e., m = n − 1 and n = m + 1. When n = 0, m = −1. In the second sum we just take n = m and we obtain ∞∑ m=−1 [(m + λ + 1)2 − 1]am+1xm+λ+1 − ∞∑ m=0 (m + λ)amxm+λ+1 = 0. We can now expand the ﬁrst term of the ﬁrst sum to match with the second sum. This way, we obtain (λ 2 − 1)a0xλ + ∞∑ m=0 [(m + λ + 2)(m + λ)am+1 − (m + λ)am] xm+λ+1 = 0. ∞∑ n=0 an(λ + n + 1)(λ + n − 1)xλ+n − ∞∑ n=1(λ + n − 1)an−1xλ+n = 0. Finally, we obtain the indicial equation: λ 2 − 1 = 0 with roots λ1 = +1 and λ2 = −1. And the recurrence relation: am+1 = am m + λ + 1 , m = 0, 1, . . . (2.39) Here, we have δ = λ1 − λ2 = 2 ∈ Z +, so we are in the case 3. Using the recurrence formula (2.39) for λ1 = 1, we obtain y1(x) = xλ1 ∞∑ n=0 an(λ1)xn = x (1 + 1 3 x + 1 3 · 4 x2 + 1 3 · 4 · 5 x3 + . . .) Moreover, using the recurrence formula (2.39) for λ2 = −1, we obtain y2(x) = xλ2 ∞∑ n=0 an(λ2)xn = x−1 ( 1 + x + 1 2x2 + 1 3 · 2x3 + 1 3 · 4 x4 + . . .) Finally, the general solution is: y = c1y1(x) + c2y2(x), where c1 and c2 are constants. In this case also, R = ∞. 36 We conclude this chapter by introducing the Frobenius series solution of the Bessel equation. These equations arise during the process of separation of variables in problems with radial or cylindrical symmetry. Depending on the parameter ν in Bessel’s equation, the roots of the indicial equation can be distinct and real, repeated, or diﬀer by an integer. 2.3 Bessel functions 2.3.1 Bessel’s function of order ν /∈ {. . . , −2, −1, 0, 1, 2 . . .} We consider the following class of ordinary diﬀerential equations: Ly = x2y′′ + xy′ + ( x2 − ν2) y = 0. (2.40) We can easily check that x = 0 is a regular singular point: therefore let us assume that y(x) = ∞∑ n=0 anxn+r, substituting this into the diﬀerential equation (2.40) we have ∞∑ n=0 [ (r + n)(r + n − 1) + (r + n) − ν2] anxn+r + ∞∑ n=0 anxn+r+2 = 0 and reindexing the last sum (m = n + 2, n = m − 2, n = 0 ⇒ m = 2) and the ﬁrst sum (n = m), we have ∞∑ m=0 [ (r + m)(r + m − 1) + (r + m) − ν2] amxr+m + ∞∑ m=2 am−2xr+m = 0 Therefore, ∞∑ m=0 [(r + m) 2 − ν2] amxr+m + ∞∑ m=2 am−2xr+m = 0 that is, (r2 − ν2) a0xr + [(r + 1)2 − ν2] a1xr+1 + ∞∑ m=2 {[(r + m) 2 − ν2] am + am−2} xr+m = 0. This must be an identity in x, hence, all coeﬃcients must vanish, so that (r2 − ν2) a0 = 0 [(r + 1)2 − ν2] a1 = 0 [ (r + m)2 − ν2] am + am−2 = 0, m ≥ 2. Since a0 ̸= 0, we get the indicial equation from the ﬁrst term r2 − ν2 = 0 ⇒ r = ±ν. (2.41) Also, from the second equation, we must have a1 = 0 provided ν ̸= −1 2 and if ν = −1 2 then a1 is arbitrary. (2.42) 37 Finally, we get the recurrence relation am = − am−2 (r + m)2 − ν2 , m ≥ 2. For r = ν : am = − am−2 (m + ν)2 − ν2 = − am−2 m2 + 2mν = − am−2 m(m + 2ν), m ≥ 2. n = 2 a2 = − a0 2(2 + 2ν) = − a0 22(1 + ν), n = 4 a4 = − a2 4(4 + 2ν) = (−1) 2a0 2.24(2 + ν)(1 + ν). Therefore, a2m = (−1)ma0 m!22m(1 + ν) . . . (m + ν). Hence, y1(x) = xν ∞∑ m=0 (−1) m(x/2) 2m m!(1 + ν)(2 + ν) . . . (m + ν) x→0 −−→ 0. For r = −ν : am = − am−2 m(m − 2ν), m ≥ 2. n = 2 a2 = − a0 2(2 − 2ν) = − a0 22(1 − ν) n = 4 a4 = − a2 4(4 − 2ν) = (−1) 2a0 224(1 − ν)(2 − ν) Therefore, a2m = (−1)ma0 m!22m(1 − ν) . . . (m − ν). Hence, y2(x) = x−ν ∞∑ m=0 (−1) m(x/2) 2m m!(1 − ν) . . . (m − ν) x→0 −−→ ∞. 2.3.2 Bessel’s function of order ν = 0: repeated roots In this case Ly = x2y + xy′ + x2y = 0. 38 y(x) = ∞∑ n=0 anxn+r Ly = ∞∑ n=0 an{(n + r)(n + r − 1) + (n + r)}xn+r + ∞∑ n=0 anxn+r+2 = 0. Let us take m = n + 2 in the ﬁrst sum i.e, n = m − 2. We obtain ∞∑ n=2 [an(n + r) 2 + an−2] xn+r + a0[r(r − 1) + r]xr + a1[(r + 1)r + r + 1]xr+1 = 0. r2a0 = 0 [(r + 1)r + r + 1] a1 = 0 (r + n) 2an + an−2 = 0, n ≥ 2. Since a0 ̸= 0, we get the indicial equation from the ﬁrst term r2 = 0 ⇒ r = 0 (double root). (2.43) Also, from the second equation, we must have a1 = 0. (2.44) Finally, we get the recurrence relation an = −an−2 n2 , n ≥ 2. Note that an = 0 for n = 3, 5, . . .. a2 = −a0 22 ; a4 = −a2 42 = a0 2242 ; a6 = −a4 62 = − a0 224262 ; a8 = a0 22426282 a2m = (−1) m 22m(m!)2 a0. Hence, y1(x) = { 1 + ∞∑ m=1 (−1) mx2m 22m(m!)2 } = J0(x). To get a second solution y(x, r) =a0xr { 1 − x2 (2 + r)2 + x4 (2 + r)2(4 + r)2 + · · · + (−1)mx2m (2 + r)2(4 + r)2 . . . (2m + r)2 + · · · } . Hence, ∂y ∂r (x, r) \f \f \f \fr=r1 = a0 ln xy1(x) + a0xr ∞∑ m=1 (−1) mx2m ∂ ∂r { 1 (2 + r)2 . . . (2m + r)2 } . Let a2m(r) = 1 (2 + r)2 . . . (2m + r)2 ⇒ ln a2m(r) = −2 ln(2 + r) − . . . − 2 ln(2m + r). 39 Figure 2.1: Zeroth order Bessel functions j0(x) and Y0(x) This implies that a′ 2m(0) = ( − 2 2 + r − 2 4 + r · · · − 2 (2m + r) )\f \f \f \fr=0 a2m(0) = (−1 − 1 2 − . . . − 1 m ) a2m(0) = −Hma2m(0), Where Hm = 1 + 1 2 + · · · + 1 m . Therefore, y2(x) = J0(x) ln x + ∞∑ m=1 (−1) m+1Hm 22m(m!)2 x2m x > 0. It is conventional to deﬁne Y0(x) = 2 π [y2(x) + (γ − ln 2)J0(x)] . where γ = lim n→∞ (Hn − ln n) = 0.5772 Euler’s Constant . Finally y(x) = c1J0(x) + c2Y0(x). 2.3.3 Bessel’s Function of Order ν = 1 2 : Consider the case ν = 1 2 : Ly = x2y′′ + xy′ + ( x2 − 1 4) y = 0. Let 40 y = ∞∑ n=0 anxn+r Ly = ∞∑ n=0 an {(n + r) 2 − 1 4 } xn+r + ∞∑ n=0 anxn+r+2 = 0, (m = n + 2, n = m − 2; n = 0 → m = 2) Ly = a0 { r2 − 1 4 } xr + a1 { (r + 1)2 − 1 4 } xr+1 + ∞∑ n=2 [ an { (n + r) 2 − 1 4 } + an−2 ] xn+r = 0. (r2 − 1 4 )a0 = 0 [(r + 1)2 − 1 4 ] a1 = 0 an { (n + r) 2 − 1 4 } + an−2 = 0, n ≥ 2. Indicial Equation: r2 − 1 4 = 0, r = ± 1 2. Roots diﬀer by an integer. Recurrence: an = − an−2 (n + r)2 − 1 4 n ≥ 2. For r1 = + 1 2 : an = − an−2 (n + 1 2)2 − 1 4 = − an−2 (n + 1)n n ≥ 2 ( 9 4 − 1 4) a1 = 0 ⇒ a1 = 0, and using the recurrence formula, an = 0 for n = 3, 5, . . . a2 = − a0 3.2 a4 = (−1) 2a0 5 · 4.3 · 2 . . . a2n = (−1) na0 (2n + 1)! . Hence, the ﬁrst solution is y1(x) = x 1 2 ∞∑ n=0 (−1) nx2n (2n + 1)! = x− 1 2 ∞∑ n=0 (−1)nx2n+1 (2n + 1)! = x− 1 2 sin x. For r2 = − 1 2 : an = − an−2 ( n − 1 2)2 − 1 4 = − an−2 n(n − 1), n ≥ 2, n = 1 ⇒ a1 {( −1 2 + 1)2 − 1 4 } = a1.0 = 0 a1 and a0 arbitrary. a0 : a2 = − a0 2.1 a4 = (−1) 2a0 4.3.2.1 . . . a2n = (−1) na0 (2n)! . a1 : 41 a3 = − a1 3.2 a5 = (−1) 2a1 5 · 4.3 · 2 a2n+1 = (−1) na1 (2n + 1)! (5.20) y2(x) = a0x− 1 2 ∞∑ n=0 (−1) nx2n (2n)! + a1x− 1 2 ∞∑ n=0 (−1)nx2n+1 (2n + 1)! =a0x− 1 2 cos x + a1x− 1 2 sin x ↖ included in y1(x). y(x) = c1y1(x) + c2y2(x), where c1 and c2 are constants. 42 Method: Review power series methods Review power series methods Consider the ODE: P (x)y′′ + Q(x)y′ + R(x)y = 0, (2.45) which can be rewritten as y′′ + p(x)y′ + q(x)y = 0, (2.46) where p(x) = Q(x)/P (x) and q(x) = R(x)/P (x). Ordinary point If p and q are analytic at x0 = 0, then we say that x0 = 0 is an ordinary point. In that case we can ﬁnd a general solution of (2.45) of the form y(x) = ∞∑ n=0 anxn, where an are obtained by substituting y into the equation (2.45). Regular singular point We rewrite the equation (2.45) on the form x2y′′ + x ( xQ(x) P (x) ) y′ + (x2R(x) P (x) ) y = 0 (2.47) x0 = 0 is a regular singular point if p(x) = ( xQ(x) P (x) ) and q(x) = ( x2R(x) P (x) ) are analytic at x = 0. In that case we proceed as follows to solve (2.45): Step 1: Indicial equation λ(λ − 1) + bλ + c = 0, (2.48) where b = lim x→0 x Q(x) P (x) and c = lim x→0 x2 Q(x) P (x) . Step 2: First solution If λ1 > λ2 are roots of the indicial equation (2.48) then a solution of (2.45) is y1(x) = ∞∑ n=0 anxn+λ1 where a0 ̸= 0 ( and can be chosen to be equal to 1). To obtain an, we proceed as in the method of power series with y(x) = ∞∑ n=0 anxn+λ = a0xλ + a1xλ+1 + a2xλ+2 + . . . + anxn+λ + . . . (2.49) 43 The Frobenius method always gives a solution y1(x) = ∞∑ n=0 anxn+λ1, (2.50) and the general solution is y(x) = c1y1(x) + c2y2(x). (2.51) The method for obtaining the second solution depends on the relations between the two roots of the indicial equation. Note Note that after substituting y(x) given in (2.49) into (2.45), you will obtain a recurrence formula depending on λ. To obtain y1, you just have to replace λ = λ1 into the recurrence formula and proceed as the case of classical power series method. Step 3: Second solution The second linearly independent solution y2 is of the form: Case 1: If λ1 − λ2 is neither 0 nor a positive integer: y2(x) = ∞∑ n=0 bnxn+λ2 where we can choose b0 = 1 The coeﬃcients bn are obtained from the recurrence formula exactly as for an but with λ = λ2. Case 2: If λ1 − λ2 = 0 : y2(x) = y1(x) ln x + ∞∑ n=1 bnxn+λ2 for some b1, b2... The coeﬃcients bn are obtained from the recurrence formula exactly as for an but with λ = λ2. Case 3: If λ1 − λ2 is a positive integer and from the recurrence formula, it is possible to compute the coeﬃcients for λ = λ2 (i.e. the coeﬃcients do not blow-up for some values of n): y2(x) = ∞∑ n=0 bnxn+λ2 where we can choose b0 = 1. The coeﬃcients bn are obtained from the recurrence formula exactly as for an but with λ = λ2. Case 4: If λ1−λ2 is a positive integer and from the recurrence formula we get inﬁnite coeﬃcients for λ = λ2 (i.e. the coeﬃcients do blow-up for some values of n): For this case, we proceed to calculate y2(x) using the following formula: y2 = ∂ ∂λ [(λ − λ2)y(λ, x)] \f \f \f \fλ=λ2 . 44 Chapter 3 Introduction to partial diﬀerential equations 3.1 Introduction This chapter introduces some basic partial diﬀerential equations (PDEs). A partial diﬀerential equa- tion is a mathematical equation in which the unknown is a function of several variables and involves the partial derivatives of this function with respect to those variables. For example, one might want to determine the temperature at a speciﬁc point in space over time. In this case, the unknown function represents the temperature, and the PDE involves its partial derivatives with respect to time and spatial variables. Partial diﬀerential equations appear in many models in physics, engineering, or biology, such as the propagation of heat or sound, ﬂuid ﬂow, electrodynamics, and the spread of epidemics. They are also used in weather forecasting models and climate models. 3.2 Classiﬁcation of PDEs In the previous chapters, we discussed linear ordinary diﬀerential equations (ODEs). We saw that these are equations that deﬁne functions of a single independent variable by establishing a relationship between the values of the function and its derivatives. Now, let us give an example of nonlinear ODE Example 3.2.1: Examples of nonlinear ODEs Some examples of nonlinear ODEs are given by: 1. x2y′(x) + 2xy(x) = y2(x) (ﬁrst order); 2. y′′(x) + e y(x) = 0 (second order). PDEs involve multivariable functions u(x, t), u(x, y) that are determined by prescribing a rela- tionship between the function value and its partial derivatives. Deﬁnition 3.2.1 The order of a PDE is deﬁned as the order of the highest partial derivative occurring in the equation. A PDE is said to be linear if the dependent variable and its partial derivatives occur only in 45 the ﬁrst degree and are not multiplied, otherwise it is said to be non-linear. In the remainder of the chapter, we will occasionally use ux to denote ∂u ∂x , uxx to denote ∂2u ∂x2 , and uxy to denote ∂2u ∂x∂y = ∂ ∂x ( ∂u ∂y ). Example 3.2.2 Let u be an unknown function dependent on x, y. 1. Linear ﬁrst order PDE: a(x, y)ux+b(x, y)uy +c(x, y)u = d(x, y), a and b are not identically 0; 2. Second order linear PDE: Auxx +Buxy +Cuyy +Dux +Euy +F u = G; A, B, C, D, E, F, G : Constants or functions of x, y; A, B, C not identically 0. If G = 0 the PDE is homoge- neous, if G ̸= 0 the PDE is non homogeneous. Analogous to characterizing quadratic equations AX 2 + BXY + CY 2 + DX + EY = k, as either hyperbolic, parabolic, or elliptic, determined by the discriminant: ∆ = B2 − 4AC, we do the same for partial diﬀerential equations (PDEs). This brings us to the following classiﬁcation. ∆ Type of PDE Quadric (Analogous) Example of PDE PDE Nature ∆ > 0 Hyperbolic T 2 − c2X 2 = k utt = c2uxx Wave equation ∆ = 0 Parabolic T = X 2 ut = uxx Heat equation/ diﬀusion equation ∆ < 0 Elliptic X 2 + Y 2 = k uxx + uyy = f Laplace’s equation if f = 0 Poisson equation iff ̸= 0 All linear and second-order PDEs can be transformed into one of these types. 3.3 A one dimensional conservation law Assume we are looking at the traﬃc ﬂow at a length ∆x of a highway. We denote by u(x, t) the density of cars at position x at time t. [u] = number of cars /unit length. Let q(x, t) be the ﬂux of cars at position x at time t. [q] = number of cars /unit time. x x + ∆x q(x, t) ﬂux in q(x + ∆x, t) ﬂux out u(x, t) u(x + ∆x, t) number of cars: u(x, t)∆x ∆x Figure 3.1: Traﬃc ﬂow along the x axis with density u(x, t) and ﬂux q(x, t) at x and time t. The conservation law tells us that the change in the number of cars over [t, t + ∆t] is equal to: number of cars in − number of cars out. 46 That is u(x, t + ∆t)∆x − u(x, t)∆x = q(x, t)∆t − q(x + ∆x, t)∆t. (3.1) Check dimensions: # cars L · L = # cars T · T. Divide (3.1) by ∆t · ∆x : u(x, t + ∆t) − u(x, t) ∆t = q(x, t) − q(x + ∆x, t) ∆x Now let ∆t → 0, ∆x → 0 : ∂u ∂t = − ∂q ∂x or ∂u ∂t + ∂q ∂x = 0. (3.2) This is a conservation law PDE. In this equation, u(x, t) and q(x, t) are both unknowns and to be able to solve this PDE we need to know how q is related to u. This information comes from the nature of the problem. For instance it can be: an equation of state (thermodynamics) or a constitutive relation (continuum mechanics). Linear ﬂux density relationship Assume that the ﬂux of cars q increases linearly with the density of cars u, i.e., q = cu, c > 0, then it follows that ∂u ∂t + c ∂u ∂x = 0. (3.3) Since the PDE has constant coeﬃcients and is a linear combination of time and spatial partial derivatives, we might expect to ﬁnd a solution of the form of an exponential of a linear function of x and t, since either derivative of such a function is in the form of a constant times the exponential. We therefore consider the trial solution of the form: u(x, t) = eikx+σt. (3.4) Substituting (3.4) into (3.3), we obtain ( ∂ ∂t + c ∂ ∂x ) eikx+σt = (σ + ikc)e ikx+σt which is a solution of (3.3) provided σ and k satisfy the following “dispersion relation” σ = −ikc So, the solution of (3.3) is u(x, t) = e ik(x−ct). You can show that any diﬀerentiable function f with the functional form f (x − ct) is a solution to (3.3): To see this, let u(x, t) = f (x − ct), then ut = −cf ′(x − ct) and ux = f ′(x − ct) and ut + cux = −cf ′ + cf ′ = 0. The PDE (3.3) with the solution u(x, t) = f (x − ct) can be interpreted as a right moving wave using the Galilean transformation (see Figure 3.2). The wave propagates in time to the right: • observer in blue, stationary, sees x; • observer in red, moving with the wave, sees x′ = x − ct. 47 Figure 3.2: The Galilean transformation of coordinates from x to x′ = x − ct [1]. Assume that the ﬂux of cars q decreases linearly with the density of cars u, q = −cu, c > 0, i.e. the wave is moving to the left. Then it follows that ∂u ∂t − c∂u ∂x = 0 (3.5) and the solution is u(x, t) = f (x + ct). The second order wave equation: Here, we consider a wave moving in both directions. If we apply the left ∂ ∂t − c ∂ ∂x and right ∂ ∂t + c ∂ ∂x moving wave operators in succession, we obtain ( ∂ ∂t + c ∂ ∂x ) ( ∂ ∂t − c ∂ ∂x ) u(x, t) = ∂2u ∂t2 − c2 ∂2u ∂x2 = 0, (3.6) which is the second order wave equation that has both left and right moving wave solutions. The convection-diﬀusion equation Consider the traﬃc ﬂowing down the highway as shown in Figure 3.1 and assume that the ﬂux q increases linearly with the car density u. Now what happens if drivers slow down if they see an increase in car density ahead of them? This situation can be represented by a ﬂux function of the form q = cu − Dux. (3.7) # cars T = L T · # Cars L − [D]# Cars L2 , so, D should have dimensions: [D] = L2 T . Combining (3.3) and (3.7) we obtain the convection-diﬀusion equation ut + cux| {z } Convection = Duxx. | {z } Diﬀusion (3.8) So, now in addition to moving at speed c to the right, the wave diﬀuses too, with a diﬀusion coeﬃcient D. Now, if you make a change of variable: z = x − ct, i.e., you move with the center of the wave, and ﬁnd the PDE for U (z) : Ut = DUzz. (3.9) 48 This means the observer that travels with the wave only sees the diﬀusion. Finding the dispersion relation for the Convection-diﬀusion equation Consider a solution: u(x, t) = e ikx+σt. Substitute in: ∂u ∂t + c∂u ∂x = D ∂2u ∂x2 , to obtain (σ + ick)eikx+σt = ( −k2) Deikx+σt. Hence, σ = − ikC|{z} due to convection − k2D|{z} due to diﬀusion (the dispersion relation). Therefore, u(x, t) = eikx−ikct−k2Dt = eik(x−ct) | {z } right moving wave · e −k2Dt | {z } decay in time due to diﬀusion D > 0 3.4 The heat/diﬀusion equation Consider the heat conduction in a length ∆x of a conducting bar: x x + ∆x q(x, t) q(x + ∆x, t) u(x, t) u(x + ∆x, t) ∆x Figure 3.3: Heat conduction along the x axis. • u(x, t) : The temperature at location x, time t, and has units degrees Kelvin, [u] = K; • q(x, t) : The heat ﬂux, or the ﬂux of heat energy per unit area, [q] = J m2·S ; • C : The speciﬁc heat capacity. The amount of energy needed to increase the temperature of one kilogram of the material by one degree Kelvin: [C] = J kg·K (a material property); • ρ : Density of the material, [ρ] = kg m3 ; • A: The cross sectional area of the bar [A] = m 2. Now, let us write down the conservation of energy: The increase in the thermal energy of the bar with length ∆x = thermal energy in − thermal energy out. That is C · [u(x, t + ∆t) − u(x, t)] · ρ · ∆x · A = [q(x, t) − q(x + ∆x, t)]A · ∆t. (3.10) 49 Check the dimensions in equation above: J kg · K · K · kg m3 · m · m2 = J m2 · S · m 2 · S J = J Divide (3.10) by A∆x · ∆t : ρC [u(x, t + ∆t) − u(x, t)] ∆t = [q(x, t) − q(x + ∆x, t)] ∆x Let ∆t → 0 and ∆x → 0, we obtain ρC ∂u ∂t = − ∂q ∂x i.e., ρC ∂u ∂t + ∂q ∂x = 0 (the energy conservation PDE). (3.11) Now, we need to ﬁnd a constitutive relation between u and q. 1. Fourier’s law: Experimental evidence suggests that the ﬂux of heat is proportional to the negative of the spatial gradient of the temperature. This means that heat always ﬂows from higher temperature to lower temperature regions. In this case: q = −k ∂u ∂x , (3.12) where k is the thermal conductivity having dimensions [k] = J S·m·K = W m·K . Substituting (3.12) Figure 3.4: Fourier’s Law of heat Conduction [1]. into (3.11) and dividing by ρC we obtain the heat equation ∂u ∂t = α2 ∂2u ∂x2 (3.13) where α2 = k ρC is the diﬀusion coeﬃcient, which has dimensions [α2] = m2 S . 2. Fick’s law: The heat ﬂux is from regions of high Concentration of energy to regions of low concentration of energy. q = −α2 ∂(ρCu) ∂x , (3.14) Here, ρCu is the concentration of thermal energy, and has units: [ρCu] = kg m3 · J kg·K · K = J m3 . Substituting (3.14) into (3.11), we obtain ∂u ∂t = α2 ∂2u ∂x2 , (3.15) 50 where α2 is the diﬀusion coeﬃcient. A similar line of reasoning for the heat ﬂow in a conduction plate leads to the two dimensional Heat Equation: ∂u ∂t = α2 ( ∂2u ∂x2 + ∂2u ∂y2 ) . Another nice way of arriving at the diﬀusion equation: random walk see lecture 7 [1] of Prof. Peirce’s lectures. 3.5 The Wave Equation: Consider an elastic rod having a density ρ and cross-sectional area A, and let σ(x, t) be the pressure in the rod at x at time t and u(x, t) the displacement of the rod from its equilibrium position. x x + ∆x σ(x, t) σ(x + ∆x, t) u(x, t) u(x + ∆x, t) F • u(x, t) : displacement from equilibrium, [u] = m; • σ(x, t) : the normal stress [σ] = N m2·S ; • ρ : density, [ρ] = kg m3 ; • A: The cross sectional area of the bar [A] = m 2. Now, let us write down Newton second law (F = M a): [σ(x + ∆x, t) − σ(x, t)] · A | {z } net force = ρ · ∆x · A | {z } mass · ∂2u ∂t2 . |{z} acceleration (3.16) Divide (3.10) by A∆x : σ(x + ∆x, t) − σ(x, t) ∆x = ρ ∂2u ∂t2 Let ∆x → 0, we obtain ∂σ ∂x + ρ ∂2u ∂t2 = 0 (balance of linear momentum). (3.17) Now, we need a constitutive law that gives a relation between σ and u to solve the PDE. In order to have suﬃcient information to solve for the unknowns we need an additional equation, which is provided by a constitutive relation known as Hooke’s Law (see Figure 3.5). Experimental data characterizes the ”stiﬀness” of the material by the parameter E known as the Young’s Modulus, which provides a linear relationship between the stress to which the bar is subjected and the relative displacement ∆u ∆x = u(x+∆x,t)−u(x,t) ∆x ≈ ∂u ∂x := ϵ, or strain ϵ. 51 Figure 3.5: The stress on the bar σ is related to the strain ϵ by Hooke’s Law[1]. Substituting the stress strain relationship σ = E ∂u ∂x into (3.17), we obtain the second order wave equation ∂2u ∂t2 = ( E ρ ) ∂2u ∂x2 = c2 ∂2u ∂x2 , where c = √ E ρ . (3.18) A general form of the solution to this equation is u(x, t) = f (x − ct) + f (x + ct). 3.6 Laplace’s equation: Flow in porous media Consider the steady-state 2D ﬂow in porous media. • u: x component of velocity, [u] = m/S • v: y component of velocity, [V ] = m/S • ρ: density, [p] = kg/m 3 • q: mass ﬂux, [q] = kg/S The Conservation of mass tells us that the sum of ﬂuxes through all boundaries should be zero: We denote by l is a unit length in the y direction. The equation is: ρ[u(x + ∆x, y) − u(x, y)]∆y · l + ρ[v(x, y + ∆y) − v(x, y)]∆x · l = 0. (3.19) Check the dimensions of each term: 52 kg m3 · m S · m · m = kg S (3.20) This ensures that the unit of mass ﬂux is correct. Next, divide equation (3.19) by ρ∆x · ∆y · l: u(x + ∆x, y) − u(x, y) ∆x + v(x, y + ∆y) − v(x, y) ∆y = 0 Let ∆x → 0 and ∆y → 0. This leads to the continuity equation: ∂u ∂x + ∂v ∂y = 0 (3.21) Now, we need to ﬁnd a constitutive relation between u and v. For ﬂow in porous media, you use Darcy’s law as a constitutive law: u = −k ∂h ∂x , v = −k ∂h ∂y (3.22) where: • k: hydraulic conductivity, [k] = m S • h: hydraulic head, [h] = m So, Darcy’s law states that the ﬂow direction is from regions with higher hydraulic head to regions with lower hydraulic head. Note 3.6.1 Darcy’s law is sometimes stated as: u = − x µ ∂P ∂x , where where: • x: permeability, [x] = m 2 • µ: viscosity of ﬂuid, [µ] = Pa · S • P : pore pressure, [P ] = Pa Substituting u = −k ∂h ∂x and v = −k ∂h ∂y into the continuity equation (3.21) gives the 2D Laplace’s equation: ∂2h ∂x2 + ∂2h ∂y2 = 0. (3.23) 53 Chapter 4 Introduction to numerical methods for partial diﬀerential equations 4.1 Introduction Note that numerical methods always involve discretizing analytical problems into numerical problems, and there is an inﬁnite number of discretization methods for an equation. In this chapter, we introduce the ﬁnite diﬀerence method, which is widely used for approximating partial diﬀerential equations (PDEs) using a computer. The ﬁnite diﬀerence method is a common technique for ﬁnding approximate solutions to partial diﬀerential equations. It involves solving a system of relations (numerical scheme) that connects the values of unknown functions at points that are suﬃciently close to each other. At ﬁrst glance, this method seems to be the simplest to implement, as it proceeds in two steps: ﬁrst, the discretization of the diﬀerentiation operators using ﬁnite diﬀerences, and second, the convergence of the resulting numerical scheme as the distance between the points decreases. 4.2 Approximating the derivatives of a function by ﬁnite diﬀerences In previous courses, you have introduced the notion of the derivative for a diﬀerentiable function and the Taylor’s formula. Deﬁnition 4.2.1: Derivative of a function and Taylor’s formula Assume that the function f : R → R is diﬀerentiable, then f ′(x) = lim ∆x→0 f (x + ∆x) − f (x) ∆x . (4.1) If we assume that the function can be diﬀerentiated many times then Taylor’s formula is given by: f (x + ∆x) = f (x) + ∆xf ′(x) + ∆x2 2! f ′′(x) + ∆x3 3! f (3)(x) + . . . (4.2) or, with −∆x instead of +∆x : f (x − ∆x) = f (x) − ∆xf ′(x) + ∆x2 2! f ′′(x) − ∆x3 3! f (3)(x) + . . . (4.3) 54 Three basic types of ﬁnite diﬀerence methods are commonly considered: forward, backward, and central ﬁnite diﬀerences. Deﬁnition 4.2.2: Forward diﬀerence On a computer, derivatives are approximated by ﬁnite diﬀerence expressions; rearranging (4.2) gives the forward diﬀerence approximation f (x + ∆x) − f (x) ∆x = f ′(x) + O(∆x), (4.4) where O(∆x) means ’terms of order ∆x ’, ie. terms which have size similar to or smaller than ∆x when ∆x is small. Technically, a term or function E(∆x) is O(∆x) if lim ∆x→0 E(∆x) ∆x = constant. So the expression on the left approximates the derivative of f at x, and has an error of size ∆x; the approximation is said to be ’ﬁrst order accurate’. Deﬁnition 4.2.3: Backward diﬀerence Rearranging (4.3) similarly gives the backward diﬀerence approximation f (x) − f (x − ∆x) ∆x = f ′(x) + O(∆x), (4.5) which is also ﬁrst order accurate, since the error is of order ∆x. Deﬁnition 4.2.4: Central diﬀerence Combining (4.2) and (4.3) gives the central diﬀerence approximation f (x + ∆x) − f (x − ∆x) 2∆x = f ′(x) + O (∆x2) , (4.6) which is ’second order accurate’, because the error this time is of order ∆x2. Deﬁnition 4.2.5: Second derivative, central diﬀerence Adding (4.2) and (4.3) gives f (x + ∆x) + f (x − ∆x) = 2f (x) + ∆x2f ′′(x) + ∆x4 12 f (4)(x) + . . . (4.7) Rearranging this therefore gives the central diﬀerence approximation to the second derivative: f (x + ∆x) − 2f (x) + f (x − ∆x) ∆x2 = f ′′(x) + O ( ∆x2) , (4.8) which is second order accurate. 55 Note: How many boundary conditions are needed to solve a PDE? Typically, for a PDE, to obtain a unique solution, we need one condition (either boundary or initial) for each derivative in each variable. For instance: • The heat equation: ut = uxx, involves one time derivative and two spatial derivatives, meaning we require: – One initial condition (IC) – Two boundary conditions (BCs) • The wave equation: utt = uxx, involves two time derivatives and two spatial derivatives, meaning we require: – Two initial conditions (ICs) – Two boundary conditions (BCs) • Laplace’s equation: uxx + uyy = 0, involves two spatial derivatives in both the x and y directions, meaning we require: – Four boundary conditions (BCs) 4.3 Solving the heat equation using the method of ﬁnite diﬀerences 4.3.1 Dirichlet boundary conditions Method 4.3.1: Dirichlet boundary conditions To ﬁnd a numerical solution to the heat equation: ∂u ∂t = α2 ∂2u ∂x2 , 0 < x < L, t > 0 BC: u(0, t) = A, u(L, t) = B, IC: u(x, 0) = f (x), (4.9) we approximate the time derivative using forward diﬀerences, and the spatial derivative using central diﬀerences; u(x, t + ∆t) − u(x, t) ∆t = α2 u(x + ∆x, t) − 2u(x, t) + u(x − ∆x, t) ∆x2 + O ( ∆t, ∆x2) . (4.10) This approximation is second order accurate in space and ﬁrst order accurate in time. The use of the forward diﬀerence means the method is explicit, because it gives an explicit formula for u(x, t + ∆t) depending only on the values of u at time t. 56 Divide the interval 0 < x < L into N + 1 evenly spaced points, with spacing ∆x; ie. xn = n∆x, for n = 0, 1, . . . , N , and divide the time interval [0, T ] into M + 1 equal time levels tk = k∆t for k = 0, 1, . . . , M . Then seek the solution by ﬁnding the discrete values uk n = u (xn, tk) . From (4.10), these satisfy the equations u k+1 n − u k n ∆t = α2 u k n+1 − 2uk n + u k n−1 ∆x2 or, rearranging, u k+1 n = uk n + α2∆t ∆x2 ( uk n+1 − 2uk n + uk n−1) . (4.11) If the values of un at time step k are known, this formula gives all the values at time step k + 1, and it can then be iterated again and again. The initial condition gives u 0 n = f (xn) , (4.12) for all n ∈ {0, . . . , N }, and the boundary conditions require uk 0 = A, u k N = B, (4.13) for all k ∈ {0, . . . , M } (equation (4.11) only has to be solved for 1 ≤ n ≤ N − 1 ). Figure 4.1: Mesh points and ﬁnite diﬀerence stencil for the heat equation. Blue points are prescribed the initial condition, red points are prescribed by the boundary conditions. For Neumann boundary conditions, ﬁctional points at x = −∆x and x = L + ∆x can be used to facilitate the method. 57 4.3.2 Neumann boundary conditions Method 4.3.2: Neumann boundary conditions To apply ∂u ∂x (0, t) = C. (4.14) instead of u(0, t) = A in (4.9), notice that the central diﬀerence version of this boundary condition would be u(0 + ∆x, t) − u(0 − ∆x, t) 2∆x = C, (4.15) and therefore u(−∆x, t) = u(∆x, t) − 2∆xC ( i.e, uk −1 = u k 1 − 2∆xC). (4.16) x = −∆x is outside the domain of interest, but knowing the value of u(−∆x, t) there allows the discretised equation (4.11) to be used also for x = 0 (n = 0), and it becomes uk+1 0 = u k 0 + α2∆t ∆x2 ( 2u k 1 − 2u k 0 − 2∆xC) . (4.17) So, in this case we must solve (4.11) for 1 ≤ n ≤ N −1, and (4.17) for n = 0 at each time step. If there is a derivative condition at x = L the same procedure is followed and an equation similar to (4.17) must be solved for n = N too. A handy way to implement this type of boundary condition, which enables the same formula (4.11) to be used for all points, is to introduce ’ﬁctional’ mesh points for n = −1 and n = N + 1, and to prescribe the value uk −1 given by (4.16) (or the equivalent for u k N +1 ) at those points. Stability Theorem 4.3.1: Stability condition Note that, this method will only be stable, provided the condition α2∆t ∆x2 ≤ 1 2 (4.18) is satisﬁed; otherwise it will not work. 4.4 Solving the Wave equation using the method of ﬁnite diﬀerences 4.4.1 Dirichlet boundary conditions 58 Method 4.4.1: Dirichlet boundary conditions For the wave equation, ∂2u ∂t2 = c2 ∂2u ∂x2 , 0 < x < L, t > 0 BC: u(0, t) = 0, u(L, t) = 0, IC: u(x, 0) = f (x), ∂u ∂t (x, 0) = g(x), (4.19) discretise x into N + 1 evenly spaced mesh points xn = n∆x, discretise the time interval [0, T ] into M + 1 equal time levels tk = k∆t for k = 0, 1, . . . , M , and seek the solution at these mesh points; u k n = u (xn, tk). Using central diﬀerence approximations to the two second derivatives, the discrete equation is u k+1 n − 2u k n + uk−1 n ∆t2 = c2 u k n+1 − 2uk n + n k n−1 ∆x2 + O ( ∆x2, ∆t 2) . (4.20) This can be rearranged to give uk+1 n = 2u k n − uk−1 n + c 2∆t 2 ∆x2 (u k n+1 − 2u k n + n k n−1) (4.21) which gives an explicit method to calculate u k+1 n in terms of the values at the previous two time steps k and k − 1. This method is second order accurate in space and time - it is sometimes referred to as the ’leap-frog’ method. To apply Dirichlet boundary conditions given in (4.19), the values of u k+1 0 and u k+1 N are simply prescribed to be 0; there is no need to solve an equation for these end points. 4.4.2 Neumann Boundary conditions Method 4.4.2: Neumann boundary conditions If the boundary conditions are Neumann, on the other hand: ∂u ∂x (0, t) = 0, ∂u ∂x (L, t) = 0 (4.22) then (4.21) can still be solved for n = 0 and n = N if we use the central diﬀerence approxi- mations to the boundary conditions in order to determine the ’ﬁctional’ values uk −1 and uk N +1 respectively. At x = 0, for instance, the discretised condition (4.22) is uk −1 − uk 1 2∆x = 0 (28) and therefore uk −1 = u k 1. Similarly u k N +1 = u k N −1. 59 Figure 4.2: Mesh points and ﬁnite diﬀerence stencil for the wave equation. 4.4.3 Initial conditions Method 4.4.3: Initial conditions To apply the initial conditions, note that to use (4.21) for the ﬁrst time step k = 1, we need to know the values of u0 n and also u−1 n . The values of u0 n follow from the initial condition on u(x, 0); u 0 n = f (xn) . (4.23) The values of u−1 n come from considering the central diﬀerence approximation to the derivative condition given in (4.19); u(x, 0 + ∆t) − u(x, 0 − ∆t) 2∆t = g(x), from which we deduce that u −1 n = u1 n − 2∆tg (xn) . Substituting this into the discrete equation (4.21), and rearranging, gives the formula u1 n = u0 n + 1 2 c2∆t 2 ∆x2 (u 0 n+1 − 2u 0 n + n 0 n−1) + ∆tg (xn) (32) for the ﬁrst time step. Once the solution has been initialised in this way, all subsequent time steps can be made using (4.21). 60 Stability Theorem 4.4.1: Stability condition This method is stable provided c∆t ∆x ≤ 1 (33) which is often called the Courant-Friedrichs-Levy (or CFL) condition. 4.5 Solving the Laplace’s equation using the method of ﬁnite diﬀerences 4.5.1 Dirichlet boundary conditions Method 4.5.1: Dirichlet boundary conditions Laplace’s equation on a square domain is ∂2u ∂x2 + ∂2u ∂y2 = 0, 0 < x < L, 0 < y < L (34) with boundary conditions, u(0, y) = 0, u(L, y) = 0, u(x, 0) = f (x), u(x, L) = 0 (35) Choose a mesh with spacing ∆x in the x direction, ∆y in the y direction (often it is sensible to choose ∆x = ∆y ), so grid points are xn = n∆x for n = 0, 1, . . . N , and ym = m∆y for m = 0, 1, . . . M . Then seek solution values unm = u (xn, ym). Using second order accurate central diﬀerences for the two derivatives, the discrete equation is un+1m − 2unm + un−1m ∆x2 + unm+1 − 2unm + unm−1 ∆y2 = O ( ∆x2, ∆y2) (36) and if ∆x = ∆y this simpliﬁes to unm = 1 4 (un+1m + un−1m + unm+1 + unm−1) . (4.24) Note this equation shows that the solution has the property that the value at each point (xn, ym) is the average of the values at its four neighbouring points. This is an important property of Laplace’s equation. The boundary conditions (35) determine the values unm on each of the four boundaries, so (4.24) does not have to be solved at these mesh points: u0m = 0, uN m = 0, un0 = f (xn) , unM = 0. (4.25) 61 Note 4.5.1: Neumann boundary conditions Neumann boundary conditions can be incorporated by calculating values for u−1m (for instance), as for the heat equation. Figure 4.3: Mesh points and ﬁnite diﬀerence stencil for Laplace’s equation. Jacobi Iteration Note that (4.24) is not an explicit formula, since the solution at each point depends on the unknown values at other points, and it is therefore harder to solve than the previous explicit methods. One way to do it, however, is to take a guess at the solution (eg. unm = 0 everywhere), and then go through each of the mesh points updating the solution according to (4.24) by taking the values of the previous guess on the right hand side. Iterating this process many times, the successive approximations will hopefully change by less and less, and the values they converge to provide the solution. Given an initial guess u (0) nm, the successive iterations u(1) nm, . . . , u(k) nm, u (k+1) nm , . . ., are given by u(k+1) nm = 1 4 ( u(k) n+1m + u(k) n−1m + u(k) nm+1 + u(k) nm−1) . (39) Here the superscript ( k ) denotes the values for the k th iteration. The process is continued until the change between successive iterations is less than some desired tolerance. 62 Chapter 5 Fourier series and separation of variables 5.1 Introduction In the previous chapter, we introduced the ﬁnite diﬀerence method as a numerical approach for solving a speciﬁc class of linear partial diﬀerential equations (PDEs). In this chapter, our goal is to solve these same PDEs analytically. We recall that a PDE is said to be linear if the dependent variable and its derivatives appear at most to the ﬁrst power and in no functions. To achieve this, we will employ the method of separation of variables, which involves transforming the PDE into a system of ordinary diﬀerential equations (ODEs). General idea of the separation of variables The idea of separation of variables is quite simple. Assume that you have a linear PDE along with some boundary and/or initial conditions. For clarity, let us assume that the PDE is linear and homogeneous, and that the boundary conditions are also linear and homogeneous. Assume also that the equation involves two variables: the ﬁrst, referred to as time t, and the second, referred to as space x. Method 5.1.1: Idea of the separation of variables The method of separation of variables can be broken down into three steps: Step 1: Find nonzero solutions of the PDE which have a product form u(x, t) = X(x)T (t). Step 2: Select from among the solutions found in Step 1 those solutions which satisfy the BC. There will typically be an inﬁnite sequence of these: un(x, t) = Xn(x)Tn(t), n = 1, 2, . . . Step 3: Observe that, because the PDE and BC are linear and homogeneous, any linear combination of solutions of these will again be a solution. Thus for any choice of coeﬃcients b1, b2, . . . the linear combination u(x, t) = ∞∑ n=1 bnun(x, t) 63 will again be a solution of the PDE and BC (assuming the series converges). Choose the constants bn so that u(x, t) satisﬁes the initial condition. 5.2 Solution to the heat equation by separation of variables Now, let us apply the previous method to the heat (diﬀusion) equation. We recall that the equation is given by: ut(x, t) = α2uxx(x, t), 0 < x < L, t > 0. (5.1) We consider the heat equation (5.1) subject to the following initial condition: u(x, 0) = f (x). (5.2) 5.2.1 Dirichlet boundary conditions: Fourier sine series Consider the heat conduction in an insulated rod whose endpoints are held at zero degrees for all time and within which the initial temperature is given by f (x) as shown in Figure 5.1. Figure 5.1: Consider a conducting bar with thermal conductivity α2 that has an initial temperature distribution u(x, 0) = f (x) and whose endpoints are maintained at 0 ◦C, i.e. embedded in ice [1]. The system of equations is given by: ut = α2uxx, 0 < x < L, t > 0 BC: u(0, t) = 0, u(L, t) = 0, IC: u(x, 0) = f (x), (5.3) We assume a solution of the form: u(x, t) = X(x)T (t). (5.4) Diﬀerentiating both sides: ut = X(x) · ˙T (t), uxx = X ′′(x) · T (t). 64 Substituting these into the heat equation (5.3): X(x) · ˙T (t) = α2X ′′(x) · T (t). (5.5) Dividing by α2X(x)T (t): ˙T (t) α2T (t) = X ′′(x) X(x) = λ. (5.6) Since both sides depend on diﬀerent variables, they must equal a constant, denoted as λ. This gives two ordinary diﬀerential equations (ODEs): Time equation: ˙T (t) = λα2T (t). Solving this gives: T (t) = Ce λα2t. (5.7) Space equation: an eigenvalue problem X ′′(x) = λX(x), X(0) = 0 = X(L). (5.8) An obvious solution of (5.8) is X = 0. This is a trivial solution. Can we ﬁnd nontrivial solutions? The nature of X(x) depends on λ. Case 1: λ > 0 Let λ = µ2, then: X ′′ − µ 2X = 0. The general solution is (we have chosen this form for simpliﬁcations, it is also ﬁne to use the expo- nential form): X(x) = A sinh(µx) + B cosh(µx). (5.9) Applying boundary conditions: X(0) = 0 ⇒ B = 0, X(L) = 0 ⇒ A sinh(µL) = 0. (5.10) Since sinh(µL) ̸= 0, we must have A = 0, leading to the trivial solution. Case 2: λ = 0 The equation simpliﬁes to: X ′′(x) = 0 ⇒ X(x) = Ax + B. Applying boundary conditions: X(0) = 0 ⇒ B = 0, X(L) = 0 ⇒ AL = 0 ⇒ A = 0. (5.11) Again, we get the trivial solution. 65 Case 3: λ < 0 Let λ = −µ2, then: X ′′ + µ2X = 0. The general solution is: X(x) = A sin(µx) + B cos(µx). (5.12) Applying boundary conditions: X(0) = 0 ⇒ B = 0, X(L) = 0 ⇒ A sin(µL) = 0. For a nontrivial solution (A ̸= 0), we require: sin(µL) = 0 ⇒ µL = nπ, n = 1, 2, 3, . . . (5.13) Thus, µn = nπ L , λn = − ( nπ L )2 . (5.14) λn are eigenvalues. The corresponding eigenfunctions are: Xn(x) = sin ( nπ L x) . (5.15) So, the solution will have the form un(x, t) = e−α2( nπ L ) 2t sin (nπx L ) , n = 1, 2, . . . (5.16) Since the equation (5.3) is linear, a linear combination of solutions is again a solution. Thus the most general solution is: u(x, t) = ∞∑ n=1 bne−α2( nπ L ) 2t sin (nπx L ) , (5.17) for some coeﬃcients bn, n = 1, 2, . . .. Now the question is: How can we ﬁnd those coeﬃcients? Using the initial condition u(x, 0) = f (x): f (x) = ∞∑ n=1 bn sin (nπx L ) . (5.18) This is the Fourier sin series of f (x). We have the following question in mind: Note Given a function f (x) deﬁned on [0, L], do there exist constants b1, b2, . . . such that (5.18) holds? If the answer is ”yes” then (5.17) furnishes a solution to our problem (5.3). We want to write the function f (x) in terms of the sum of an inﬁnite number of basis functions sin ( nπx L ) . This is similar to projecting a vector on a set of basis vectors. Note that the sine function is periodic on the interval [0, 2L] or [−L, L]: sin ( nπ(x + 2L) L ) = sin (nπx L + 2nπ) = sin (nπx L ) , sin ( nπ(x − L) L ) = sin (nπx L − nπ + 2nπ) = sin (nπ(x + L) L ) . Now, we use an important property of trigonometric functions: 66 Theorem 5.2.1: Orthogonality of trigonometric functions The trigonometric functions sin ( nπx L ) and cos ( nπx L ) (for n = 1, 2, 3, . . . ) are orthogonal over the interval [0, L] in the following sense: ∫ L 0 sin ( nπx L ) sin (mπx L ) dx = { 0, if n ̸= m L 2 , if n = m , (5.19) ∫ L 0 cos (nπx L ) cos (mπx L ) dx =  | | 0, if n ̸= m L 2 , if n = m ̸= 0 L, if n = m = 0 , (5.20) The concept of orthogonality means that the inner product of two functions (or two vectors) is zero over an interval. ⟨f (x), g(x)⟩ = ∫ L 0 f (x)g(x)dx. (Inner product of two functions) Proof. To prove the ﬁrst property (5.19), we use the trigonometric identities: cos(A + B) = cos A cos B − sin A sin B, cos(A − B) = cos A cos B + sin A sin B. Using these, we rewrite the product of two sine functions: sin A sin B = 1 2 (cos(A − B) − cos(A + B)) . For m ̸= n: ∫ L 0 sin (nπx L ) sin ( mπx L ) dx = ∫ L 0 1 2 [ cos ( πx(n − m) L ) − cos (πx(n + m) L )] dx = 1 2 [ L (n − m)π sin ( πx(n − m) L )\f \f \f \f L 0 − L (n + m)π sin ( πx(n + m) L )\f \f \f \f L 0 ] = 0. For m = n, we use: sin 2 A = 1 2 (1 − cos 2A). Thus: ∫ L 0 sin2 ( nπx L ) dx = ∫ L 0 1 2 (1 − cos ( 2nπx L )) dx = 1 2 [x\f \f \f \f L 0 − L 2nπ sin ( 2nπx L )\f \f \f \f L 0 ] = L 2 . The case of (5.20) and (??) follow with similar arguments. 67 Finding Fourier coeﬃcients bn Going back to our Fourier series representation: f (x) = ∞∑ n=1 bn sin (nπx L ) . To ﬁnd bn, multiply both sides by sin ( mπx L ) and integrate over [0, L]: ∫ L 0 f (x) sin ( mπx L ) dx = ∞∑ n=1 bn ∫ L 0 sin ( nπx L ) sin ( mπx L ) dx. So: ∫ L 0 f (x) sin ( mπx L ) dx = 0 + 0 + · · · + bm ∫ L 0 sin 2 ( mπx L ) dx + 0 + 0 + . . . all terms in the sum, except the m th term are zero. Hence, ∫ L 0 f (x) sin (mπx L ) dx = bm · L 2 Thus, the Fourier sin coeﬃcients are: bm = 2 L ∫ L 0 f (x) sin ( mπx L ) dx, m = 1, 2, 3, . . . . (5.21) Finally, the most general solution to (5.3) is given by u(x, t) = ∞∑ n=1 ( 2 L ∫ L 0 f (x) sin (nπx L ) dx) e−α2( nπ L ) 2t sin (nπx L ) . (5.22) Example 5.2.1: Fourier sine expansion Let us solve (5.3) with f (x) = x, 0 < x < 1, L = 1. We have using the integration by part formula, bn = 2 ∫ 1 0 x sin(nπx)dx = 2(−1) n+1 nπ . Hence, u(x, t) = 2 π ∞∑ n=1 (−1) n+1 n e −α2(nπ) 2t sin (nπx) . We can use the latter expression to compute some series: for example, if t = 0 and x = 1 2, then we have u(1/2, 0) = f (1/2) = 1/2 = 2 π ∞∑ n=1 (−1) n+1 n sin (nπ/2) . 68 k n sin (nπ/2) 0 1 1 2 0 1 3 −1 4 0 2 5 1 Therefore, ∞∑ k=0 (−1) k (2k + 1) = π 4 . Figure 5.2: Representation of the terms of the Fourier series [1]. Example 5.2.2 We want to solve the following PDE problem: ut = 0.003uxx, 0 < x < 1, t > 0 u(0, t) = u(1, t) = 0 u(x, 0) = 50x(1 − x) for 0 < x < 1 As previously, we need to write f (x) = 50x(1 − x) for 0 < x < 1 as a sine series. That is, f (x) = ∞∑ n=1 bn sin(nπx), where bn = 2 ∫ 1 0 50x(1 − x) sin(nπx)dx = 200 π3n3 − 200(−1) n π3n3 = { 0 if n even 400 π3n3 if n odd Hence the solution u(x, t), is given by: u(x, t) = 400 π3 ∞∑ k=0 1 (2k + 1)3 sin((2k + 1)πx)e −(2k+1)2π20.003t. 69 5.2.2 Neumann boundary conditions: Fourier cosine series Consider the heat conduction in an insulated rod whose endpoints are insulated and within which the initial temperature is given by f (x) as shown in Figure 5.3. Figure 5.3: Consider a conducting bar with thermal conductivity α2 that has an initial temperature distribution u(x, 0) = f (x) and whose endpoints are insulated [1]. The system of equations is given by: ut = α2uxx, 0 < x < L, t > 0 BC: ux(0, t) = 0, ux(L, t) = 0, IC: u(x, 0) = f (x), (5.23) We assume a solution of the form: u(x, t) = X(x)T (t). (5.24) Diﬀerentiating both sides: ut = X(x) · ˙T (t), uxx = X ′′(x) · T (t). Substituting these into the heat equation (5.23): X(x) · ˙T (t) = α2X ′′(x) · T (t). (5.25) Dividing by α2X(x)T (t): ˙T (t) α2T (t) = X ′′(x) X(x) = λ. (5.26) Since both sides depend on diﬀerent variables, they must equal a constant, denoted as λ. This gives two ordinary diﬀerential equations (ODEs): Time equation: ˙T (t) = λα2T (t). Solving this gives: T (t) = Ce λα2t. (5.27) Space equation: an eigenvalue problem X ′′(x) = λX(x), X ′(0) = 0 = X ′(L). (5.28) The nature of X(x) depends on λ. 70 Case 1: λ > 0 Let λ = µ2, then: X ′′ − µ2X = 0. The general solution is (we have chosen this form for simpliﬁcations, it is also ﬁne to use the expo- nential form): X(x) = A sinh(µx) + B cosh(µx). (5.29) We note that X ′(x) = Aµ cosh(µx) + Bµ sinh(µx). Applying boundary conditions: X ′(0) = 0 ⇒ A = 0, X ′(L) = 0 ⇒ Bµ sinh(µL) = 0. (5.30) Since sinh(µL) ̸= 0, we must have B = 0, leading to the trivial solution. Case 2: λ = 0 The equation simpliﬁes to: X ′′(x) = 0 ⇒ X(x) = Bx + A. Applying boundary conditions: X ′(0) = 0 ⇒ B = 0, X ′(L) = 0 ⇒ B = 0. (5.31) So, X(x) = A is a non-trivial solution. The eigenvalue λ0 = 0 and the corresponding eigenfunction is X0(x) = 1. Case 3: λ < 0 Let λ = −µ2, then: X ′′ + µ2X = 0. The general solution is: X(x) = A sin(µx) + B cos(µx). (5.32) We have X ′(x) = µA cos(µx) − µB sin(µx). (5.33) Applying boundary conditions: X ′(0) = 0 ⇒ A = 0, X ′(L) = 0 ⇒ −µB sin(µL) = 0. For a nontrivial solution (B ̸= 0), we require: sin(µL) = 0 ⇒ µL = nπ, n = 1, 2, 3, . . . (5.34) Thus, µn = nπ L , λn = − ( nπ L )2 . (5.35) λn are eigenvalues. The corresponding eigenfunctions are: Xn(x) = cos (nπx L ) , n = 1, 2, 3, . . . (5.36) 71 So, the solution will have the form un(x, t) = e−α2( nπ L ) 2t cos ( nπx L ) , n = 1, 2, . . . (5.37) We also had another eigenvalue/function from λ = 0, u0(x, t) = A0 · e0·t = A0. Since the equation (5.23) is linear, a linear combination of solutions is again a solution. Thus the most general solution is of the form: u(x, t) = A0 + ∞∑ n=1 Ane −α2( nπ L ) 2t cos ( nπx L ) , (5.38) for some coeﬃcients An, n = 0, 1, 2, . . .. Using the initial condition u(x, 0) = f (x): f (x) = A0 + ∞∑ n=1 An cos ( nπx L ) . (5.39) As previously, we use the inner product ⟨·, ·⟩ to project f (x) onto the basis functions in the series: We multiply both sides of (5.39) by X0(x) = 1 and integrate over [0, L]: ∫ L 0 f (x)dx = A0 ∫ L 0 1dx + ∞∑ n=1 An ∫ L 0 cos (nπx L ) dx. i.e ∫ L 0 f (x)dx = A0 · x| L 0 + ∞∑ n=1 An L nπ sin (nπx L )\f \f \f \f \f L 0 = A0L + 0. Hence, A0 = 1 L ∫ L 0 f (x)dx. Now, if we multiply both sides of (5.39) by cos ( mπx L ) and integrate over [0, L]: ∫ L 0 f (x) cos ( mπx L ) dx = A0 ∫ L 0 cos (mπx L ) dx + ∞∑ n=1 An ∫ L 0 cos ( nπx L ) cos ( mπx L ) dx, i.e. ∫ L 0 f (x) cos (mπx L ) dx = A0 L nπ sin ( nπx L )\f \f \f \f L 0 + Am · L 2 . So, Am = 2 L ∫ L 0 f (x) cos ( mπx L ) dx. Thus, the Fourier cos coeﬃcients are: A0 = 1 L ∫ L 0 f (x)dx, Am = 2 L ∫ L 0 f (x) cos (mπx L ) dx, m = 1, 2, 3, . . . . (5.40) The ﬁnal solution is: u(x, t) = 1 L ∫ L 0 f (x)dx + ∞∑ n=1 ( 2 L ∫ L 0 f (x) cos ( nπ L x) dx ) cos (nπ L x) e−α2 n2π2 L2 t. (5.41) 72 We observe that as t → ∞ it follows that u(x, t) → A0 = 1 L ∫ L 0 f (x)dx, which is just the average value of the initial heat f (x) distributed in the bar. This is consistent with physical intuition. It is sometimes convenient to re-deﬁne the Fourier coeﬃcients as follows: a0 = 2A0 ak = Ak, k = 1, 2, . . . so that the ak can be rewritten on a uniﬁed form ak = 2 L ∫ L 0 f (x) cos (kπx L ) dx k = 0, 1, 2, . . . In terms of the new coeﬃcients ak deﬁned, the Fourier expansion for the initial condition function f (x) is of the form f (x) = a0 2 + ∞∑ n=1 an cos (nπx L ) (5.42) while the solution of the heat equation (5.23) is of the form u(x, t) = a0 2 + ∞∑ n=1 an cos (nπx L ) e−α2( nπ L ) 2t. (5.43) Example 5.2.3: Fourier cosine expansion Determine the Fourier coeﬃcients ak for the function f (x) = x, 0 < x < 1 = L and use the resulting Fourier cosine expansion to prove the identity ∞∑ k=0 1 (2k + 1)2 = π2 8 . We have, a0 = 2 ∫ 1 0 xdx = 2 [x2 2 ]1 0 = 1 and an = 2 ∫ 1 0 x cos(nπx)dx = 2(−1) n − 1 n2π2 = { − 4 n2π2 , n odd, 0, n even. Therefore, f (x) = 1 2 − 4 π2 ∞∑ k=0 1 (2k + 1)2 cos((2k + 1)πx). (5.44) To obtain the required identity we set x = 1 in and rearrange terms. We can also deduce that the solution of (5.23) with the initial condition u(x, 0) = x is given by u(x, t) = 1 2 − 4 π2 ∞∑ k=0 1 (2k + 1)2 cos((2k + 1)πx)e −α2((2k+1)π) 2t. (5.45) The partial sums are shown in ﬁgure 5.4 73 (a) Sum till n = 2 terms (b) Sum till n = 3 terms (c) Sum till n = 5 terms Figure 5.4: These ﬁgures show the partial sums of the Fourier Cosine Series In Figure 5.5 we plot the same graphs but on a larger domain than [0, L] = [0, 1]. Figure 5.5: These ﬁgures show the partial sums of the Fourier cosine Series. Example 5.2.4 Let us solve now the following PDE problem ut = 0.003uxx, 0 < x < 1, t > 0 ux(0, t) = ux(1, t) = 0, u(x, 0) = 50x(1 − x) for 0 < x < 1. We must ﬁnd the cosine series of u(x, 0). For 0 < x < 1 we have 50x(1 − x) = 25 3 − 50 π2 ∞∑ k=1 1 k2 cos(2kπx) Hence, the solution to the PDE problem, is given by: u(x, t) = 25 3 − 50 π2 ∞∑ k=1 1 k2 cos(2kπx)e−k2π20.012t. 74 5.2.3 Heat equation on a circular ring - Full Fourier series Consider a thin circular wire in which there is no radial temperature dependence, i.e., u(r, θ) = u(θ) so that ∂u ∂r = 0. In this case the polar Laplacian reduces to ∆u = ∂2u ∂r2 + 1 r ∂u ∂r + 1 r2 ∂2u ∂θ2 = ∂2u ∂(rθ)2 , and if we let x = rθ then ∂2u ∂(rθ)2 = uxx. In this case the heat distribution in the ring is determined by the following initial value problem with periodic boundary conditions: ut = α2uxx, 0 < x < L, t > 0 BC: u(−L, t) = u(L, t), ux(−L, t) = ux(L, t), IC: u(x, 0) = f (x), (5.46) Figure 5.6: Consider a thin conducting ring with thermal conductivity α2 that has a given initial temperature distribution. We assume a solution of the form: u(x, t) = X(x)T (t). (5.47) Diﬀerentiating both sides: ut = X(x) · ˙T (t), uxx = X ′′(x) · T (t). Substituting these into the heat equation (5.46): X(x) · ˙T (t) = α2X ′′(x) · T (t). (5.48) Dividing by α2X(x)T (t): ˙T (t) α2T (t) = X ′′(x) X(x) = λ. (5.49) Since both sides depend on diﬀerent variables, they must equal a constant, denoted as λ. This gives two ordinary diﬀerential equations (ODEs): Time equation: ˙T (t) = λα2T (t). 75 Solving this gives: T (t) = Ce λα2t. (5.50) Space equation: an eigenvalue problem X ′′(x) = λX(x), X(−L) = X(L), X ′(−L) = X ′(L). (5.51) The nature of X(x) depends on λ. Case 1: λ > 0 Let λ = µ2, then: X ′′ − µ2X = 0. The general solution is (we have chosen this form for simpliﬁcations, it is also ﬁne to use the expo- nential form): X(x) = A cosh(µx) + B sinh(µx). (5.52) We note that X ′(x) = Aµ sinh(µx) + Bµ cosh(µx). Applying boundary conditions: X(−L) = X(L) ⇒ 2B sinh(µL) = 0 ⇒ B = 0, (5.53) and X ′(−L) = X ′(L) ⇒ 2Aµ sinh(µL) = 0 ⇒ A = 0 (5.54) leading to the trivial solution. Case 2: λ = 0 The equation simpliﬁes to: X ′′(x) = 0 ⇒ X(x) = Ax + B. We also have X ′(x) = A. Applying boundary conditions: X(−L) = X(L) ⇒ 2AL = 0 ⇒ A = 0, and X ′(−L) = A = 0 = X ′(L). So, X(x) = B is a non trivial solution. For this case, the eigenvalue λ0 = 0 and the corresponding eigenfunction is X0 = 1. 76 Case 3: λ < 0 Let λ = −µ2, then: X ′′ + µ2X = 0. The general solution is: X(x) = A cos(µx) + B sin(µx). (5.55) We have X ′(x) = −µA sin(µx) + µB cos(µx). (5.56) Applying boundary conditions: X(−L) = X(L) ⇒ 2B sin(µL) = 0, and X ′(−L) = X ′(L) ⇒ 2Aµ sin(µL) = 0. For a nontrivial solution (A, B, µ ̸= 0), we require: sin(µL) = 0 ⇒ µL = nπ, n = 1, 2, 3, . . . (5.57) Thus, µn = nπ L , λn = − ( nπ L )2 . (5.58) λn are eigenvalues. The corresponding eigenfunctions are: Xn(x) ∈ { cos (nπx L ) , sin ( nπx L )} , n = 1, 2, 3, . . . (5.59) So, the solution will have the form un(x, t) = e −α2( nπ L )2t [An cos ( nπx L ) + Bn sin (nπx L )] , n = 1, 2, . . . (5.60) We also had another eigenvalue/function from λ = 0, u0(x, t) = A0 · e0·t = A0. Since the equation (5.46) is linear, a linear combination of solutions is again a solution. Thus the most general solution is of the form: u(x, t) = A0 + ∞∑ n=1 e −α2( nπ L ) 2t [An cos ( nπx L ) + Bn sin (nπx L )] , (5.61) for some coeﬃcients A0, An, Bn, n = 1, 2, . . .. Using the initial condition u(x, 0) = f (x): f (x) = A0 + ∞∑ n=1 [An cos ( nπx L ) + Bn sin (nπx L )] . (5.62) As previously, we use the inner product ⟨·, ·⟩ to project f (x) onto the basis functions in the series. We state the following important result: 77 Theorem 5.2.2: Orthogonality of trigonometric functions The trigonometric functions sin ( nπx L ) and cos ( nπx L ) (for n = 1, 2, 3, . . . ) are orthogonal over the interval [−L, L] in the following sense: ∫ L −L sin (nπx L ) sin ( mπx L ) dx = { 0, if n ̸= m L, if n = m , (5.63) ∫ L −L cos ( nπx L ) cos ( mπx L ) dx =  | | 0, if n ̸= m L, if n = m ̸= 0 2L, if n = m = 0 , (5.64) ∫ L −L cos (nπx L ) sin ( mπx L ) dx = 0, for all n, m. (5.65) The concept of orthogonality means that the inner product of two functions (or two vectors) is zero over an interval. ⟨f (x), g(x)⟩ = ∫ L −L f (x)g(x)dx. (Inner product of two functions) We multiply both sides of (5.62) by X0(x) = 1 and integrate over [−L, L]: ∫ L −L f (x)dx = A0 ∫ L −L 1dx + ∞∑ n=1 An ∫ L −L cos (nπx L ) dx + ∞∑ n=1 Bn ∫ L −L sin (nπx L ) dx. i.e ∫ L −L f (x)dx = A0 · x| L −L + ∞∑ n=1 An L nπ sin ( nπx L )\f \f \f \f \f L −L − ∞∑ n=1 Bn L nπ cos (nπx L )\f \f \f \f \f L −L = 2A0L + 0 + 0. Hence, A0 = 1 2L ∫ L −L f (x)dx. (5.66) Now, if we multiply both sides of (5.62) by cos ( mπx L ) and integrate over [−L, L]: ∫ L −L f (x) cos (mπx L ) dx = A0 ∫ L −L cos ( mπx L ) dx + ∞∑ n=1 An ∫ L −L cos (nπx L ) cos ( mπx L ) dx + ∞∑ n=1 Bn ∫ L −L sin ( nπx L ) cos (mπx L ) dx, i.e. (using (5.64) and (5.65)) ∫ L −L f (x) cos ( mπx L ) dx = A0 L nπ sin (nπx L )\f \f \f \f L −L + AmL + 0. So, Am = 1 L ∫ L −L f (x) cos (mπx L ) dx. 78 Now, if we multiply both sides of (5.62) by sin ( mπx L ) and integrate over [−L, L]: ∫ L −L f (x) sin ( mπx L ) dx = A0 ∫ L −L sin (mπx L ) dx + ∞∑ n=1 An ∫ L −L cos ( nπx L ) sin (mπx L ) dx + ∞∑ n=1 Bn ∫ L −L sin (nπx L ) sin (mπx L ) dx, i.e. (using (5.63) and (5.65)) ∫ L −L f (x) sin (mπx L ) dx = −A0 L nπ cos (nπx L )\f \f \f \f L −L + 0 + BmL. So, Bm = 1 L ∫ L −L f (x) sin (mπx L ) dx. So, in summary, the full Fourier series of f (x) is: f (x) = a0 2 + ∞∑ n=1 an cos ( nπx L ) + bn sin (nπx L ) , where an = 1 L ∫ L −L f (x) cos ( nπx L ) dx, n = 0, 1, 2, 3, . . . (5.67) and bn = 1 L ∫ L −L f (x) sin (mπx L ) dx, n = 1, 2, 3, . . . (5.68) The ﬁnal solution is: u(x, t) = a0 2 + ∞∑ n=1 e −α2( nπ L ) 2t [ an cos ( nπx L ) + bn sin (nπx L )] . (5.69) We observe that as t → ∞ it follows that u(x, t) → a0 2 = 1 2L ∫ L −L f (x)dx, which is just the average value of the initial heat f (x). Example 5.2.5: Full Fourier expansion Let us solve the equation (5.46) with the initial condition u(x, 0) = f (x), where f (x) = { 0, −π < x < 0, x, 0 ≤ x ≤ π. In this case, L = π. We need to write the full Fourier expansion of u(x, 0). We have, a0 = 1 π ∫ π −π f (x)dx = 1 π ∫ π 0 xdx = π 2 . 79 an = 1 π ∫ π −π f (x) cos(nx)dx = 1 π ∫ π 0 x cos(nx)dx = 1 π { xsin(nx) n \f \f \f \f π 0 − 1 n ∫ π 0 sin(nx)dx} = 1 π { π sin(nπ) n + 1 n2 cos(nx) \f \f \f \f π 0 } = 1 πn2 [(−1) n − 1] . (13.6) For even indices, a2m = 0 for m = 0, 1, . . .. For odd indices: a2m+1 = − 2 π(2m + 1)2 , m = 0, 1, 2, . . . Now, we compute bn bn = 1 π ∫ π −π f (x) sin(nx)dx = 1 π ∫ π 0 x sin(nx)dx = 1 π { − x cos(nx) n \f \f \f \f π 0 + 1 n ∫ π 0 cos(nx)dx} = 1 π { −π cos(nπ) n + 1 n2 sin(nx) \f \f \fπ 0 } . Therefore, the Fourier series representation of f (x) is given by f (x) = a0 2 + ∞∑ n=1 an cos(nx) + bn sin(nx) = π 4 − 2 π ∞∑ m=0 cos[(2m + 1)x] (2m + 1)2 + ∞∑ n=1(−1) n+1 sin(nx) n . (13.9) Finally, the solution takes the form: u(x, t) = π 4 − 2 π ∞∑ m=0 e−α2(2m+1)2t cos[(2m + 1)x] (2m + 1)2 + ∞∑ m=1 (−1) m+1 e −α2m2t sin(mx) m . 80 Important! I: The Dirichlet Problem (Ice on Both Sides) X ′′ + λ 2X = 0 X(0) = 0 = X(L) } =⇒ { λn = nπ L , n = 1, 2, . . . Xn(x) = sin ( nπx L ) II: The Neumann Problem (Insulation on Both Sides) X ′′ + λ2X = 0 X ′(0) = 0 = X ′(L) } =⇒ { λn = nπ L , n = 0, 1, 2, . . . Xn(x) = cos ( nπx L ) III: The Periodic Boundary Value Problem (The Closed Ring) X ′′ + λ 2X = 0 X(−L) = X(L) X ′(−L) = X ′(L)  ‖  =⇒ { λn = nπ L , n = 0, 1, 2, . . . Xn(x) ∈ {1, cos ( nπx L ) , sin ( nπx L )} IV: Mixed Boundary Value Problem A (Ice Left and Insulation Right) X ′′ + λ2X = 0 X(0) = 0 = X ′(L) } =⇒ { λk = (2k+1)π 2L , k = 0, 1, 2, . . . Xn(x) = sin ( (2k+1)π 2L x) 81 V: Mixed Boundary Value Problem B (Insulation Left and Ice Right) X ′′ + λ2X = 0 X ′(0) = 0 = X(L) } =⇒ { λk = (2k+1)π 2L , k = 0, 1, 2, . . . Xn(x) = cos ( (2k+1)π 2L x) 5.3 Fourier Series In the previous sections, we solved the heat equation under various boundary conditions, including periodic, Dirichlet, and Neumann conditions. In each case, a key step in the solution was expressing the initial condition u(x, 0) as a sum of sine and/or cosine functions-an expansion known as a Fourier series. The fundamental idea behind Fourier series is that any “reasonable” function can be repre- sented as an inﬁnite sum of trigonometric terms. In the following section, we explore Fourier series in more detail, developing the formalism that allows us to decompose functions into their fundamental frequency components. We consider the expansion of the function f (x) of the form f (x) ∼ a0 2 + ∞∑ n=1 an cos (nπx L ) + bn sin (nπx L ) = S(x) (5.70) where an = 1 L ∫ L −L f (x) cos (nπx L ) dx, a0 2 = 1 2L ∫ L −L f (x)dx = average value of f (5.71) and bn = 1 L ∫ L −L f (x) sin ( nπx L ) dx. (5.72) We recall that a function f (x) deﬁned on for all x is periodic with period T if f (x + T ) = f (x) for all x. We observe that cos ( nπ L (x + T )) = cos ( nπx L ) provided nπT L = 2π, T = 2L n and similarly sin ( nπ L (x + 2L) ) = sin ( nπx L ) . Thus each of the terms of the Fourier Series S(x) on the RHS of (5.70) is a periodic func- tion having a maximal period 2L (a constant function is periodic with any period). As a result the function S(x) is also periodic. But the question is: How does this relate to f (x) which may not be periodic? The function S(x) represented by the series is known as the periodic extension of f on [−L, L]. Deﬁnition 5.3.1: Periodic extension Idea of the periodic extension of a given function: if f is deﬁned on the interval [a, b] then the periodic extension fper of f , which has period T = b − a, is deﬁned simply by “repeating” f in all the intervals [a + nT, b + nT ] for n = 0, ±1, . . ., so that for all x, fper (x) = f (x − nT ) whenever a + nT < x ≤ b + nT, n = 0, ±1, ±2, . . . (5.73) 82 In Figure 5.7, we show a picture for a = −1, b = 1, and f (x) = x2. Figure 5.7: Periodic extension fper (x) of the function f (x) = x2, a = −1 and b = 1. Note that fper may be discontinuous at a, b, etc., even if f is continuous. A related fact is that in deﬁning fper we have taken fper (a) = f (b) and not fper (a) = f (a); some choice must be made but this has no eﬀect in practice. Note 5.3.1: It can be useful to shift the interval of integration Since the periodic extension fper is periodic with period 2L (as are the basis functions cos ( nπx L ) and sin ( nπx L ) ), the interval [−L, L] over which the integration is carried out may be replaced by any other interval of the same length: that is for any X, a0 = 1 2L ∫ X+2L X fper (x)dx, an = 1 L ∫ X+2L X fper (x) cos nπx L dx, n ≥ 1 bn = 1 L ∫ X+2L X fper (x) sin nπx L dx, n ≥ 1. Example 5.3.1 Consider the example given in Example 5.2.3. Then, we have f (x) = { 0 −π < x < 0 x 0 ≤ x ≤ π On [π, 3π], fper(x) = { 0 π < x < 2π x − 2π 2π ≤ x ≤ 3π an = 1 π ∫ 3π π fper(x) cos(nx)dx, change of variables: t = x − 2π dx = dt, x = t + 2π = 1 π ∫ 3π 2π (x − 2π) cos(nx)dx = 1 π ∫ 2π 0 t cos(nt)dt since cos n(t + 2π) = cos nt. 83 5.3.1 Half range Fourier Series: even and odd functions We consider the Fourier Expansions for Even and Odd functions, which give rise to cosine and sine half range Fourier Expansions. If we are only given values of a function f (x) over half of the range [0, L], we can deﬁne two diﬀerent extensions of f to the full range [−L, L], which yield distinct Fourier Expansions. The even extension gives rise to a half range cosine series, while the odd extension gives rise to a half range sine series. We ﬁrst recall the elementary deﬁnitions of even, odd, and periodic functions. Deﬁnition 5.3.2: Even and odd functions A function f (x) is even if it is deﬁned for all x (or possibly in some interval symmetric about x = 0, that is, of the form (−L, L) or [−L, L]) and satisﬁes f (x) = f (−x); it is odd if it is similarly deﬁned and satisﬁes f (−x) = −f (x). We will frequently use the observation that if f (x) is deﬁned for −L ≤ x ≤ L then, ∫ L −L f (x)dx = { 0, if f is odd 2 ∫ L 0 f (x)dx, if f is even . (5.74) This formula is easily derived by writing ∫ L −L f (x)dx = ∫ 0 −L f (x)dx + ∫ L 0 f (x)dx and making the change of variable y = −x in the ﬁrst integral. Note 5.3.2 Let E(x) represent an even function and O(x) an odd function. Then, (a) If f (x) = E(x) · O(x) then f (−x) = E(−x)O(−x) = −E(x)O(x) = −f (x) ⇒ f is odd. (b) E1(x) · E2(x) → even. (c) O1(x) · O2(x) → even. (d) Any function can be expressed as a sum of an even part and an odd part: f (x) = 1 2 [f (x) + f (−x)] | {z } even part +1 2 [f (x) − f (−x)] | {z } odd part Check: Let E(x) = 1 2[f (x) + f (−x)]. Then E(−x) = 1 2[f (−x) + f (x)] = E(x) (even.) Similarly let O(x) = 1 2[f (x) − f (−x)]. Then, O(−x) = 1 2[f (−x) − f (x)] = −O(x) ( odd.) Important!: Consequences of the Even/Odd property for Fourier Series One may use (5.74) to considerably simplify the formulas (5.71)-(5.72) when f is even or odd. • If f is even then f (x) cos(nπx/L) is even and f (x) sin(nπx/L) is odd, so that from (5.74), a0 = 1 L ∫ L 0 f (x)dx, an = 2 L ∫ L 0 f (x) cos nπx L dx, bn = 0, n ≥ 1. (5.75) 84 • If f is odd one has f (x) cos(nπx/L) is odd and f (x) sin(nπx/L) is even a0 = 0, an = 0, bn = 2 L ∫ L 0 f (x) sin nπx L dx, n ≥ 1. (5.76) • Since any function can be written as the sum of an even and an odd part, we can interpret the cosine and sine series as corresponding to even and odd functions: f (x) = 1 2 [ f (x) + f (−x)] + 1 2[ f (x) − f (−x) ] , where the ﬁrst term represents the even part of f (x) and the second term represents the odd part. Thus, the Fourier series expansion can be expressed as: f (x) = { a0 2 + ∞∑ n=1 an cos (nπx L )} + { ∞∑ n=1 bn sin (nπx L ) } , where the cosine terms correspond to the even part and the sine terms correspond to the odd part of the function. In addition, an = 2 L ∫ L 0 1 2[f (x) + f (−x)] cos (nπx L ) dx = 1 L ∫ L −L f (x) cos ( nπx L ) dx bn = 2 L ∫ L 0 1 2[f (x) − f (−x)] sin (nπx L ) dx = 1 L ∫ L −L f (x) sin (nπx L ) dx. Let us emphasize that in (5.75)-(5.76) we are considering the Fourier series of a function deﬁned on the interval [−L, L]. 5.3.2 Half-range expansions If we are given a function f (x) on an interval [0, L] and we want to represent f by a Fourier series, we have two choices: a cosine series or a sine series. : Cosine series If f (x) is extended as an even function on [−L, L], it can be represented by a Fourier cosine series: f (x) = a0 2 + ∞∑ n=1 an cos (nπx L ) , where the Fourier coeﬃcients are given by an = 2 L ∫ L 0 f (x) cos (nπx L ) dx. We note that the even periodic extension is obtained by simply computing the Fourier series representation for the even function fe(x) ≡ { f (x), 0 < x < L f (−x) −L < x < 0 85 Since fe(x) is an even function on a symmetric interval [−L, L], we expect that the resulting Fourier series will not contain sine terms (see (5.75)). Therefore, the series expansion will be given by (5.71). However, we can simplify this by noting that the integrand is even and the interval of integration can be replaced by [0, L]. On this interval fe(x) = f (x). So, we have the Cosine Series representation of f (x) for x ∈ [0, L] given as above. : Sine series If f (x) is extended as an odd function on [−L, L], it can be represented by a Fourier sine series: f (x) = ∞∑ n=1 bn sin (nπx L ) , where the Fourier coeﬃcients are given by bn = 2 L ∫ L 0 f (x) sin (nπx L ) dx. Similarly as the case of the cosine series, given f (x) deﬁned on [0, L], the odd periodic extension is obtained by simply computing the Fourier series representation for the odd function fo(x) ≡ { f (x), 0 < x < L −f (−x) −L < x < 0 The resulting series expansion leads to deﬁning the Sine Series representation of f (x) for x ∈ [0, L] as described above. Example 5.3.2 Expand f (x) = x, 0 < x < 2 in a half-range: (a) Sine Series, (b) Cosine Series. (a) Sine Series (L = 2) bn = 2 L ∫ L 0 f (x) sin ( nπ L x) dx = ∫ 2 0 x sin ( nπ 2 x) dx = − x cos ( nπ 2 x) ( nπ 2 ) \f \f \f \f \f 2 0 + 2 nπ ∫ 2 0 cos (nπ 2 x) dx = − 4 nπ cos(nπ) + ( 2 nπ )2 sin (nπ 2 x)\f \f \f \f \f 2 0 = − 4 nπ (−1) n. Therefore, the sine series expansion is: f (x) = 4 π ∞∑ n=1 (−1) n+1 n sin (nπ 2 x) . 86 We can deduce the following sum: f (1) = 1 = 4 π ∞∑ n=1 (−1) n+1 n sin ( nπ 2 ) Therefore π 4 = 1 − 1 3 + 1 5 − 1 7 + · · · (a) Cosine Series (L = 2) a0 = 2 2 ∫ 2 0 x dx = x2 2 \f \f \f \f 2 0 = 2, an = ∫ 2 0 x cos nπ 2 x dx = ( 2 nπ ) x sin nπ 2 x\f \f \f \f 2 0 − ( 2 nπ ) ∫ 2 0 sin nπ 2 x dx + ( 2 nπ )2 cos nπ 2 x \f \f \f \f \f 2 0 = 4 n2π2 {cos(nπ) − 1}. Therefore, f (x) = 1 + 4 π2 ∞∑ n=1 [(−1) n − 1] n2 cos nπ 2 x = 1 − 8 π2 ∞∑ n=0 cos ( (2n+1)πx 2 ) (2n + 1)2 . The cosine series converges faster than the sine series. f (2) = 2 = 1 + 8 π2 ∞∑ n=0 1 (2n + 1)2 , π2 8 = 1 + 1 32 + 1 52 + · · · . Example 5.3.3 Find the Fourier cosine series of f (x) = sin x over the interval [0, π]. We have a0 = 2 π ∫ π 0 sin xdx = 4 π a1 = 2 π ∫ π 0 sin x cos xdx = 1 π ∫ π 0 sin(2x)dx = 0 To ﬁnd an with n ≥ 2, we use the identity 2 sin x cos(nx) = sin[(n + 1)x] − sin[(n − 1)x]. 87 Thus, an = 2 n ∫ π 0 sin x cos(nx)dx = 1 π ∫ π 0 (sin(n + 1)x − sin(n − 1)x)dx = 1 π [ cos(n − 1)x n − 1 − cos(n + 1)x n + 1 ]π 0 = 2 ((−1) n−1 − 1) π (n2 − 1) . We get the Fourier cosine of sin x on [0, π] as sin x = 2 π + 2 π ∞∑ n=2 (−1)n−1 − 1 n2 − 1 cos(nx) = 2 π − 4 π ∞∑ j=1 cos(2jx) (2j)2 − 1 Example 5.3.4: Periodic extension Assume that f (x) = x, 0 < x < 2 represents one full period of the function so that f (x + 2) = f (x). Then 2L = 2 ⇒ L = 1. a0 = 1 L ∫ L −L f (x)dx = ∫ 1 −1 f (x)dx = ∫ 2 0 xdx = x2 2 \f \f \f \f 2 0 = 2. since f (x + 2) = f (2) For n ≥ 1, an = 1 L ∫ L −L f (x) cos ( nπx L ) dx = ∫ 1 −1 f (x) cos(nπx) dx (with L = 1) = ∫ 2 0 x cos(nπx) dx = [x sin(nπx) nπ ]2 0 − 1 nπ ∫ 2 0 sin(nπx) dx = [x sin(nπx) nπ ]2 0 = 1 (nπ)2 [cos(2nπ) − 1] = 0 (since cos(2nπ) = 1) bn = 1 L ∫ L −L f (x) sin (nπx L ) dx = ∫ 1 −1 f (x) sin(nπx) dx = ∫ 2 0 x sin(nπx) dx = [−x cos(nπx) nπ ]2 0 + 1 nπ ∫ 2 0 cos(nπx) dx = −2 nπ + [sin(nπx) (nπ)2 ]2 0 = −2 nπ + 0 = −2 nπ 88 Therefore, f (x) = 1 − 2 π ∞∑ n=1 sin(nπx) n . Note 5.3.3: Hmmm... In the previous example, if we compute f (0) using the original deﬁnition of f , we obtain 0. However, if we use the Fourier expansion of f , we obtain f (0) = 1. What is wrong? 5.3.3 Convergence of Fourier Series In this section, we state the fundamental convergence theorem for Fourier Series, which assumes that the function f (x) is piecewise continuous. At points of discontinuity of f (x), the Fourier approxima- tion SN (x) takes on the average value: 1 2 ( f (x+) + f (x−)) where f (x+) and f (x−) represent the right-hand and left-hand limits of f (x) at the discontinuity, respectively. Before stating the main result of this section, we introduce the following notion. Deﬁnition 5.3.3: Piecewise continuous A function f : [a, b] → R is piecewise continuous if there are numbers t0, t1, . . . , tn with a = t0 < t1 < · · · < tn = b, such that f is continuous on each of the intervals (ti, ti+1), and tends to a ﬁnite value at each endpoint of these intervals. That is, the limits f ( t+ i ) = lim t↘ti f (t) and f ( t − i+1) = lim t↗ti+1 f (t) exist (and are ﬁnite). A function f : [a, b] → R is piecewise continuous if, roughly speaking, it is made up of a ﬁnite number of continuous pieces. A function f : R → R is piecewise continuous if it is piecewise continuous on every closed interval [a, b]. Thus it can have inﬁnitely many discontinuities, but only ﬁnitely many on any ﬁnite interval. Example 5.3.5 • For the square wave function f (t) = { 0 if − π < t ≤ 0 π if 0 < t ≤ π we have f (0 −) = 0 and f (0 +) = π and f is continuous on (−π, 0), (0, π). Therefore, f is piecewise continuous on [−π, π]. • The function f : R → R given by f (t) = tan t (and f (t) = 0 if t is an odd multiple of π/2 ) is not piecewise continuous. Although it has only ﬁnitely many discontinuities on any ﬁnite interval, the function “blows up” at these discontinuities, so that (for example) the limits f ( π 2 −) and f ( π 2 +) do not exist. 89 Theorem 5.3.1: Convergence of Fourier Series Let f and f ′ be piecewise continuous functions on [−L, L] (i.e. f is piecewise continuously diﬀerentiable or piecewise C 1) and periodic with period 2L, then f has a Fourier Series f (x) ∼ a0 2 + ∞∑ n=1 an cos (nπx L ) + bn sin (nπx L ) = S(x) where an = 1 L ∫ L −L f (x) cos (nπx L ) dx and bn = 1 L ∫ L −L f (x) sin (nπx L ) dx. The Fourier Series converges to f (x) at all points at which f is continuous and to 1 2[f (x+) + f (x−)] at all points at which f is discontinuous. Thus a Fourier Series converges to the average value of the left and right limits at a point of discontinuity of the function f (x). Important! Under the hypothesis of Theorem 5.3.3, ∼ can be replaced by = and S(x) = { f (x), if f is continuous at x f (x+)+f (x−) 2 , if f is discontinuous at the point x. Remark 5.3.1 A way to look at the connection of Fourier series on an interval with the Fourier series of periodic functions is to start with a piecewise continuous function g deﬁned only on the interval [−L, L]. Then gper, a periodic extension of g of period 2L, can play the role of f above; in particular, the Fourier series of g converges to gper everywhere, in our usual sense: S(x) = { gper(x), if gper is continuous at x gper(x+)+gper(x−) 2 , if gper is discontinuous at the point x Remark 5.3.2: Gibbs phenomenon The Fourier series has a diﬃcult time converging at the point of discontinuity and these graphs of the Fourier series show a distinct overshoot which does not go away. This is called the Gibbs phenomenon and the amount of overshoot can be computed. We refer to the Lectures notes of Prof. Peirce for more on this phenomenon. Theorem 5.3.2: Uniform convergence of Fourier Series Let f a continuous functions on [−L, L] and periodic with period 2L. If f ′ is piecewise contin- uous on [−L, L], then the Fourier series for f converges uniformly to f on [−L, L] and hence on any interval. That is, for each ε > 0, there exists an integer N0 (that depends on ε ) such that sup x∈[−L,L] |f (x) − SN (x)| < ε 90 for all N > N0, where SN (x) = a0 2 + N∑ n=1 [an cos ( nπx L ) + bn sin (nπx L )] . 5.3.4 Complex form of Fourier Series Finally, everything said above applies also to the complex form of the Fourier series: a function g(x), periodic with period 2L, has a complex Fourier series g(x) ∼ ∞∑ n=−∞ cne i( nπx L ) with cn = 1 2L ∫ L −L g(x)e −i( nπx L )dx. Example 5.3.6 f (x) = { −1 −π ≤ x < 0 1 0 < x < π L = π cn = 1 2π {− ∫ 0 −π e −inxdx + ∫ π 0 e −inxdx} = 1 2π { − e −inx| 0 −π (−in) + e−inx| π 0 (−in) } = i 2πn {−2 + e+inπ + e−inπ} = { 0 n even 2 iπn n odd Therefore, f (x) = ∞∑ n=−∞ 2 πi(2n + 1) ei((2n+1)x). 5.4 Bessel’s inequality and Parseval Identity Bessel’s inequality and Parseval’s identity are fundamental results in functional analysis and Fourier analysis, particularly in the study of Hilbert spaces. These results provide insights into the decom- position of functions into orthonormal bases and the convergence of series representations. We will explore an analogue of Pythagoras’ Theorem for functions that are square-integrable. Such functions are signiﬁcant in mathematical physics, as they correspond to systems with ﬁnite energy. Addition- ally, we show some applications of the Parseval’s identity into summation formulas involving series of reciprocal powers of n. 91 Deﬁnition 5.4.1: Square integrable function A function f is square-integrable if it satisﬁes the condition: ∫ L −L[f (x)] 2 dx < ∞, in which case we write f ∈ L2([−L, L]). Consider the Fourier series associated with f (x): f (x) ∼ a0 2 + ∞∑ n=1 [an cos ( nπx L ) + bn sin (nπx L )] = S∞. (5.77) Deﬁne the partial sum: SN (x) = a0 2 + N∑ n=1 [an cos (nπx L ) + bn sin ( nπx L )] . (5.78) Then, we have the following results. Theorem 5.4.1: Bessel’s Inequality Let f ∈ L 2[−L, L]. Then a2 0 2 + ∞∑ n=1 a2 n + b2 n ≤ 1 L ∫ L −L f 2(x)dx, in particular the series a2 0 2 + ∞∑ n=1 a2 n + b2 n is convergent. Proof. We have that [f (x) − SN (x)] 2 = f 2(x) − 2f (x)SN (x) + S2 N (x). Consider the least-square error deﬁned as: E2 [f, SN ] = 1 L ∫ L −L [f (x) − SN (x)] 2 dx = 1 L {∫ L −L f 2(x)dx − 2 ∫ L −L f (x)SN (x)dx + ∫ L −L S2 N (x)dx} = 1 L {⟨f, f ⟩ − 2⟨f, SN ⟩ + ⟨SN , SN ⟩} . Now, we compute: ⟨SN , SN ⟩ = ∫ L −L [ a0 2 + N∑ n=1 an cos ( nπx L ) + bn sin (nπx L )]2 dx = a 2 0 2 L + N∑ n=1 ( a2 n ∫ L −L cos 2 (nπx L ) dx + b2 n ∫ L −L sin 2 (nπx L ) dx) = L [ a2 0 2 + N∑ n=1(a2 n + b2 n) ] . 92 In addition, ⟨f, SN ⟩ = ∫ L −L f (x)SN (x)dx = a0 2 ∫ L −L f (x)dx + N∑ n=1 ( an ∫ L −L f (x) cos (nπx L ) dx + bn ∫ L −L f (x) sin ( nπx L ) dx) . Therefore, E2 [f, SN ] = 1 L ∫ L −L [f (x) − SN (x)] 2 dx = 1 L⟨f, f ⟩ − {a2 0 2 + N∑ n=1(a2 n + b2 n) } . (5.79) Since we know that E2 [f, SN ] = ∫ L −L [f (x) − SN (x)] 2 dx ≥ 0, it follows that a2 0 2 + N∑ n=1 a2 n + b2 n ≤ 1 L ∫ L −L f 2(x)dx = 1 L⟨f, f ⟩ = E[f ], where E[f ] is known as the energy of the 2L-periodic function f . Theorem 5.4.2: Parseval’s Identity Let f ∈ L 2[−L, L]. Then the Fourier coeﬃcients an and bn satisfy Parseval’s Formula a2 0 2 + ∞∑ n=1(a2 n + b2 n) = 1 L ∫ L −L f 2(x)dx = E[f ] (5.80) if and only if lim N →∞ ∫ L −L [f (x) − SN (x)] 2 dx = 0. (5.81) Remark 5.4.1 The convergence in (5.81) should be understood in the L2-sense (mean square sense). This is a convergence in an average sense. When {SN } tends to f uniformly, {SN } must tend to f in L 2-sense. The converse is not always true. Hence convergence in L 2-sense is weaker than uniform convergence. Important!: In practice... • The Fourier series of every L2-integrable function converges to the function in L2-sense. • Let f be a piecewise continuous function on [−L, L]. Then SN converges to f in the mean square sense. • If f is piecewise continuous on [−L, L], then Parseval’s identity (5.80) holds. 93 Example 5.4.1 1. Consider the Fourier cosine series of f (x) = x, 0 < x < 2: x ∼ 1 + ∞∑ n=1 4 π2n2 [cos(nπ) − 1] cos nπx 2 a) Write Parseval’s identity corresponding to the above Fourier series. b) Determine from a) the sum of the series 1 14 + 1 24 + 1 34 + . . . Solution: a) We ﬁrst ﬁnd the Fourier coeﬃcient and the period of the Fourier series just by comparing the given series with the standard Fourier series a0 = 2, an = 4 π2n2 [cos(nπ) − 1], n = 1, 2 . . . , bn = 0 period: L = 2. We note that ∫ 2 −2 x2 dx = 16 3 < ∞, so that f ∈ L2(−2, 2). Thus, the conditions for Parseval’s identity are satisﬁed, and we can write a2 0 2 + ∞∑ n=1 (a2 n + b2 n) = 1 2 ∫ 2 −2 f 2(x) dx = ∫ 2 0 f 2(x) dx. This implies ∫ 2 0 x2 dx = 4 2 + ∞∑ n=1 16 π4n4 (cos(nπ) − 1) 2 This can be simpliﬁed to give 8 3 = 2 + 64 π4 [ 1 14 + 1 34 + 1 54 + . . .] Then we obtain 1 14 + 1 34 + 1 54 + . . . = π4 96 b) Let S = 1 14 + 1 24 + 1 34 + . . . 94 This series can be rewritten as S = ( 1 14 + 1 34 + 1 54 + . . .) + ( 1 24 + 1 44 + 1 64 + . . .) = π4 96 + 1 24 S Then we have the required sum as S = π4 90 . 2. Find the Fourier series of x2, −π < x < π and use it along with Parseval’s theorem to show that ∞∑ n=1 1 (2n − 1)4 = π4 96 Solution: Since f (x) = x2 is an even function, so bn = 0. The Fourier coeﬃcients an will be given as an = 2 π ∫ π 0 f (x) cos(nx)dx = 2 π ∫ 0 πx 2 cos(nx)dx This can be further simpliﬁed for n ̸= 0 to an = 2 π [0 − 2 n ∫ π 0 x sin(nx)dx] = 4 n2 (−1) n The coeﬃcient a0 can be evaluated separately as a0 = 2 π ∫ 0 πx 2 dx = 2π2 3 The the Fourier series of f (x) = x2 will be given as x2 = π2 3 + 4 ∞∑ n=1 (−1)n n2 cos(nx). We note that ∫ π −π x4 dx = 2π5 5 < ∞, so that f ∈ L 2(−π, π). Now by parseval’s theorem we have 1 π ∫ π −π f 2(x)dx = a2 0 2 + ∞∑ n=1 (a2 n + b2 n) Using 1 π ∫ π −π x4 dx = 2π4 5 we get 95 4π4 18 + ∞∑ n=1 16 n4 = 2π4 5 This implies ∞∑ n=1 1 n4 = π4 90 Now using the idea of splitting of the series, we have ∞∑ n=1 1 (2n − 1)4 = ∞∑ n=1 1 n4 − 1 16 ∞∑ n=1 1 n4 = 15 16 ∞∑ n=1 1 n4 Substituting the value of ∞∑ k=1 1 n4 in the above equation we get the required sum. 5.5 Heat conduction problems with time-independent inho- mogeneous boundary conditions In this section, we consider heat conduction problems with inhomogeneous boundary conditions. To determine a solution, we exploit the linearity of the problem, which ensures that linear combinations of solutions remain solutions. In particular, we ﬁrst determine a well-chosen particular solution, known as the steady-state solution, which can be used to eliminate the inhomogeneous boundary conditions. This reduces the problem to solving the same boundary value problem but with homo- geneous boundary conditions and an adjusted initial condition. Although the steady-state solution is a natural choice in this case, the selection of a particular solution, as always, is not unique. We will introduce two methods: The separation of variables and a more generally applicable method of eigenfunction expansions. Steady state We convert an inhomogeneous heat equation to a homogeneous problem when the inhomogeneous terms are all time-independent. Dirichlet nonhomogeneous BC Consider the Boundary Value Problem (BVP) modelling heat propagation in a rod where the end points are kept at constant temperatures u0 and u1 : ut = α2uxx, 0 < x < L, t > 0 BC: u(0, t) = u0, u(L, t) = u1, IC: u(x, 0) = f (x), (5.82) 96 Since u0 and u1 are not necessarily zero, we cannot apply directly the method of separation of variables. To solve such a problem, we can proceed as follows. Method (a) Find the steady-state solution (i.e., when ut = 0) which we denote by u∞(x). (5.82) gives u ′′ ∞(x) = 0 ⇒ u∞(x) = Ax + B. Using the boundary conditions, we obtain u∞(0) = B = u0 and u∞(L) = AL + B = u1 ⇒ A = u1−u0 L . Therefore, u∞(x) = ( u1 − u0 L ) x + u0, (Steady-state solution.) (5.83) (b) Let v(x, t) = u(x, t) − u∞(x). We verify that if u(x, t) solves the given BVP (5.82), then v(x, t) solves the following problem vt = α2vxx, 0 < x < L, t > 0 BC: v(0, t) = 0, v(L, t) = 0, IC: v(x, 0) = f (x) − u∞(x). (5.84) Indeed, we have: vt = ut = α2uxx = α2vxx. Hence, vt = α2vxx. Moreover, v(0, t) = u(0, t) − u∞(0) = u0 − u0 = 0 and v(L, t) = u(L, t) − u∞(L) = u1 − u1 = 0. So, v solves (5.84). Now, (5.84), has homogeneous BC and can be solved using the separation of method. Proceeding as in Section 5.2.1, we deduce that v(x, t) = ∞∑ n=1 bne −α2( nπ L ) 2t sin ( nπx L ) , (5.85) where bn = 2 L ∫ L 0 (f (x) − u∞(x)) sin ( nπx L ) dx, n = 1, 2, 3, . . . . (5.86) Finally, the most general solution to (5.82) is given by u(x, t) = v(x, t) + u∞(x) = u0 + ( u1 − u0 L ) x + ∞∑ n=1 bne −α2( nπ L )2t sin ( nπx L ) , (5.87) with bn given by (5.86). Here, we also show a more general method known as the eigenfunction expansions. Eigenfunction expansions In order to solve the boundary value problem (5.84), we could recognize that{sin ( nπx L )}∞ n=1 are eigenfunctions of the spatial operator: − ∂2 ∂x2 97 along with the homogeneous Dirichlet BC v(0, t) = 0 = v(L, t). We therefore assume an eigenfunction expansion of the form: v(x, t) = ∞∑ n=1 ˆvn(t) sin (nπx L ) ∂v ∂t = ∞∑ n=1 ˙ˆvn(t) sin (nπx L ) and ∂2v ∂x2 = − ∞∑ n−1 ˆvn(t) (nπ L )2 sin ( nπx L ) vt = α2vxx ⇒ ∞∑ n=1 { ˙ˆvn(t) + α2 (nπ L )2 ˆvn(t) } sin ( nπx L ) = 0 The sine functions sin ( nπx L ) form an orthogonal set over the interval x ∈ [0, L]. Therefore, the above equation holds if and only if each term in the summation is zero: ˙ˆvn(t) + α2 ( nπ L )2 ˆvn(t) = 0 This is a ﬁrst-order linear ordinary diﬀerential equation for ˆvn(t). The equation is separable and can be solved as follows: ˙ˆvn(t) = −α2 ( nπ L )2 ˆvn(t) This has the general solution: ˆvn(t) = ˆvn(0)e−α2( nπ L ) 2t where ˆvn(0) is the initial condition for ˆvn(t). Therefore, v(x, t) = ∞∑ n=1 ˆvn(0)e −α2( nπ L ) 2t sin (nπx L ) . We have v(x, 0) = ∞∑ n=1 ˆvn(0) sin (nπx L ) = f (x) − u∞(x). Hence by projection, ˆvn(0) = 2 L ∫ L 0 {f (x) − u∞(x)} sin ( nπx L ) dx. which is the same solution as above. 98 Example 5.5.1 We aim to solve the heat conduction problem:  | | ut = uxx, 0 < x < 2, t > 0, u(0, t) = 100, u(2, t) = 100, t > 0, u(x, 0) = 0, 0 < x < 2. (5.88) The steady state function u∞(x) satisﬁes the equation: u′′ ∞(x) = 0, u∞(0) = 100, u∞(2) = 100. Solving this, we obtain: u∞(x) = 100. Deﬁne v(x, t) = u(x, t) − u∞(x). Then v satisﬁes the following boundary value problem:  | | vt = vxx, 0 < x < 2, t > 0, v(0, t) = 0, v(2, t) = 0, t > 0, v(x, 0) = −100, 0 < x < 2. The solutions of the homogeneous part, using separation of variables, are given by: v(x, t) = ∞∑ n=1 Cne− π2n2 4 t sin nπx 2 . The initial condition implies: v(x, 0) = −100 = ∞∑ n=1 Cn sin nπx 2 . The Fourier coeﬃcients are computed as: Cn = ∫ 2 0 (−100) sin nπx 2 dx = 200 [(−1) n − 1] nπ . Thus, we obtain: v(x, t) = 200 π ∞∑ n=1 [(−1) n − 1] n e − π2n2 4 t sin nπx 2 . Rewriting in terms of odd indices: v(x, t) = −400 π ∞∑ k=0 1 2k + 1 e − π2(2k+1)2 4 t sin (2k + 1)πx 2 . Finally, the solution to the original problem is: u(x, t) = 100 − 400 π ∞∑ k=0 1 2k + 1 e− π2(2k+1)2 4 t sin (2k + 1)πx 2 . 99 Mixed Dirichlet-Neumann nonhomogeneous BC We consider the initial-boundary value problem ut = α2uxx, 0 < x < L, t > 0, BC: u(0, t) = u0, ux(L, t) = u1, IC: u(x, 0) = g(x). (5.89) Method (a) Steady-State Solution. For the steady state we set ut = 0, so that u ′′ ∞(x) = 0. Integrating twice, we obtain u∞(x) = Ax + B. The steady state must satisfy the boundary conditions. Since u∞(0) = B = u0, and, because the boundary condition at x = L is on the derivative, u ′ ∞(L) = A = u1, we deduce that u∞(x) = u0 + u1x. (Steady-state solution) (5.90) (b) Reduction to a Homogeneous Problem. Deﬁne v(x, t) = u(x, t) − u∞(x). Since u∞(x) is independent of t, we have vt = ut and vxx = uxx. Thus, v(x, t) satisﬁes vt = α2vxx, 0 < x < L, t > 0. Moreover, the boundary conditions transform as follows: v(0, t) = u(0, t) − u∞(0) = u0 − u0 = 0, vx(L, t) = ux(L, t) − u ′ ∞(L) = u1 − u1 = 0. And the initial condition becomes v(x, 0) = g(x) − u∞(x) = g(x) − ( u0 + u1x) . Thus, v(x, t) satisﬁes the homogeneous problem vt = α2vxx, 0 < x < L, t > 0, v(0, t) = 0, vx(L, t) = 0, v(x, 0) = g(x) − (u0 + u1x). (5.91) 100 (c) Separation of Variables. Assume a solution of the form v(x, t) = X(x)T (t). Substitute into the PDE in (5.91): X(x)T ′(t) = α2X ′′(x)T (t). Dividing by α2X(x)T (t), we obtain T ′(t) α2T (t) = X ′′(x) X(x) = λ, where λ is the separation constant. In order to have a decaying solution in time we set λ = −µ2, µ > 0. Then the spatial ODE becomes X ′′(x) + µ2X(x) = 0, (5.92) with the boundary conditions X(0) = 0, X ′(L) = 0. The general solution of (5.92) is X(x) = A cos(µx) + B sin(µx). The condition X(0) = 0 forces A = 0, hence X(x) = B sin(µx). Enforcing the Neumann condition at x = L, X ′(x) = Bµ cos(µx), X ′(L) = Bµ cos(µL) = 0, we require (for nontrivial B) cos(µL) = 0. This implies µL = (2n + 1)π 2 , n = 0, 1, 2, . . . , so that µn = (2n + 1)π 2L and λn = −µ2 n = − ( (2n + 1)π 2L )2 . Thus, the eigenfunctions are Xn(x) = sin( (2n + 1)πx 2L ). The time-dependent ODE becomes T ′(t) = α2λnT (t) = −α2µ2 nT (t), which has the solution Tn(t) = e−α2µ2 nt. 101 (d) Series Solution. By superposition, the solution to (5.91) is given by v(x, t) = ∞∑ n=0 bn sin ( (2n + 1)πx 2L ) e−α2( (2n+1)π 2L ) 2t, (5.93) where the Fourier coeﬃcients are determined from the initial condition: bn = 2 L ∫ L 0 [ g(x) − (u0 + u1x)] sin ( (2n + 1)πx 2L ) dx. (5.94) (e) Final Solution. Returning to the original variable, we have u(x, t) = v(x, t) + u∞(x). That is, u(x, t) = u0 + u1x + ∞∑ n=0 bn sin ( (2n + 1)πx 2L ) e −α2( (2n+1)π 2L ) 2t, (5.95) with the coeﬃcients bn given in (5.94). Heat conduction with some heat loss and inhomogeneous boundary conditions We consider the initial-boundary value problem ut = α2uxx − u, 0 < x < L, t > 0, BC: u(0, t) = 0, u(L, t) = u1, IC: u(x, 0) = g(x). (5.96) Method (a) Steady-State Solution. To ﬁnd the steady-state solution u∞(x), we set ut = 0 in (5.96) so that α2u ′′ ∞(x) − u∞(x) = 0. This ODE can be rewritten as u′′ ∞(x) − 1 α2 u∞(x) = 0. Its characteristic equation is r2 − 1 α2 = 0, with roots r = ± 1 α . 102 Hence, the general solution is u∞(x) = C1ex/α + C2e−x/α. Applying the boundary condition at x = 0: u∞(0) = C1 + C2 = 0 =⇒ C2 = −C1, so that u∞(x) = C1 (e x/α − e−x/α) = 2C1 sinh ( x α ) . Next, using the condition at x = L: u∞(L) = 2C1 sinh (L α ) = u1, we ﬁnd C1 = u1 2 sinh(L/α). Thus, the steady-state solution is u∞(x) = u1 sinh ( x α) sinh ( L α ) . (5.97) (b) Reduction to a Homogeneous Problem. Deﬁne the transient variable v(x, t) = u(x, t) − u∞(x). Since u∞(x) is independent of t, we have vt = ut and vxx = uxx − u′′ ∞(x). Substituting u(x, t) = v(x, t) + u∞(x) into the PDE (5.96) yields vt = α2( vxx + u ′′ ∞(x) ) − (v + u∞(x)) . But u∞(x) satisﬁes α2u ′′ ∞(x) − u∞(x) = 0, so that the transient variable satisﬁes vt = α2vxx − v. The boundary conditions become v(0, t) = u(0, t) − u∞(0) = 0 − 0 = 0, v(L, t) = u(L, t) − u∞(L) = u1 − u1 = 0, 103 and the initial condition is v(x, 0) = g(x) − u∞(x) = g(x) − u1 sinh ( x α) sinh ( L α ) . Therefore, v(x, t) satisﬁes vt = α2vxx − v, 0 < x < L, t > 0, v(0, t) = 0, v(L, t) = 0, v(x, 0) = g(x) − u1 sinh ( x α) sinh ( L α ) . (5.98) (c) Separation of Variables. Rewrite the PDE for v(x, t) as vt + v = α2vxx. Assume a solution of the form v(x, t) = X(x)T (t). Substituting into the equation gives X(x)T ′(t) + X(x)T (t) = α2X ′′(x)T (t). Dividing by α2X(x)T (t) (assuming nonzero factors) leads to T ′(t) α2T (t) + 1 α2 = X ′′(x) X(x) . Since the left side depends only on t and the right only on x, we set them equal to a constant −µ2: T ′(t) α2T (t) + 1 α2 = −µ2. Multiplying by α2 yields T ′(t) T (t) + 1 = −α2µ2. Hence, the timedependent equation is T ′(t) = (−α2µ2 − 1 )T (t) =⇒ T (t) = e(−α2µ2−1)t. The spatial part satisﬁes X ′′(x) X(x) = −µ2 =⇒ X ′′(x) + µ2X(x) = 0. With the boundary conditions X(0) = 0 and X(L) = 0 the eigenfunctions are Xn(x) = sin (nπx L ) , µ = nπ L , n = 1, 2, 3, . . . . Thus, for each n we have Tn(t) = e−t−α2( nπ L ) 2t. 104 (d) Series Representation and Final Solution. By superposition, the solution to the homogeneous problem for v(x, t) is given by v(x, t) = ∞∑ n=1 bn sin ( nπx L ) e −t−α2( nπ L ) 2t, where the Fourier sine coeﬃcients are determined from the initial condition: bn = 2 L ∫ L 0 [ g(x) − u1 sinh ( x α ) sinh ( L α ) ] sin (nπx L ) dx. (5.99) Returning to the original variable, the full solution is u(x, t) = u∞(x) + v(x, t) = u1 sinh ( x α ) sinh ( L α ) + ∞∑ n=1 bn sin ( nπx L ) e −t−α2( nπ L ) 2t, (5.100) where bn are given by (5.99). Heat Conduction problems with distributed time-independent sources We consider the initial-boundary value problem ut = α2uxx + x, 0 < x < L, t > 0, BC: u(0, t) = 0, u(L, t) = B, IC: u(x, 0) = g(x). (5.101) Method (a) Steady-State Solution. To ﬁnd the steady-state solution u∞(x), we set ut = 0 in (5.101), leading to the ODE α2u ′′ ∞(x) + x = 0. Integrating twice, we obtain α2u ′ ∞(x) = −x2 2 + C1, u∞(x) = − x3 6α2 + C1x + C2. Applying the boundary conditions u∞(0) = 0 and u∞(L) = B, we get C2 = 0, − L 3 6α2 + C1L = B. Solving for C1, we ﬁnd C1 = B + L 3/6α2 L . 105 Thus, the steady-state solution is u∞(x) = − x3 6α2 + (B + L 3/6α2 L ) x. (5.102) (b) Reduction to a Homogeneous Problem. Deﬁne the transient variable v(x, t) = u(x, t) − u∞(x). Since u∞(x) is independent of t, we have vt = ut, vxx = uxx − u′′ ∞(x). Substituting u(x, t) = v(x, t) + u∞(x) into (5.101) gives vt = α2(vxx + u′′ ∞(x)) + x. But from (5.102), we know u ′′ ∞(x) = −x/α2, so vt = α2vxx + x − x = α2vxx. The boundary conditions remain homogeneous: v(0, t) = u(0, t) − u∞(0) = 0, v(L, t) = u(L, t) − u∞(L) = 0. The initial condition is v(x, 0) = g(x) − u∞(x). (c) Separation of Variables. Assume a solution of the form v(x, t) = X(x)T (t). Substituting into the homogeneous equation vt = α2vxx gives X(x)T ′(t) = α2X ′′(x)T (t). Dividing by X(x)T (t) leads to T ′(t) α2T (t) = X ′′(x) X(x) . Since the left-hand side depends only on t and the right-hand side only on x, both must equal a constant λ: T ′(t) α2T (t) = λ, T ′(t) = α2λT (t). Solving for T (t), we get T (t) = e α2λt. The spatial equation becomes X ′′(x) = λX(x). 106 For nontrivial solutions under homogeneous Dirichlet conditions X(0) = X(L) = 0, we set λ = −k2, k = nπ L , n = 1, 2, 3, . . . . The eigenfunctions are Xn(x) = sin (nπx L ) , and the separation constants are λn = − (nπ L )2 . Hence, the time-dependent part is Tn(t) = e −α2( nπ L ) 2t. (d) Series Representation and Final Solution. By superposition, the solution to the homogeneous problem for v(x, t) is given by v(x, t) = ∞∑ n=1 bn sin ( nπx L ) e −α2( nπ L )2t, where the Fourier sine coeﬃcients are determined from the initial condition: bn = 2 L ∫ L 0 [g(x) − u∞(x)] sin (nπx L ) dx. (5.103) Finally, recalling that u(x, t) = v(x, t) + u∞(x), we obtain the full solution: u(x, t) = − x3 6α2 + ( B + L3/6α2 L ) x + ∞∑ n=1 bn sin ( nπx L ) e −α2( nπ L ) 2t, (5.104) where bn are given by (5.103). We remark that lim x→∞ u(x, t) = x {B L + 1 6α2 ( L2 − x2)} . More generally, if we consider the problem: ut = α2uxx + h(x), 0 < x < L, t > 0, BC: u(0, t) = A, u(L, t) = B, IC: u(x, 0) = f (x), (5.105) which represents heat ﬂow with a time-independent source and/or ends ﬁxed at some temperature. The expectation is that over time, the heat will approach a steady state (equilibrium): ¯u(x) = lim t→∞ u(x, t). 107 Formally, we can obtain this equilibrium shape as follows: if ¯u(x) is a steady-state, then it solves the PDE but does not depend on time. Thus it must satisfy 0 = α2 ¯uxx + h(x), ¯u(0) = A, ¯u(L) = B. This we can then solve. The important point is that the diﬀerence between the PDE solution and steady state, v = u − ¯u solves the homogeneous problem vt = α2vxx, 0 < x < L, t > 0, BC: v(0, t) = 0, v(L, t) = 0, IC: u(x, 0) = f (x) − ¯u(x), (5.106) So to solve (5.105) we can ﬁnd the steady state (formally), subtract it out and then solve (5.106) for the “homogeneous” part. Particular solution in some cases, the previous method may fail. This trick works when there is a steady state and only when the source term and boundary conditions do not depend on time. For instance, ut = tuxx + sin x cannot be solved using this method. Assuming ut = 0 is not enough since we also need to take t → ∞ and we cannot ﬁnd a u = ¯u(x) that solves tuxx + sin x = 0 We can, however, guess a particular solution of the equation or more generally solve the full problem using the eigenfunction method. Neumann nonhomogeneous BC We wish to solve ut = α2uxx, 0 < x < L, t > 0, BC: ux(0, t) = A, ux(L, t) = B, IC: u(x, 0) = g(x). (5.107) Method (a) Finding a Particular Solution. Try for a steady solution: u′′ ∞(x) = 0, u∞(x) = ax + b, ux = a but then we cannot match both BC unless A = B = a. This means that if we are pumping and removing heat from the rod at diﬀerent rates then the temperature does not reach a steady state. Since the boundary conditions are given on the spatial derivative, we ﬁrst look for a func- tion ϕ(x, t) that satisﬁes both the PDE and the nonhomogeneous Neumann conditions. 108 We assume a solution of the form ϕ(x, t) = ax2 + bx + c + dt, where a, b, c, d are constants (possibly depending on the data). We have ϕt = d and ϕxx = 2a. Substituting into the PDE ϕt = α2ϕxx gives d = α2(2a) =⇒ a = d 2α2 . Next, we require that ϕ satisﬁes the boundary conditions on the derivative. Since ϕx(x, t) = 2ax + b, we impose ϕx(0, t) = b = A, and at x = L ϕx(L, t) = 2aL + b = B. Thus, 2aL = B − A =⇒ a = B − A 2L . Then, from a = d 2α2 we deduce d = α2 B − A L . We may choose c = 0 for simplicity. Hence, a particular solution is given by ϕ(x, t) = B − A 2L x2 + Ax + α2(B − A) L t. (5.108) (b) Reduction to a Homogeneous Problem. Deﬁne v(x, t) = u(x, t) − ϕ(x, t). Since both u and ϕ satisfy the heat equation, it follows that vt = ut − ϕt = α2uxx − α2ϕxx = α2vxx. The boundary conditions for v become vx(0, t) = ux(0, t) − ϕx(0, t) = A − A = 0, vx(L, t) = ux(L, t) − ϕx(L, t) = B − B = 0. The initial condition is v(x, 0) = u(x, 0) − ϕ(x, 0) = g(x) − [B − A 2L x2 + Ax] . Thus, v(x, t) satisﬁes the homogeneous problem vt = α2vxx, 0 < x < L, t > 0, vx(0, t) = 0, vx(L, t) = 0, v(x, 0) = g(x) − [ B − A 2L x2 + Ax] . (5.109) 109 (c) We know from Section 5.2.2 that the solution to (5.109) can be written as v(x, t) = a0 2 + ∞∑ n=1 an cos ( nπx L ) e −α2( nπ L )2t, where the Fourier coeﬃcients are determined from an = 2 L ∫ L 0 [g(x) − ( B − A 2L x2 + Ax)] cos (nπx L ) dx, n = 0, 1, . . . . (5.110) (d) Final Solution. Recalling that u(x, t) = v(x, t) + ϕ(x, t), with ϕ(x, t) given in (5.108), we obtain the full solution u(x, t) = B − A 2L x2 + Ax + α2(B − A) L t + a0 2 + ∞∑ n=1 an cos ( nπx L ) e−α2( nπ L ) 2t, (5.111) where the coeﬃcients an are given by (5.110). Solving the non-homgeneous heat equation with homogeneous BC The aim of this section is to solve the following non-homogeneous heat equation: ut = α2uxx + F (x, t) for 0 < x < L, t > 0 u(0, t) = 0 and u(L, t) = 0 for t > 0 u(x, 0) = f (x) for 0 ≤ x ≤ L, (5.112) where F (x, t) represents a source of heat energy in the medium. Method We know that the eigenvalues and eigenfunctions associated with Dirichlet homogeneous BC are λn = (nπ L )2 n = 1, 2, . . . Xn(x) = sin ( nπx L ) . For the non-homogeneous problem, we will take a cue from that case and attempt a solution u(x, t) = ∞∑ n=1 Tn(t) sin (nπx L ) . (5.113) The problem is to determine each Tn(t). We observe that, for a given t, equation (5.113) can be interpreted as the Fourier sine expansion of u(x, t), considered as a function of x, with Tn(t) being the nth Fourier coeﬃcient in this expansion, carefully establish the expression for Tn(t). Therefore, by projecting u(x, t) on the basis {sin ( nπx L )} and using orthogonality relations, we can deduce that Tn(t) = 2 L ∫ L 0 u(ξ, t) sin ( nπξ L ) dξ. (5.114) 110 Next, we assume that for each t ≥ 0, F (x, t), as a function of x, can also be expanded in a Fourier sine series: F (x, t) = ∞∑ n=1 Bn(t) sin (nπx L ) dx (5.115) where Bn(t) = 2 L ∫ L 0 F (ξ, t) sin ( nπξ L ) dξ (5.116) is the coeﬃcient in this expansion, and may depend on t. We diﬀerentiate equation (5.114) to obtain T ′ n(t) = 2 L ∫ L 0 ut(ξ, t) sin ( nπξ L ) dξ Since ut = α2uxx + F (x, t), the previous equation becomes T ′ n(t) = 2α2 L ∫ L 0 uxx(ξ, t) sin ( nπξ L ) dξ + 2 L ∫ L 0 F (ξ, t) sin ( nπξ L ) dξ In view of equation (5.116), T ′ n(t) = 2α2 L ∫ L 0 uxx(ξ, t) sin ( nπξ L ) dξ + Bn(t). (5.117) Apply integration by parts twice to the integral in equation (5.117), using the boundary con- ditions and, at the last step, equation (5.114): ∫ L 0 uxx(ξ, t) sin ( nπξ L ) dξ = [ sin ( nπξ L ) ux(ξ, t)]L 0 − ∫ L 0 ux(ξ, t)nπ L cos ( nπξ L ) dξ = − ∫ L 0 nπ L ux(ξ, t) cos ( nπξ L ) dξ = [ −nπ L u(ξ, t) cos ( nπξ L )]L 0 + nπ L ∫ L 0 u(ξ, t) (−nπ L ) sin ( nπξ L ) dξ = −n 2π2 L2 ∫ L 0 u(ξ, t) sin ( nπξ L ) dξ = −n 2π2 L2 · L 2 Tn(t) = −n 2π2 2L Tn(t). Substituting this into equation (5.117) yields T ′ n(t) = −α2 n 2π2 L2 Tn(t) + Bn(t) For n = 1, 2, · · · , we now have an ordinary diﬀerential equation for Tn(t) : T ′ n + α2 n2π2 L2 Tn = Bn(t). (5.118) 111 Next, use equation (5.114) to obtain the condition, Tn(0) = 2 L ∫ L 0 u(ξ, 0) sin ( nπξ L ) dξ = 2 L ∫ L 0 f (ξ) sin ( nπξ L ) dξ = bn (5.119) the n-th coeﬃcient in the Fourier sine expansion of f on [0, L]. We solve the ordinary diﬀerential equation (5.119) subject to the condition Tn(0) = bn to obtain the unique solution Tn(t) = ∫ t 0 e−α2n2π2(t−τ )/L2Bn(τ )dτ + bne −α2n2π2t/L2. Finally, substitute this into equation (5.113) to obtain u(x, t) = ∞∑ n=1 (∫ t 0 e−α2n2π2(t−τ )/L2Bn(τ )dτ ) sin (nπx L ) + ∞∑ n=1 bn sin ( nπx L ) e−α2n2π2t/L2, (5.120) where bn = 2 L ∫ L 0 f (ξ) sin ( nπξ L ) dξ and Bn is given (5.116). Example 5.5.2 Solve the following partial diﬀerential equation:  |||| |||| ut = 4uxx + t 2 cos(x/2), 0 < x < π, t > 0, u(0, t) = u(π, t) = 0, t ≥ 0, u(x, 0) = { 0, 0 ≤ x ≤ π/2, 25, π/2 < x < π. (5.121) We let F (x, t) = t 2 cos(x/2) and L = π. Using (5.116), we compute Bn(t) = 2 π ∫ π 0 t 2 cos(ξ/2) sin(nξ)dξ = 8 π 2n 4n2 − 1t 2. Now we can evaluate the integral occurring in the ﬁrst summation of equation (5.120): ∫ t 0 8 π 2n 4n2 − 1 τ 2e−4n2(t−τ )dτ = 1 2 −4n2t + 8n4t 2 + 1 − e−4n2t n5π (4n2 − 1) . Finally, compute bn = 2 π ∫ π 0 f (ξ) sin(nξ)dξ = 2 π ∫ π π/2 25 sin(nξ)dξ = 50 nπ (cos(nπ/2) − (−1) n) . 112 The solution is u(x, t) = ∞∑ n=1 (1 2 −4n 2t + 8n4t 2 + 1 − e−4n2t n5π (4n2 − 1) ) sin(nx) + ∞∑ n=1 50 nπ (cos(nπ/2) − (−1) n) sin(nx)e −4n2t. We note that the second summation in u(x, t) is the solution of the initial-boundary value problem if the source term t 2 cos(x/2) is omitted. If we denote this solution as uh(x, t) (the subscript h is for the fact that the heat equation without F (x, t) is homogeneous), then uh(x, t) = ∞∑ n=1 50 nπ (cos(nπ/2) − (−1) n) sin(nx)e−4n2t and u(x, t) = uh(x, t) + ∞∑ n=1 (1 2 −4n2t + 8n4t 2 + 1 − e −4n2t n5π (4n2 − 1) ) sin(nx). This way of writing the solution clariﬁes which terms in the solution arise from the t 2 cos(x/2) term in the partial diﬀerential equation. 5.6 Solving the non-homgeneous heat equation with non- homogeneous time dependent BC Now, we aim to solve the following non-homogeneous heat equation: ut = α2uxx + F (x, t) for 0 < x < L, t > 0 u(0, t) = ϕ0(t) and u(L, t) = ϕ1(t) for t > 0 u(x, 0) = f (x) for 0 ≤ x ≤ L. (5.122) Method It is too much to try to ﬁnd a simple function that satisﬁes the PDE and the BCs simultane- ously. However, we can instead compromise and ﬁnd a function w that satisﬁes the boundary conditions but not the PDE. There are many choices for such a function w. For Dirichlet BCs, the easiest is just to construct a line that goes from (0, ϕ0(t)) to (L, ϕ1(t)): w(x, t) = ϕ0(t) + x ( ϕ1(t) − ϕ0(t) L ) . Now let u(x, t) = w(x, t) + v(x, t). Then, v(x, t) is the solution of the following: 113 vt = c2vxx + F (x, t) − ˙ϕ0(t) − x ( ˙ϕ1(t) − ˙ϕ0(t) L ) , t > 0, 0 < x < L, v(0, t) = 0, v(L, t) = 0, t > 0, v(x, 0) = f (x) − ϕ0(0) − x ( ϕ1(0) − ϕ0(0) L ) . (5.123) We then obtain a system similar to (5.112). Hence, we can use the eigenfunction expansions method to solve the system. Example 5.6.1 We aim to solve the following equation: ut = α2uxx for 0 < x < L, t > 0 u(0, t) = At and u(L, t) = 0 for t > 0 u(x, 0) = 0 for 0 ≤ x ≤ L. (5.124) In this case, w(x, t) = At + x L(0 − At) = At ( 1 − x L ) . Next let u(x, t) = w(x, t) + v(x, t), then vt = α2vxx − A (1 − x L ) v(0, t) = 0 = v(L, t) v(x, 0) = 0 We know that the eigenvalues and eigenfunctions associated with Dirichlet homogeneous BC are λn = (nπ L )2 n = 1, 2, . . . Xn(x) = sin ( nπx L ) . So we let v(x, t) = ∞∑ n=1 ˆvn(t) sin (nπx L ) vt = ∞∑ n=1 ˙ˆvn(t) sin (nπx L ) , vxx = − ∞∑ n=1 ˆvn(t) (nπ L )2 sin ( nπx L ) . Moreover, we write s(x, t) = −A (1 − x L ) = ∞∑ n=1 ˆsn(t) sin ( nπx L ) where ˆsn = 2 L ∫ L 0 A ( x L − 1 ) sin ( nπx L ) dx = −2A nπ . In addition, the initial condition gives ˆvn(0) = 0. Therefore, 0 = vt − α2vxx − s(x, t) = ∞∑ n=1 { ˙ˆvn(t) + α2 (nπ L )2 ˆvn + 2A nπ } sin (nπx L ) . 114 Since {sin ( nπx L )} form a basis, we deduce that ˙ˆvn(t) + α2 (nπ L )2 ˆvn(t) = −2A nπ . This is a ﬁrst-order linear ordinary diﬀerential equation. We can solve it using the integrating factor method. The equation becomes: d dt (e α2( nπ L )2tˆvn(t) ) = −2A nπ eα2( nπ L )2t. Now, integrating both sides: e α2( nπ L ) 2tˆvn(t) = − 2AL2 α2(nπ)3 e α2( nπ L ) 2t + Bn. Here, Bn is the constant of integration. Solving for ˆvn(t), we get: ˆvn(t) = − 2AL2 α2(nπ)3 + Bne −α2( nπ L )2t To determine Bn, we use the initial condition ˆvn(0) = 0: 0 = ˆvn(0) = − 2AL2 α2(nπ)3 + Bn. Solving for Bn, we ﬁnd: Bn = 2AL2 α2(nπ)3 . Thus, the solution for ˆvn(t) becomes: ˆvn(t) = 2AL2 α2(nπ)3 (e −α2( nπ L )2t − 1 ) . Finally, the solution for u(x, t) is obtained by summing over all modes n: u(x, t) = At (1 − x L ) + 2AL 2 π3α2 ∞∑ n=1 (e−α2( nπ L ) 2t − 1 ) n3 sin (nπx L ) . 115CategoryDirichletNeumannPeriodicBoundaryConditionsϕ(0)=0,ϕ(L)=0ϕ′(0)=0,ϕ′(L)=0ϕ(−L)=ϕ(L),ϕ′(−L)=ϕ′(L)Eigenvaluesλn=(nπL)2,n=1,2,3,...λn=(nπL)2,n=0,1,2,...λn=(nπL)2,n=0,1,2,...Eigenfunctionsϕn(x)=sin(nπxL)ϕn(x)=cos(nπxL)Bothϕn(x)={cos(nπxL),sin(nπxL)SeriesExpansionf(x)=∞∑n=1bnsin(nπxL)f(x)=a02+∞∑n=1ancos(nπxL)f(x)=a02+∞∑n=1[ancos(nπxL)+bnsin(nπxL)]Coeﬃcientsbn=2L∫L0f(x)sin(nπxL)dxan=2L∫L0f(x)cos(nπxL)dxan=1L∫L−Lf(x)cos(nπxL)dx,bn=1L∫L−Lf(x)sin(nπxL)dxTable5.1:Boundaryvalueproblemfortheequationd2ϕdx2=−λϕ 116 Chapter 6 The Wave Equation In this chapter, we will explore the solution of the one-dimensional wave equation on a ﬁnite domain. The ﬁrst approach is by using the method of separation of variables. The approach is quite similar to that used for the heat equation, but with key diﬀerences. In this case, the time equation becomes a second-order ordinary diﬀerential equation (ODE) with an indicial equation that yields complex roots. This results in time-dependent functions that are sines and cosines, rather than the exponential decay found in the heat equation. Depending on the boundary conditions applied to the spatial ODE, we derive eigenvalue problems that are analogous to those encountered in the heat equation case. Each eigenfunction corresponds to a particular periodic extension of the solution. For instance, Dirichlet boundary conditions lead to eigenfunctions that are sine functions, which are associated with the odd periodic extension of the solution deﬁned on the domain (0, L). We will also to derive the classical D’Alembert Solution to the wave equation on the domain (−∞, +∞) with prescribed initial displacements and velocities. This solution fully describes the equations of motion of an inﬁnite elastic string that has a prescribed shape and initial velocity. We will demonstrate, using separation of variables, that the solution to the wave equation on a ﬁnite domain is essentially the D’Alembert solution. In this context, the initial condition functions are periodic extensions that match the boundary conditions speciﬁc to the problem at hand. 6.1 Solution of the Wave Equation by separation of variables In this section we will apply the method of separation of variables to the one dimensional wave equation, given by ∂2u ∂2t = c2 ∂2u ∂x2 , 0 < x < L, t > 0 BC: u(0, t) = 0, u(L, t) = 0, IC: u(x, 0) = f (x) ut(x, 0) = g(x), 0 < x < L. (6.1) This problem concerns wave propagation on a string of length L with ﬁxed ends, meaning the displacement at both ends is zero. The function u(x, t) represents the vertical displacement of the string as a function of time. The wave equation is derived under the assumptions that the displacement remains small and the string is uniform. The wave speed c is given by c = √ τ µ 117 where τ represents the tension in the string and µ is the mass per unit length. This principle is evident in string instruments. By adjusting the tension, diﬀerent tones can be produced, while the properties of the string–such as material (e.g., nylon or steel) and thickness–also inﬂuence the sound. A thicker string increases mass density, which in turn lowers the frequency, explaining why piano bass strings produce deeper notes. The term utt represents the acceleration of a segment of the string, while uxx describes its concavity. A positive concavity indicates an upward curve, meaning adjacent points pull toward equilibrium. Conversely, a negative concavity leads to downward acceleration. As usual, we let u(x, t) = X(x)T (t). Then we ﬁnd X ¨T = c2X ′′T which can be rewritten as 1 c2 ¨T T = X ′′ X Again, we have separated the functions of time on one side and space on the other side. Therefore, we set each function equal to a constant, λ. 1 c2 ¨T T = X ′′ X = λ This leads to two equations: ¨T = c2λT X ′′ = λX. As before, we have the boundary conditions on X(x) : X(0) = 0, and X(L) = 0 giving the solutions, Xn(x) = sin (nπx L ) , λn = − (nπ L )2 , n = 1, 2, . . . The main diﬀerence from the solution of the heat equation is the form of the time equation which is a second-order ODE. T ′′ + (nπc L )2 T = 0. The solutions to this latter equation are Tn(t) = An cos ( nπct L ) + Bn sin ( nπct L ) . The general solution, a superposition of all product solutions, is given by u(x, t) = ∞∑ n=1 [An cos ( nπct L ) + Bn sin ( nπct L )] sin (nπx L ) . (6.2) This solution satisﬁes the wave equation and the boundary conditions. We still need to satisfy the initial conditions. Note that there are two initial conditions, since the wave equation is second order in time. First, we have u(x, 0) = f (x). Thus, f (x) = u(x, 0) = ∞∑ n=1 An sin (nπx L ) . (6.3) 118 Hence, An = 2 L ∫ L 0 f (x) sin (nπx L ) dx. (6.4) In order to obtain the condition on the initial velocity, ut(x, 0) = g(x), we need to diﬀerentiate the general solution with respect to t : ut(x, t) = ∞∑ n=1 nπc L [−An sin (nπct L ) + Bn cos (nπct L )] sin ( nπx L ) . (6.5) Then, we have from the initial velocity g(x) = ut(x, 0) = ∞∑ n=1 nπc L Bn sin (nπx L ) . (6.6) Hence, Bn = 2 nπc ∫ L 0 g(x) sin (nπx L ) dx. (6.7) Example 6.1.1 We aim to solve the following Wave equation: ∂2u ∂t2 = c2 ∂2u ∂x2 for all 0 < x < 1 and t > 0 u(0, t) = u(1, t) = 0 for all t > 0 u(x, 0) = x(1 − x) for all 0 < x < 1 ut(x, 0) = 0 for all 0 < x < 1 This is a special case of equation (6.1) with L = 1, f (x) = x(1 − x) and g(x) = 0. So, by (8), u(x, t) = ∞∑ k=1 sin(kπx) [Ak cos(ckπt) + Bk sin(ckπt)] with Ak = 2 ∫ 1 0 x(1 − x) sin(kπx)dx, Bk = 0 We have ∫ 1 0 x sin(kπx)dx = − cos(kπ) 1 kπ ∫ 1 0 x2 sin(kπx)dx = cos(kπ)2 − k2π2 k3π3 − 2 k3π3 . Hence, Ak = 2 ∫ 1 0 x(1 − x) sin(kπx)dx = 4 k3π3 [1 − cos(kπ)] = { 8 k3π3 for k odd 0 for k even and u(x, t) = ∞∑ k=1 k odd 8 k3π3 sin(kπx) cos(ckπt). 119 Example 6.1.2 Now we consider ∂2u ∂t2 = c2 ∂2u ∂x2 for all 0 < x < 1 and t > 0 u(0, t) = u(1, t) = 0 for all t > 0 u(x, 0) = sin(5πx) + 2 sin(7πx) for all 0 < x < 1 ut(x, 0) = 0 for all 0 < x < 1. This is again a special case of equations (6.1) with L = 1. So, u(x, t) = ∞∑ k=1 sin(kπx) [Ak cos(ckπt) + Bk sin(ckπt)] . This time it is very ineﬃcient to use the integral formulae to evaluate Ak and Bk. It is easier to observe directly, just by matching coeﬃcients, that sin(5πx) + 2 sin(7πx) = u(x, 0) = ∞∑ k=1 Ak sin(kπx) ⇒ Ak =  | | 1 if k = 5 2 if k = 7 0 if k ̸= 5, 7 0 = ut(x, 0) = ∞∑ k=1 ckπBk sin(kπx) ⇒ Bk = 0. u(x, t) = sin(5πx) cos(5cπt) + 2 sin(7πx) cos(7cπt). Note The following observations are in order. • Period and Frequency of Vibration: cos (nπc L (t + T ) ) = cos ( nπct L ) , provided nπcT L = 2π. Thus, the period of mode n is given by: Tn = ( 2L c ) 1 n which represents the seconds per cycle. The natural frequencies of vibration are: fn = 1 Tn = n ( c 2L ) . • Modes of Vibration: Standing waves have a wavelength given by: λn = 2L n . In the following four ﬁgures, we plot the ﬁrst four modes of vibration. The ﬁrst, known as the 120 fundamental mode of vibration, is associated with the lowest frequency: f1 = 1 T1 = ( c 2L ) . All higher frequencies, also known as overtones, are integer multiples of this fundamental fre- quency. The nodes in these modal plots are indicated by solid circles, which represent the points at which the displacement associated with a given mode is zero. (a) Fundamental mode X1(x) = sin ( πx L ) (b) Second mode X2(x) = sin ( 2πx L ) (c) Third mode X3(x) = sin ( 3πx L ) (d) Fourth mode X4(x) = sin ( 4πx L ) 6.2 Non-homogeneous Wave Equation We consider the wave equation with a source: utt = c2uxx + s(x, t) BCs: u(0, t) = u(L, t) = 0 ICs u(x, 0) = f (x), ut(x, 0) = g(x) To solve this, we ﬁrst look for a particular solution v(x, t) of the PDE and boundary conditions. Then the general solution will be u(x, t) = v(x, t) + w(x, t), where w(x, t) is the general solution of the homogeneous PDE utt = c2uxx and boundary conditions. To satisfy our initial conditions, we must take the initial conditions for w as w(x, 0) = f (x) − v(x, 0), wt(x, 0) = g(x) − vt(x, 0). Steady state If the source term s(x, t) does not depend on the time t (so we can write s(x, t) = s(x) ), then we can look for v(x, t) = v(x) not depending on the time t. The PDE becomes 0 = c2v′′ + s(x), and we must solve this subject to the boundary conditions v(0) = v(L) = 0. In this case it can be solved by integrating twice. Example 6.2.1 Consider the problem utt = uxx + x boundary conditions u(0, t) = u(1, t) = 0 initial conditions u(x, 0) = 0, ut(x, 0) = 1. 121 The diﬀerential equation says v′′ = −x. One integration gives v′ = −x2/2 + A where A is a constant, another gives v = −x3/6 + Ax + B. For v(0) = 0 we need B = 0, and then for v(1) = 0 we need −1/6 + A = 0 or A = 1/6. So v(x) = (x − x3) /6 is our particular solution. The other part of the solution, w(x, t), satisﬁes wtt = wxx boundary conditions w(0, t) = w(1, t) = 0 initial conditions w(x, 0) = − ( x − x3) /6, wt(x, 0) = 1 We could use a Fourier series for this: w(x, t) = ∞∑ n=1 sin(nπx) (an cos(nπt) + bn sin(nπt)) , where an = −1 3 ∫ 1 0 (x − x3) sin(nπx)dx = 2(−1) n n3π3 bn = 2 nπ ∫ 1 0 sin(nπx)dx = 2 (1 − (−1) n) n2π2 . And thus the complete solution is u(x, t) = x − x3 6 + ∞∑ n=1 ( 2(−1) n n3π3 cos(nπt) + 2 (1 − (−1)n) n2π2 sin(nπt) ) sin(nπx). Exponential in t If the source term is a function of x times an exponential in t, we may look for a particular solution v(x, t) that is also of this form. For example, consider utt = uxx + xe−t BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = 0, ut(x, 0) = 1 We look for a solution of the form v(x, t) = V (x)e −t. Then the diﬀerential equation says V (x)e −t = V ′′(x)e −t + xe−t or V = V ′′ + x. The general solution of this diﬀerential equation is V (x) = x + c1e x + c2e−x. The boundary conditions say V (0) = 0 = c1 + c2 and V (1) = 0 = 1 + c1e + c2e−1. Solving for c1 and c2 we get c1 = −e/ (e 2 − 1) , c2 = e/ (e 2 − 1), i.e. v(x, t) = ( x − e1+x e2 − 1 + e1−x e2 − 1 ) e−t The initial conditions for w(x, t) are w(x, 0) = −v(x, 0) = −V (x) = −x + e1+x − e 1−x e2 − 1 wt(x, 0) = 1 − vt(x, 0) = 1 + V (x) = 1 + x − e 1+x − e1−x e2 − 1 122 Arbitrary function of x and t We will use an eigenfunction expansion. We can write u(x, t) and s(x, t) for any t as such a series, obtaining series expansions where the coeﬃcients are functions of t : u(x, t) = ∞∑ n=1 bn(t)ϕn(x) s(x, t) = ∞∑ n=1 cn(t)ϕn(x) Our PDE will give us relations between these, which will be ordinary diﬀerential equations in bn(t) for each n. Example 6.2.2 Consider the problem utt = uxx + xt BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = 0, ut(x, 0) = 1 The appropriate eigenfunctions for the homogeneous problem are ϕn(x) = sin(nπx), the ex- pansion being the Fourier sine series on the interval [0, 1]. In particular, xt = ∞∑ n=1 2(−1) n+1t nπ sin(nπx) so cn(t) = 2(−1) n+1t/(nπ). Putting these series into the diﬀerential equation, we get ∞∑ n=1 b′′ n(t) sin(nπx) = − ∞∑ n=1 bn(t)(nπ)2 sin(nπx) + ∞∑ n=1 cn(t) sin(nπx). By the uniqueness of Fourier series, the coeﬃcients for each n must match, i.e. b′′ n = −(nπ) 2bn + cn(t) = −(nπ) 2bn − 2(−1) nt nπ . The initial conditions for u and ut give us initial conditions for bn and b′ n : u(x, 0) = 0 so bn(0) = 0, and ut(x, 0) = 1 = ∑∞ n=1 2(1−(−1)n nπ sin(nπx) so b′ n(0) = 2(1−(−1)n nπ . The general solution of the diﬀerential equation for bn is bn(t) = An cos(nπt) + Bn sin(nπt) − 2(−1)nt (nπ)3 . From the initial conditions we get bn(0) = 0 = An and b′ n(0) = 2(1−(−1)n nπ = nπBn− 2(−1)n (nπ)3 , so Bn = 2(−1)n (nπ)4 + 2(1−(−1)n) (nπ)2 . The complete solution is u(x, t) = ∞∑ n=1 (( 2(−1) n (nπ)4 + 2 (1 − (−1) n) (nπ)2 ) sin(nπt) − 2(−1) nt (nπ)3 ) sin(nπx). 123 6.3 d’Alembert’s solution of the Wave Equation A general solution of the one-dimensional wave equation can be found. This solution was ﬁrst derived by Jean-Baptiste le Rond d’Alembert (1717–1783) and is referred to as d’Alembert’s formula. In this section, we derive d’Alembert’s formula and then use it to obtain solutions to the wave equation on inﬁnite, semi-inﬁnite, and ﬁnite intervals. We consider the wave equation in the form utt = c2uxx, (6.8) and introduce the transformation u(x, t) = U (ξ, η), where ξ = x + ct and η = x − ct. (6.9) Note that ξ and η are the characteristics of the wave equation. We also need to see how the derivatives transform. For example, ∂u ∂x = ∂U (ξ, η) ∂x = ∂U (ξ, η) ∂ξ ∂ξ ∂x + ∂U (ξ, η) ∂η ∂η ∂x = ∂U (ξ, η) ∂ξ + ∂U (ξ, η) ∂η . Thus, as an operator, we have ∂ ∂x = ∂ ∂ξ + ∂ ∂η . Similarly, one can show that ∂ ∂t = c ∂ ∂ξ − c ∂ ∂η . Using these results, the wave equation becomes 0 = utt − c2uxx = ( ∂ ∂t + c ∂ ∂x )( ∂ ∂t − c ∂ ∂x ) u == ( c ∂ ∂ξ − c ∂ ∂η + c ∂ ∂ξ + c ∂ ∂η ) ( c ∂ ∂ξ − c ∂ ∂η − c ∂ ∂ξ − c ∂ ∂η ) U = −4c2 ∂2U ∂ξ ∂η . This simpliﬁes to Uξη = 0. A further integration gives U (ξ, η) = F (ξ) + G(η). (6.10) Thus, the general solution of the wave equation is u(x, t) = F (x + ct) + G(x − ct), (6.11) 124 where F and G are two arbitrary, twice diﬀerentiable functions. As t increases, F (x + ct) shifts to the left and G(x − ct) shifts to the right, so that the solution may be seen as the sum of left- and right-traveling waves. We now use initial conditions to determine the unknown functions. Suppose that u(x, 0) = f (x), ut(x, 0) = g(x), |x| < ∞. (6.12) Then, applying these to the general solution, we obtain f (x) = F (x) + G(x), (6.13) g(x) = c[ F ′(x) − G ′(x) ] . (6.14) Integrating (6.14), we have 1 c ∫ x 0 g(s)dx = F (x) − G(x) − F (0) + G(0). (6.15) Adding this result to (6.13), gives F (x) = 1 2f (x) + 1 2c ∫ x 0 g(s)ds + 1 2[F (0) − G(0)]. (6.16) Subtracting from (6.13), gives G(x) = 1 2f (x) − 1 2c ∫ x 0 g(s)ds − 1 2[F (0) − G(0)]. (6.17) Now we can write out the solution u(x, t) = F (x + ct) + G(x − ct), the d’Alembert solution: u(x, t) = 1 2 [ f (x + ct) + f (x − ct)] + 1 2c ∫ x+ct x−ct g(s) ds. (6.18) Example 6.3.1 The d’Alembert solution of the wave equation: utt = 7uxx BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = x2, ut(x, 0) = cos(3x) is given by u(x, t) = (x + √7t) 2 + (x − √ 7t)2 2 + sin(3(x + √7t)) − sin(3(x − √7t)) 6 √7 . 125 Example 6.3.2 We consider the following Wave equation: utt = c2uxx for − ∞ < x < ∞ and t > 0 u(x, 0) = f (x) = { 2 if − 1 < x < 1 0 otherwise ut(x, 0) = 0. Using d’Alembert’s solution u(x, t) = 1 2 [f (x + ct) + f (x − ct)] We note that: • Along lines where x + ct is constant the term f (x + ct) is constant. • Likewise along lines where x − ct is constant the term f (x − ct) is constant. • These lines are called characteristics. f (x) = { 2 if − 1 < x < 1 0 otherwise 1 2f (x + ct) = { 1 if − 1 − ct < x < 1 − ct 0 otherwise 1 2f (x − ct) = { 1 if − 1 + ct < x < 1 + ct 0 otherwise The characteristics where x + ct = ±1 and x − ct = ±1 help determine the solution. Region 1: {(x, t) | x + ct < −1}; Region 2: {(x, t) | −1 < x − ct and x + ct < 1} Region 3: {(x, t) | 1 < x − ct}; Region 4: {(x, t) | 1 < x + ct and −1 < x − ct < 1} Region 5: {(x, t) | 1 < x + ct and x − ct < −1}; Region 6: {(x, t) | −1 < x + ct < 1 and 126 x − ct < −1}. Hence, u(x, t) = 1 2 f (x + ct) + 1 2f (x − ct) =  |||||||| |||||||| 0 if x + ct < −1 2 if − 1 < x − ct < 1 and − 1 < x + ct < 1 0 if 1 < x − ct 1 if 1 < x + ct and − 1 < x − ct < 1 0 if 1 < x + ct and x − ct < −1 1 if − 1 < x + ct < 1 and x − ct < −1 6.4 Interpretation of the Fourier series solution in terms of d’Alembert’s solution Recall the double-angle trigonometric identities: sin(A ± B) = sin A cos B ± cos A sin B, (6.19) cos(A ± B) = cos A cos B ∓ sin A sin B, (6.20) which we will use to interpret the solution (6.2) in terms of D’Alembert’s solution for an inﬁnite domain. Using (6.19) and (6.20) we obtain cos ( nπct L ) sin (nπx L ) = 1 2 { sin ( nπ L (x + ct)) + sin(nπ L (x − ct))} , (6.21) sin ( nπx L ) sin (nπct L ) = 1 2 { cos (nπ L (x − ct) ) − cos (nπ L (x + ct))} . (6.22) Now, ∞∑ n=1 An cos (nπct L ) sin(nπx L ) = 1 2 ∞∑ n=1 An[ sin(nπ L (x + ct) ) + sin(nπ L (x − ct) )] = 1 2 [ f0(x + ct) + f0(x − ct) ], (6.23) 127 where fo is the odd periodic extension of f . Similarly, ∞∑ n=1 Bn sin (nπct L ) sin(nπx L ) = 1 2 ∞∑ n=1 Bn[cos (nπ L (x − ct) ) − cos (nπ L (x + ct))] = 1 2 [G(x − ct) − G(x + ct)] , (6.24) where G(x) := ∞∑ n=1 Bn cos (nπx L ), Bn = 2 nπc ∫ L 0 g(x) sin (nπx L ) dx. (6.25) We can write G(x) = 2 πc ∞∑ n=1 1 n [∫ L 0 g(ξ) sin ( nπξ L ) dξ] cos (nπx L ). Therefore, G ′(x) = −1 c ∞∑ n=1 2 L [∫ L 0 g(ξ) sin ( nπξ L ) dξ] sin(nπx L ) = −1 c go(x). Consequently, G(x) = −1 c ∫ x 0 g0(s) ds + D, (6.26) where D is an integration constant. Returning to the series representation, we have ∞∑ n=1 Bn sin ( nπct L ) sin (nπx L ) = 1 2 [G(x − ct) − G(x + ct) ], (6.27) = 1 2c { [ − ∫ x−ct 0 go(s) ds + D] − [− ∫ x+ct 0 go(s) ds + D] } , (6.28) = 1 2c ∫ x+ct x−ct go(s) ds. (6.29) Therefore, combining (6.23) and (6.29) we obtain the following expression for the solution of the wave equation for a ﬁnite domain in the form of D’Alembert’s solution: u(x, t) = 1 2 [ fo(x + ct) + fo(x − ct) ] + 1 2c ∫ x+ct x−ct go(s) ds, (6.30) where fo and go are the odd periodic extensions of f and g on [0, L], i.e., fo(x) = { f (x), 0 < x < L, −f (−x), −L < x < 0, with fo(x + 2L) = fo(x), (6.31) go(x) = { g(x), 0 < x < L, −g(−x), −L < x < 0, with go(x + 2L) = go(x). (6.32) 128 Note In contrast to the Heat Equation, whose solutions typically decay exponentially over time, the solutions to the Wave Equation persist, as their time dependence involves sin ( nπct L ) and cos ( nπct L ), which do not decay over time. 129 Chapter 7 The Laplace Equation In this chapter, we begin our exploration of Laplaces equation, which describes the steady-state behavior of a ﬁeld dependent on two or more independent variables, typically spatial. We illustrate how the inhomogeneous Dirichlet boundary value problem for the Laplacian on a rectangular domain can be decomposed into a series of four boundary value problems. Each of these problems features a single boundary segment with inhomogeneous boundary conditions, while the remaining boundaries satisfy homogeneous conditions. This approach allows us to solve each problem individually using the method of separation of variables. Laplace’s Equation arises as a steady state problem for the Heat or Wave Equations that do not vary with time so that ∂u ∂t = 0 = ∂2u ∂t2 . in 2D, the equation reads; ∆u = ∂2u ∂x2 + ∂2u ∂y2 = 0. This equation is Laplace’s equation in two dimensions, one of the essential equations in applied mathematics (and the most important for time-independent problems). Note that in general, the Laplacian for a function u (x1, · · · , xn) in R n → R is deﬁned to be the sum of the second partial derivatives: ∆u = n∑ j=1 ∂2u ∂x2 j The inhomogeneous case, i.e. ∆u = f the equation is called Poisson’s equation. Innumerable physical systems are described by Laplace’s equation or Poisson’s equation, beyond steady states for the heat equation: inviscid ﬂuid ﬂow (e.g. ﬂow past an airfoil), stress in a solid, electric ﬁelds, wavefunctions (time independence) in quantum mechanics, and more. 7.1 Rectangular domain We can solve Laplace’s equation in a bounded domain by the same techniques used for the heat and wave equation. In this section, we will solve Laplace’s equation on a rectangle in R 2. First, we consider the case of Dirichlet boundary conditions. That is, we consider the following boundary value problem. Let Ω = {(x, y) ∈ R 2 : 0 < x < a, 0 < y < b}. 130 Dirichlet BCs We want to look for a solution of the following, uxx + uyy = 0, 0 < x < a, 0 < y < b, BCs: u(0, y) = g1(y), u(a, y) = g2(y), 0 < y < b u(x, 0) = f1(x), u(x, b) = f2(x), 0 < x < a. (7.1) x y a b uxx + uyy = 0u(0, y) = g1(y) u(a, y) = g2(y) u(x, 0) = f1(x) u(x, b) = f2(x) Figure 7.1: Laplace equation with boundary conditions on a rectangular domain. The functions f1(x), f2(x), g1(y), and g2(y) are given functions of x and y, respectively. The partial diﬀerential equation is linear and homogeneous, but the boundary conditions, although linear, are not homogeneous. We cannot apply the method of separation of variables to this problem in its present form because, when we separate variables, the boundary value problem (which determines the separation constant) must have homogeneous boundary conditions. In this case, all the boundary conditions are nonhomogeneous. Method 7.1.1 We can address this diﬃculty by recognizing that the original problem is nonhomogeneous due to the four nonhomogeneous boundary conditions. The principle of superposition can sometimes be used for nonhomogeneous problems. To solve this, we decompose the problem into four subproblems, each having one nonhomogeneous condition. We deﬁne u(x, y) = u1(x, y) + u2(x, y) + u3(x, y) + u4(x, y), where each ui(x, y) satisﬁes Laplace’s equation with one nonhomogeneous boundary condition and the related three homogeneous boundary conditions, as illustrated in Figure 7.2. Instead of directly solving for u, we will show how to solve for u1, u2, u3, and u4. Method 7.1.2: Solving for u1 Consider uxx + uyy = 0, 0 < x < a, 0 < y < b, BCs: u(0, y) = g1(y), u(a, y) = 0, 0 < y < b u(x, 0) = 0, u(x, b) = 0, 0 < x < a. (7.2) 131 x y a b uxx + uyy = 0u(0, y) = g1(y) u(a, y) = 0 u(x, 0) = 0 u(x, b) = 0 x y a b uxx + uyy = 0u(0, y) = 0 u(a, y) = g2(y) u(x, 0) = 0 u(x, b) = 0 x y a b uxx + uyy = 0u(0, y) = 0 u(a, y) = 0 u(x, 0) = f1(x) u(x, b) = 0 x y a b uxx + uyy = 0u(0, y) = 0 u(a, y) = 0 u(x, 0) = 0 u(x, b) = f2(x) Figure 7.2: Decomposition of Laplace’s equation with four subproblems, each having one nonhomo- geneous boundary condition. We use separation of variables. We look for a solution of the form u(x, y) = X(x)Y (y). Plugging this into our equation, we get X ′′Y + XY ′′ = 0. Now dividing by XY , we arrive at X ′′ X + Y ′′ Y = 0 which implies Y ′′ Y = −X ′′ X = −λ for some constant λ. By our boundary conditions, we want Y (0) = 0 = Y (b). Therefore, we begin by solving the eigenvalue problem, { Y ′′ = −λY 0 < y < b Y (0) = 0 = Y (b) As we know, the solutions of this eigenvalue problem are given by 132 Yn(y) = sin (nπy b ) , λn = (nπ b )2 We now turn to solving X ′′ = (nπ b )2 X with the boundary condition X(a) = 0. The solutions of this ODE are given by Xn(x) = An cosh (nπx b ) + Bn sinh ( nπx b ) . Now the boundary condition X(a) = 0 implies An cosh (nπa b ) + Bn sinh ( nπa b ) = 0 Therefore, un(x, y) = Xn(x)Yn(y) = [An cosh (nπx b ) + Bn sinh (nπx b )] sin ( nπy b ) where An, Bn satisfy the condition An cosh (nπa b ) + Bn sinh ( nπa b ) = 0 is a solution of Laplace’s equation on Ω which satisﬁes the boundary conditions u(x, 0) = 0, u(x, b) = 0, and u(a, y) = 0. As we know, Laplace’s equation is linear. Therefore, we can take any combination of solutions {un} and get a solution of Laplace’s equation which satisﬁes these three boundary conditions. Therefore, we look for a solution of the form u(x, y) = ∞∑ n=1 un(x, y) = ∞∑ n=1 [ An cosh ( nπx b ) + Bn sinh (nπx b )] sin ( nπy b ) where An, Bn satisfy An cosh (nπa b ) + Bn sinh (nπa b ) = 0. (7.3) To solve our boundary-value problem (7.2), it remains to ﬁnd coeﬃcients An, Bn which not only satisfy (7.3), but also satisfy the condition u(0, y) = g1(y). That is, we need u(0, y) = ∞∑ n=1 An sin (nπy b ) = g1(y) That is, we want to be able to express g1 in terms of its Fourier sine series on the interval [0, b]. We know that the coeﬃcients An are given by An = 2 b ∫ b 0 g1(y) sin ( nπy b ) dy. (7.4) Substituting this value of An into (7.3), we deduce that Bn = −2 b coth (nπa b ) ∫ b 0 g1(y) sin (nπy b ) dy. (7.5) 133 Therefore, to summarize, we have found a solution of (7.2) given by u1(x, y) = ∞∑ n=1 un(x, y) = ∞∑ n=1 [An cosh (nπ b x) + Bn sinh (nπ b x)] sin ( nπ b y) (7.6) where An and Bn are given by 7.4 and (7.5), respectively. Similarly, we ﬁnd functions u2, u3 and u4 which vanish on three of the sides but satisfy the fourth boundary condition. Method 7.1.3: Solving for u2 Consider uxx + uyy = 0, 0 < x < a, 0 < y < b, BCs: u(x, 0) = 0, u(x, b) = 0, 0 < x < a, u(0, y) = 0, u(a, y) = f (y), 0 < y < b. (7.7) We use separation of variables. We look for a solution of the form u(x, y) = X(x)Y (y). Plugging this into our equation, we get X ′′(x)Y (y) + X(x)Y ′′(y) = 0. Now dividing by X(x)Y (y) (assuming nonzero factors), we arrive at X ′′(x) X(x) + Y ′′(y) Y (y) = 0, which implies Y ′′(y) Y (y) = −X ′′(x) X(x) = −λ for some constant λ. By our boundary conditions, we require Y (0) = 0 = Y (b). Therefore, we begin by solving the eigenvalue problem { Y ′′(y) = −λY (y), 0 < y < b, Y (0) = 0, Y (b) = 0. As we know, nontrivial solutions exist only if λ = λn = (nπ b )2 , n = 1, 2, . . . , with corresponding eigenfunctions Yn(y) = sin ( nπy b ) . We now turn to solving X ′′(x) − (nπ b )2 X(x) = 0, 134 with the boundary condition X(0) = 0 (since u(0, y) = 0). The solutions of this ODE are given by Xn(x) = An cosh (nπx b ) + Bn sinh ( nπx b ) . The condition X(0) = 0 forces Xn(0) = An = 0, so that Xn(x) = Bn sinh (nπx b ) . Therefore, a separated solution is given by un(x, y) = sinh ( nπx b ) sin ( nπy b ) . Since Laplace’s equation is linear, any linear combination of these solutions is also a solution. Hence, we look for a solution of the form u(x, y) = ∞∑ n=1 cn un(x, y) = ∞∑ n=1 cn sinh ( nπx b ) sin (nπy b ) . At x = a, we have u(a, y) = ∞∑ n=1 cn sinh ( nπa b ) sin ( nπy b ) = f (y). Thus, the coeﬃcients cn are determined by expanding f (y) in a Fourier sine series on the interval 0 < y < b: cn = 2 b sinh ( nπa b ) ∫ b 0 f (y) sin (nπy b ) dy, n = 1, 2, . . . . This completes the solution of (7.7). Neumann BCs Let us consider an example with Neumann boundary conditions. Example 7.1.1 Consider uxx + uyy = 0, 0 < x < a, 0 < y < b, BCs: uy(x, 0) = 0, uy(x, b) = 0, 0 < x < a, ux(0, y) = 0, ux(a, y) = f (y), 0 < y < b. (7.8) Assume a solution of the form u(x, y) = X(x)Y (y). Then, uxx = X ′′(x)Y (y) and uyy = X(x)Y ′′(y). 135 Substituting into Laplace’s equation, we obtain X ′′(x)Y (y) + X(x)Y ′′(y) = 0. Dividing by X(x)Y (y) (assuming both factors are nonzero) yields X ′′(x) X(x) + Y ′′(y) Y (y) = 0. Since the left-hand side is a sum of a function of x and a function of y, we set X ′′(x) X(x) = λ, Y ′′(y) Y (y) = −λ, for some constant λ. The Y -equation becomes Y ′′(y) + λY (y) = 0, with the boundary conditions (implied by uy(x, 0) = 0 and uy(x, b) = 0): Y ′(0) = 0, Y ′(b) = 0. Nontrivial solutions exist only if λ = λn = ( nπ b )2 , n = 0, 1, 2, . . . , with corresponding eigenfunctions Yn(y) = cos (nπy b ) . With λ = ( nπ b )2, the X-equation is X ′′(x) − (nπ b )2 X(x) = 0, with the boundary condition (from ux(0, y) = 0): X ′(0) = 0. The general solution of the ODE is Xn(x) = An cosh ( nπx b ) + Bn sinh (nπx b ) . The condition X ′(0) = 0 forces X ′ n(0) = An nπ b sinh(0) + Bn nπ b cosh(0) = Bn nπ b = 0, so that Bn = 0. 136 Thus, Xn(x) = An cosh ( nπx b ) . Therefore, a separated solution is given by un(x, y) = Xn(x)Yn(y) = cosh ( nπx b ) cos (nπy b ) . Notice that for n = 0 the corresponding solution is constant in y. Since Laplace’s equation is linear, the general solution is u(x, y) = c0 2 + ∞∑ n=1 cn cosh (nπx b ) cos (nπy b ) . At x = a, diﬀerentiating with respect to x yields ux(x, y) = ∞∑ n=1 cn nπ b sinh ( nπx b ) cos ( nπy b ) , so that ux(a, y) = ∞∑ n=1 cn nπ b sinh (nπa b ) cos ( nπy b ) = f (y). Therefore, we must choose the constants c1, c2, . . . such that f (y) = ∞∑ n=1 nπ b cn sinh ( nπa b ) cos (nπy b ) , 0 < y < b. (7.9) Now, we know that we can expand f (y) in the cosine series f (y) = 1 b ∫ b 0 f (y) dy + 2 b ∞∑ n=1 [∫ b 0 f (y) cos ( nπy b ) dy] cos (nπy b ) (7.10) on the interval 0 ≤ y ≤ b. However, we cannot equate coeﬃcients in (7.9) and (7.10) since the series (7.9) has no constant term. Therefore, the condition ∫ b 0 f (y) dy = 0 is necessary for this Neumann problem to have a solution. In that case, cn = 2 nπ sinh ( nπa b ) ∫ b 0 f (y) cos (nπy b ) dy, n ≥ 1. Note that the coeﬃcient c0 remains arbitrary. In fact, the solution u(x, y) is determined only up to an additive constant, which is a characteristic feature of Neumann problems. 137 Mixed BCs We now consider an example where we have a mixed boundary condition on one side. Example 7.1.2 Consider uxx + uyy = 0, (x, y) ∈ Ω, Ω = {(x, y) : 0 < x < a, 0 < y < b}, BCs: u(0, y) = 0, u(a, y) = 0, 0 < y < b, u(x, 0) − uy(x, 0) = 0, u(x, b) = f (x), 0 < x < a. (7.11) We use separation of variables by seeking a solution of the form u(x, y) = X(x)Y (y). Substituting into Laplace’s equation, we have X ′′(x)Y (y) + X(x)Y ′′(y) = 0. Dividing by X(x)Y (y) (assuming neither is zero) yields X ′′(x) X(x) + Y ′′(y) Y (y) = 0, so that X ′′(x) X(x) = −Y ′′(y) Y (y) = −λ, for some separation constant λ. We ﬁrst solve the X-problem X ′′(x) + λX(x) = 0, 0 < x < a, X(0) = 0, X(a) = 0. It is well known that the eigenfunctions and eigenvalues are Xn(x) = sin (nπx a ), λn = ( nπ a )2, n = 1, 2, . . . . For each n with λ = λn, the Y -equation becomes Y ′′(y) = (nπ a )2Y (y). Its general solution is Yn(y) = An cosh(nπy a ) + Bn sinh ( nπy a ). The boundary condition at y = 0 is u(x, 0) − uy(x, 0) = 0. Since u(x, y) = Xn(x)Yn(y) and Xn(x) ̸≡ 0, we require Yn(0) − Y ′ n(0) = 0. 138 Noting that Yn(0) = An and Y ′ n(0) = Bn nπ a , this condition yields An − Bn nπ a = 0, or An = nπ a Bn. Thus, we may write Yn(y) = Bn [ nπ a cosh ( nπy a ) + sinh(nπy a )] . The separated solution corresponding to the n-th term is therefore un(x, y) = Xn(x)Yn(y) = Bn sin(nπx a ) [nπ a cosh ( nπy a ) + sinh( nπy a )] . Since Laplace’s equation is linear, we may superimpose these solutions and write u(x, y) = ∞∑ n=1 Bn sin ( nπx a ) [nπ a cosh(nπy a ) + sinh(nπy a )] . To satisfy the boundary condition u(x, b) = f (x), we substitute y = b into the series: ∞∑ n=1 Bn sin(nπx a ) [nπ a cosh(nπb a ) + sinh(nπb a )] = f (x). Express f (x) in its Fourier sine series on (0, a): f (x) ∼ ∞∑ n=1 An sin ( nπx a ), with coeﬃcients An = 2 a ∫ a 0 f (x) sin (nπx a ) dx. Thus, equating the coeﬃcients of like sine functions, we require Bn [nπ a cosh ( nπb a ) + sinh(nπb a )] = An. That is, Bn = 2 a [nπ a cosh(nπb a ) + sinh( nπb a )]−1 ∫ a 0 f (x) sin (nπx a )dx. In summary, the solution of the boundary value problem is u(x, y) = ∞∑ n=1 Bn sin ( nπx a ) [nπ a cosh(nπy a ) + sinh(nπy a )] , where Bn = 2 a [nπ a cosh(nπb a ) + sinh( nπb a )]−1 ∫ a 0 f (x) sin (nπx a )dx. 139 7.2 Laplace’s Equation on a disk In this section, we consider Laplace’s Equation on a disk in R 2. That is, let Ω = {(x, y) ∈ R2 : x2 + y2 < a 2}. Consider { uxx + uyy = 0 (x, y) ∈ Ω u = h(θ) (x, y) ∈ ∂Ω. To solve, we write this equation in polar coordinates as follows. To transform our equation in to polar coordinates, we will write the operators ∂x and ∂y in polar coordinates. We will use the fact that x2 + y2 = r2, y x = tan θ. Consider a function u such that u = u(r, θ), where r = r(x, y) and θ = θ(x, y). That is, u = u(r(x, y), θ(x, y)). Then ∂ ∂x u(r(x, y), θ(x, y)) = urrx + uθθx = ur x (x2 + y2) 1/2 − uθ y x2 sec2 θ = ur cos θ − sin θ r uθ. Therefore, the operator ∂ ∂x can be written in polar coordinates as ∂ ∂x = cos θ ∂ ∂r − sin θ r ∂ ∂θ . Similarly, the operator ∂ ∂y can be written in polar coordinates as ∂ ∂y = sin θ ∂ ∂r + cos θ r ∂ ∂θ . Now squaring these operators we have ∂2 ∂x2 = [ cos θ ∂ ∂r − sin θ r ∂ ∂θ ]2 = cos 2 θ ∂2 ∂r2 + 2 sin θ cos θ r2 ∂ ∂θ − 2 sin θ cos θ r ∂2 ∂r∂θ + sin 2 θ r ∂ ∂r + sin 2 θ r2 ∂2 ∂θ2 . Similarly, ∂2 ∂y2 = [ sin θ ∂ ∂r + cos θ r ∂ ∂θ ]2 = sin 2 θ ∂2 ∂r2 − 2sin θ cos θ r2 ∂ ∂θ + 2 sin θ cos θ r ∂2 ∂r∂θ + cos 2 θ r ∂ ∂r + cos 2 θ r2 ∂2 ∂θ2 . Combining the above terms, we can write the operator ∂2 x + ∂2 y in polar coordinates as follows, 140 ∂2 ∂x2 + ∂2 ∂y2 = ∂2 ∂r2 + 1 r ∂ ∂r + 1 r2 ∂2 ∂θ2 . Method 7.2.1: Solving Laplace’s equation in polar coordinates Let the Laplace equation in polar coordinates be urr + 1 r ur + 1 r2 uθθ = 0, 0 < r < a, 0 ≤ θ ≤ 2π, with the boundary condition u(a, θ) = h(θ), 0 ≤ θ ≤ 2π. There are no explicit boundary conditions in θ; however, because θ is an angle there are implied periodic boundary conditions u(r, 0) = u(r, 2π), uθ(r, 0) = uθ(r, 2π). We look for a solution by separation of variables, assuming u(r, θ) = R(r)Θ(θ). Substituting into the equation yields R′′(r)Θ(θ) + 1 r R′(r)Θ(θ) + 1 r2 R(r)Θ ′′(θ) = 0. Dividing by R(r)Θ(θ) (with R, Θ ̸≡ 0) gives R′′(r) R(r) + 1 r R′(r) R(r) + 1 r2 Θ ′′(θ) Θ(θ) = 0. Multiplying through by r2 leads to r2 R′′(r) R(r) + r R′(r) R(r) + Θ ′′(θ) Θ(θ) = 0. Since the ﬁrst two terms depend only on r and the last only on θ, we set Θ ′′(θ) Θ(θ) = − (r2 R′′(r) R(r) + r R′(r) R(r) ) = λ, where λ is a separation constant. Thus, the Θ-equation becomes Θ ′′(θ) − λ Θ(θ) = 0, with the periodic conditions Θ(0) = Θ(2π) and Θ ′(0) = Θ′(2π). These imply that Θn(θ) = An cos(nθ) + Bn sin(nθ), λn = −n2, n = 0, 1, 2, . . . . For each n, the corresponding R-equation is r2R′′ n(r) + rR′ n(r) − n2Rn(r) = 0. 141 Assuming a solution of the form R(r) = rα leads to the indicial equation α2 − n2 = 0, Thus, we have two distinct solutions α = ±n when n is positive, and one double root α = 0 when n is zero. Correspondingly, we get the general solution for the radial Euler’s equation Rn(r) = Cnrn + Dnr−n for n = 1, 2, 3, . . . , R0(r) = C0 + D0 ln r for n = 0 Since the radial function R must be bounded at the origin, we are forced to set all values of D’s vanish, and the general solution becomes Rn(r) = rn, n ≥ 0. Thus, the separated solutions are un(r, θ) = rn[An cos(nθ) + Bn sin(nθ) ]. By linearity, the general solution is given by u(r, θ) = ∞∑ n=0 rn[ An cos(nθ) + Bn sin(nθ)] . To satisfy the boundary condition u(a, θ) = h(θ), we require ∞∑ n=0 an[An cos(nθ) + Bn sin(nθ) ] = h(θ). Using the orthogonality of the trigonometric functions on [0, 2π], the Fourier coeﬃcients are determined by A0 = 1 2π ∫ 2π 0 h(ϕ) dϕ, An = 1 π an ∫ 2π 0 h(ϕ) cos(nϕ) dϕ, n = 1, 2, . . . Bn = 1 π an ∫ 2π 0 h(ϕ) sin(nϕ) dϕ, n = 1, 2, . . . Hence, the solution in series form is u(r, θ) = 1 2π ∫ 2π 0 h(ϕ) dϕ + ∞∑ n=1 rn [ 1 π an ∫ 2π 0 h(ϕ) cos(nϕ) dϕ cos(nθ) + 1 π an ∫ 2π 0 h(ϕ) sin(nϕ) dϕ sin(nθ) ] . This series can be rewritten as a single integral by noting that cos(n(θ − ϕ)) = cos(nθ) cos(nϕ) + sin(nθ) sin(nϕ). 142 Thus, u(r, θ) = 1 2π ∫ 2π 0 h(ϕ) { 1 + 2 ∞∑ n=1 ( r a )n cos (n(θ − ϕ) )} dϕ. Recognizing the sum as a geometric series, we have 1 + 2 ∞∑ n=1 ( r a )n cos ( n(θ − ϕ)) = a2 − r2 a2 − 2ar cos(θ − ϕ) + r2 . Therefore, the solution is given in closed form by u(r, θ) = 1 2π ∫ 2π 0 h(ϕ) a2 − r2 a2 − 2ar cos(θ − ϕ) + r2 dϕ. Example 7.2.1 Consider the interior Dirichlet problem urr + 1 r ur + 1 r2 uθθ = 0, 0 ≤ r < 2, u(2, θ) = f (θ) ≡ { 1, if 0 ≤ θ ≤ π cos 2 θ, if π ≤ θ ≤ 2π. Its solution is known to be u(r, θ) = a0 2 + ∞∑ n=1 ( r 2 )n [an cos(nθ) + bn sin(nθ)] , 0 ≤ r ≤ 2, 0 ≤ θ ≤ 2π. To satisfy the boundary condition u(2, θ) = f (θ), we calculate the coeﬃcients as usual a0 = 1 π ∫ π 0 dϕ + 1 π ∫ π 0 cos 2 ϕ dϕ = 3 2, an = 1 π ∫ π 0 cos(nϕ)dϕ + 1 π ∫ π 0 cos(nϕ) cos 2 ϕ dϕ = { 1 4, if n = 2, 0, if n ̸= 2 bn = 1 π ∫ π 0 sin(nϕ)dϕ + 1 π ∫ π 0 sin(nϕ) cos 2 ϕ dϕ = { −4 n(n2−4), if n is odd , 0, if n is even; Therefore, the required solution is u(r, θ) = 3 4 + r2 16 cos(2θ) − 4 π ∑ k≥0 1 (2k + 1) [(2k + 1)2 − 4] ( r 2 )2k+1 sin[(2k + 1)θ]. Example 7.2.2 Consider the interior Dirichlet problem urr + 1 r ur + 1 r2 uθθ = 0, 0 ≤ r < 3, u(3, θ) = 2 sin 4θ − 3 cos 7θ. 143 Its solution is known to be u(r, θ) = a0 2 + ∞∑ n=1 ( r 3 )n [an cos(nθ) + bn sin(nθ)] , 0 ≤ r ≤ 3, 0 ≤ θ ≤ 2π. To satisfy the boundary condition u(2, θ) = f (θ), we have the expansion u(3, θ) = f (θ) = a0 2 + ∞∑ n=1 [an cos(nθ) + bn sin(nθ)] , 0 ≤ r ≤ 3, 0 ≤ θ ≤ 2π where f is a combination of eigenfunctions. So we know that all coeﬃcients in the above expansion are zeroes except n = 4 and n = 7. Hence, b4 = 2 and a7 = −3. This yields u(r, θ) = 2 ( r 3 )4 sin(4θ) − 3 ( r 3 )7 sin(7θ). Method 7.2.2: Solving Laplace’s equation with Neumann BCs inside the circle Consider the interior Neumann problem for a circle of radius a : urr + 1 r ur + 1 r2 uθθ = 0, 0 ≤ r < a, ∂u ∂r \f \f \f \fr=a = g(θ), ∫ 2π 0 g(θ)dθ = 0 where g is a given function. The general solution of Laplace’s equation inside a circle of radius a is u(r, θ) = a0 2 + ∞∑ n=1 ( r a )n [an cos(nθ) + bn sin(nθ)] , 0 ≤ r < a, 0 ≤ θ ≤ 2π. Its derivative with respect to r becomes (assuming uniform convergence of the above series) ∂u ∂r = ∞∑ n=1 n r ( r a )n [an cos(nθ) + bn sin(nθ)] , 0 ≤ r < a, 0 ≤ θ ≤ 2π. Setting r equals to a yields ∂u ∂r \f \f \f \fr=a = g(θ) = ∞∑ n=1 n a [an cos(nθ) + bn sin(nθ)] , 0 ≤ θ ≤ 2π. The coeﬃcients of the Fourier series are obtained by: a0 = 1 π ∫ 2π 0 g(ϕ)dϕ = 0 an = a nπ ∫ 2π 0 g(ϕ) cos(nϕ)dϕ, n = 1, 2, . . . bn = a nπ ∫ 2π 0 g(ϕ) sin(nϕ)dϕ, n = 1, 2, . . . 144 Note that the coeﬃcient a0 must be zero because the corresponding Fourier series for ∂u ∂r \f \f \f \fr=a = ∞∑ n=1 n a [an cos(nθ) + bn sin(nθ)] , 0 ≤ θ ≤ 2π does not contain a free term. Therefore, a Neumann problem has a solution if and only if the integral over the boundary vanishes: ∫ 2π 0 g(ϕ)dϕ = 0. Then the general solution to a Neumann problem is not unique but up to an arbitrary additive constant: u(r, θ) = C + ∞∑ n=1 ( r a )n [an cos(nθ) + bn sin(nθ)] = C + ∞∑ n=1 ( r a )n [ a nπ ∫ 2π 0 g(ϕ) cos(nϕ) dϕ cos(nθ) + a nπ ∫ 2π 0 g(ϕ) sin(nϕ) dϕ sin(nθ) ] . 7.3 Semi-inﬁnite strip problems Example 7.3.1: Homogeneous Bcs Consider the Laplace equation: uxx + uyy = 0, 0 < x < a, 0 < y < ∞. (7.12) with the boundary conditions: u(0, y) = 0, u(a, y) = 0, (7.13) u(x, 0) = f (x), u(x, y) → 0 as y → ∞. (7.14) We use the method of separation of variables and assume a solution of the form: u(x, y) = X(x)Y (y). Substituting into (7.12), we obtain: X ′′(x)Y (y) + X(x)Y ′′(y) = 0. Dividing by X(x)Y (y) (assuming nonzero factors), we separate the variables: X ′′(x) X(x) = −Y ′′(y) Y (y) = λ. Therefore, X ′′(x) X(x) = λ, −Y ′′(y) Y (y) = λ. (7.15) 145 From (7.15), the equation for X(x) is: X ′′(x) − λX(x) = 0. Using the boundary conditions (7.13), we solve the eigenvalue problem: X(0) = 0, X(a) = 0. Nontrivial solutions exist only if λn = − ( nπ a )2 , n = 1, 2, 3, . . . with corresponding eigenfunctions: Xn(x) = sin ( nπx a ) . The equation for Y (y) from (7.15) is: Y ′′(y) − ( nπ a )2 Y (y) = 0. The general solution is: Y (y) = Ane− nπ a y + Bne nπ a y. Since u(x, y) → 0 as y → ∞, we must set Bn = 0, giving: Yn(y) = Ane− nπ a y. The general solution is given by: u(x, y) = ∞∑ n=1 cne− nπ a y sin (nπx a ) . Using the boundary condition at y = 0 from (7.14), we require: ∞∑ n=1 cn sin ( nπx a ) = f (x). This represents the Fourier sine series of f (x) on 0 < x < a, giving: cn = 2 a ∫ a 0 f (x) sin (nπx a ) dx. Thus, the ﬁnal solution is: u(x, y) = ∞∑ n=1 2 a [∫ a 0 f (x) sin (nπx a ) dx] e− nπ a y sin (nπx a ) . 146 Example 7.3.2: Nonhomogeneous BCs Consider the Laplace equation: uxx + uyy = 0, 0 < x < a, 0 < y < ∞. (7.16) with the boundary conditions: u(0, y) = A, u(a, y) = B, (7.17) u(x, 0) = f (x), u(x, y) → 0 as y → ∞. (7.18) We seek a function v(x) such that v′′(x) = 0 and it satisﬁes the inhomogeneous boundary conditions: v(0) = A, v(a) = B. (7.19) The general solution for v(x) is: v(x) = αx + β. (7.20) Using the boundary conditions: A = v(0) = β, (7.21) B = v(a) = αa + A, (7.22) we solve for α and β: α = B − A a , β = A. (7.23) Thus, v(x) = ( B − A a ) x + A. (7.24) Deﬁne w(x, y) such that: u(x, y) = v(x) + w(x, y). (7.25) Substituting into (7.16), we obtain: wxx + uyy = 0. (7.26) The boundary conditions for w(x, y) are: w(0, y) = 0, w(a, y) = 0, (7.27) w(x, 0) = f (x) − v(x). (7.28) Since w satisﬁes the same boundary value problem as in Example 7.3, we directly obtain the solution: w(x, y) = ∞∑ n=1 dne−( nπ a )y sin ( nπx a ) , (7.29) where the coeﬃcients dn are given by: dn = 2 a ∫ a 0 [f (x) − v(x)] sin ( nπx a ) dx. (7.30) Using (7.25), (7.24), and (7.29), we obtain: u(x, y) = (B − A)x a + A + ∞∑ n=1 dne −( nπ a )y sin ( nπx a ) . (7.31) 147 Remark 7.3.1: Inhomogeneous Laplace’s equation Just as with the heat equation, if there are more complicated inhomogeneous terms, e.g. 0 = uxx + uyy + f (x, y) then the eigenfunction method is required unless you are lucky and there is a “particular” solution you can subtract out to remove the inhomogeneous terms. When applying the eigenfunction method, one must pick a direction for the eigenfunctions, either u = ∑ n cn(x)ϕn(y) or u = ∑ n cn(y)ϕn(x). The correct choice is one where the boundary conditions are homogeneous (if both work, then it does not matter which you choose). The details are somewhat involved but straightforward in concept. Note More examples of problems related to the Laplace equation can be found in Lecture Notes 27 by Prof. Peirce [1]. 148 Chapter 8 Sturm-Liouville boundary value problems In this chapter, we consider a class of two-point boundary value problems. The so-called Sturm- Liouville Problems. They are a class of eigenvalue problems, which include many of the previous problems as special cases. We let Lϕ = − d dx [ p(x)dϕ dx ] + q(x)ϕ. Suppose we have some Sturm-Liouville problem with diﬀerential equation Lϕ = λrϕ for 0 < x < l. (8.1) Here, the functions p, p′, q, and r are continuous on [0, ℓ] with p(x) ≥ 0 and r(x) > 0, 0 ≤ x ≤ ℓ. We couple (8.1), with the following boundary conditions α1y(0) + α2y′(0) = 0, β1y(ℓ) + β2y′(ℓ) = 0. (8.2) We deﬁne the Sturm-Liouville eigenvalue problem (SL Problem) as: Ly = λr(x)y, α1y(0) + α2y′(0) = 0, β1y(ℓ) + β2y′(ℓ) = 0, p(x) > 0, r(x) > 0. (8.3) Remark 8.0.1 • If in (8.3) we choose p(x) = 1, q(x) = 0, r(x) = 1, α1 = 1, α2 = 0, β1 = 1 and β2 = 0, then we obtain the following problem y′′ + λy = 0 y(0) = 0 = y(ℓ) } =⇒ { λn = ( nπ ℓ )2 , n = 1, 2, . . . yn(x) = sin ( nπx ℓ ) . • If in (8.3) we choose p(x) = 1, q(x) = 0, r(x) = 1, α1 = 0, α2 = 1, β1 = 0 and β2 = 1, then we obtain the following problem 149 y′′ + λy = 0 y′(0) = 0 = y′(ℓ) } =⇒ { λn = ( nπ ℓ )2 , n = 0, 1, 2, . . . yn(x) = cos ( nπx ℓ ) . • Notice that these boundary conditions are speciﬁed at separate endpoints and are called separated boundary conditions. The periodic BC X(−ℓ) = X(ℓ) are not separated, so the following problem is not technically an SL Problem. y′′ + λy = 0, y(−ℓ) = y(ℓ), y′(−ℓ) = y′(ℓ). • If p > 0 and r > 0 on a ﬁnite interval [0, ℓ], then the SL problem is said to be regular. If either p(x) or r(x) is zero for some x, or if the domain is unbounded (e.g., [0, ∞)), then the problem is singular. • If P0, P1, P2, and R are continuous and P0 and R are positive on a closed interval [a, b], then the general equation P0(x)y′′ + P1(x)y′ + P2(x)y + λR(x)y = 0. can be rewritten in the SL form. Indeed, if we divide the previous equation by P0(x), we obtain y′′ + P1(x) P0(x)y′ + P2(x) P0(x)y + λ R(x) P0(x)y = 0. We multiply the equation by the integrating factor ˜p(x) = e ∫ P1(x) P0(x) dx, we get: ˜p(x)y′′ + ˜p(x)(x)P1(x) P0(x)y′ + ˜p(x) ( P2(x) P0(x) + λ R(x) P0(x) ) y = 0. The ﬁrst two terms give: d dx (˜p(x) dy dx ) . Thus, the equation becomes: d dx (˜p(x) dy dx ) + ˜p(x)P2(x) P0(x)y = −λ˜p(x) R(x) P0(x)y. Deﬁne the following functions: p(x) = −˜p(x) = −e ∫ P1(x) P0(x) dx, q(x) = ˜p(x)P2(x) P0(x), r(x) = −˜p(x) R(x) P0(x). Therefore, − d dx ( p(x) dy dx ) + q(x)y = λr(x)y. 150 Example 8.0.1 1. Consider the boundary value problem ϕ ′′ + x ϕ ′ + λϕ = 0, 0 < x < 1, (8.4) ϕ(0) = 0, ϕ(1) = 0. (8.5) To bring (8.4) into SL form, multiply by the integrating factor µ(x) = e ∫ x dx = ex2/2. Since P (x) = 1, Q(x) = x, and R(x) = 1, we have: e x2/2ϕ ′′ + e x2/2x ϕ ′ + λe x2/2ϕ = 0, That is − (ex2/2ϕ ′)′ = λe x2/2ϕ. Thus, the SL form is obtained with p(x) = ex2/2 and r(x) = ex2/2. 2. Consider the boundary value problem −y′′ + x4y′ = λy. To convert into the SL form, we rewrite the equation as: y′′ − x4y′ = −λy. The integrating factor is µ(x) = e− ∫ x4 dx = e −x5/5. Multiplying the original equation by µ(x), we obtain: −e−x5/5y′′ + e−x5/5x4y′ = λe −x5/5y, That is − (e−x5/5y′)′ = λe −x5/5y. Thus, the SL form is obtained with p(x) = e −x5/5 and r(x) = e−x5/5. 3. Consider the eigenvalue problem y′′ + 3y′ + (2 + λ)y = 0, y(0) = 0, y(1) = 0. (8.6) The integrating factor is µ(x) = e ∫ 3dx = e 3x. Multiplying equation (8.6) by µ(x) yields − ( −e 3xy′)′ + 2e 3xy + λe 3xy = 0. So that the eigenvalue problem can be written as − (−e3xy′)′ + 2e3xy = −λe 3xy, y(0) = 0, y(1) = 0. Thus the SL form is obtained with p(x) = −e3x, q(x) = 2e3x, and r(x) = −e 3x. 151 8.1 Properties of SL problems Eigenvalues: (a) The eigenvalues λ are all real. (b) There are inﬁnitely many eigenvalues λj satisfying λ1 < λ2 < · · · < λj → ∞ as j → ∞. (c) Provided α1 α2 < 0, β1 β2 > 0, and q(x) > 0, then λj > 0. Eigenfunctions: For each eigenvalue λj, there exists an eigenfunction ϕj(x) (unique up to a multiplicative constant) such that: (a) The eigenfunctions ϕj(x) are real and can be normalized to satisfy ∫ ℓ 0 r(x)ϕ 2 j (x) dx = 1. (b) Eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function r(x): ⟨ϕj, ϕk⟩ := ∫ ℓ 0 r(x)ϕj(x)ϕk(x) dx = 0, j ̸= k. (8.7) (c) Each eigenfunction ϕj(x) has exactly j − 1 zeros in the interval (0, ℓ). Expansion property: The eigenfunctions {ϕj(x)} form a complete set. Thus, if f (x) is piecewise smooth, then we have f (x) = ∞∑ n=1 cnϕn(x), (8.8) with cn = ∫ ℓ 0 r(x)f (x)ϕn(x) dx ∫ ℓ 0 r(x)ϕ 2 n(x) dx . (8.9) Lagrange’s Identity For suﬃciently diﬀerentiable functions u and v, Lagrange’s Identity states that ∫ ℓ 0 [vLu − uLv]dx = −p(x) (u′v − uv′)\f \f \fℓ 0. (8.10) 152 Proof. Write ∫ ℓ 0 vLu dx = ∫ ℓ 0 v[−(pu ′)′ + qu ]dx = −vpu ′\f \f \fℓ 0 + ∫ ℓ 0 pu ′v′ dx + ∫ ℓ 0 quv dx = −vpu ′\f \f \fℓ 0 + upv′\f \f \fℓ 0 + ∫ ℓ 0 u[ −(pv′)′ + qv]dx = −vpu ′\f \f \fℓ 0 + upv′\f \f \fℓ 0 + ∫ ℓ 0 uLv dx. Hence, ∫ ℓ 0 vLu dx − ∫ ℓ 0 uLv dx = −p(x)(u ′v − uv′) \f \f \fℓ 0. If u and v satisfy the SL boundary conditions α1u(0) + α2u′(0) = 0, β1u(ℓ) + β2u ′(ℓ) = 0, α1v(0) + α2v′(0) = 0, β1v(ℓ) + β2v′(ℓ) = 0, then the boundary terms cancel and ∫ ℓ 0 vLu dx = ∫ ℓ 0 uLv dx. (8.11) Example 8.1.1 1. We want to solve the eigenvalue problem (xy′) ′ + 2 xy = −λ 1 xy, x > 0 subject to the following boundary conditions y′(1) = 0, y′(2) = 0. Note that r(x) = 1 x . Expanding the derivative, we have xy′′ + y′ + 2 x y = −λ 1 x y. Multiply through by x to obtain the Cauchy-Euler type equation: x2y′′ + xy′ + (λ + 2)y = 0 The characteristic equation is r2 + λ + 2 = 0. Case 1: λ + 2 < 0. 153 We have two solutions r1,2 = ±√ −(λ + 2). The general solution is then, y(x) = c1x− √−(λ+2) + c2x√−(λ+2). The boundary conditions y′(1) = y′(2) = 0 implies that c1 = c2 = 0. Trivial solution. Case 2: λ + 2 = 0. We have a double solution r1 = r2 = 0. The general solution is then, y(x) = c1 + c2 ln x. The boundary conditions y′(1) = 0 implies that c2 = 0 and so y(x) = c1 is a nontrivial solution. Case 3: λ + 2 > 0. Thus, the general solution is y(x) = c1 cos( √λ + 2 ln x) + c2 sin( √ λ + 2 ln x) Next we apply the boundary conditions. y′(1) = 0 forces c2 = 0. This leaves y(x) = c1 cos( √λ + 2 ln x) The second condition, y′(2) = 0, yields sin( √λ + 2 ln 2) = 0 This will give nontrivial solutions when √λ + 2 ln 2 = nπ, n = 1, 2, 3 . . . In summary, the eigenfunctions for this eigenvalue problem are yn(x) = cos ( nπ ln 2 ln x) , 1 ≤ x ≤ 2 and all (including λ = −2) the eigenvalues are λn = ( nπ ln 2 )2 − 2 for n = 0, 1, 2, . . . We can check the orthogonality of eigenfunctions. We recall that ⟨yn, ym⟩ = ∫ 2 1 yn(x)ym(x)r(x)dx. Let y = π ln x ln 2 . Then, we have: ⟨yn, ym⟩ = ∫ 2 1 cos ( nπ ln 2 ln x) cos ( mπ ln 2 ln x) dx x = ln 2 π ∫ π 0 cos(ny) cos(my)dy = ln 2 2 δn,m 154 where δn,m = { 1, if n = m, 0, if n ̸= m is the so called Kronecker delta. Now let us consider expanding a function f (x) in terms of a “Fourier Series” of these new eigenfunctions in the following form f (x) = ∞∑ n=0 cnϕn(x) = ∞∑ n=0 cn cos ( nπ ln x ln 2 ) . In order to determine the coeﬃcients cn we project the function f (x) onto the basis functions ϕn(x) as follows: ∫ 2 1 f (x) cos ( mπ ln x ln 2 ) dx x = ∞∑ n=0 cn ∫ 2 1 cos ( mπ ln x ln 2 ) cos ( nπ ln x ln 2 ) dx x = cm ln 2 2 . Hence, cn = 2 ln 2 ∫ 2 1 f (x) cos ( nπ ln x ln 2 ) dx x . Note 8.1.1 • If the operator L and the boundary conditions satisfy ∫ ℓ 0 v Lu dx = ∫ ℓ 0 u Lv dx, then L is said to be selfadjoint. • With the notation (f, g) = ∫ ℓ 0 f (x)g(x) dx, the selfadjoint property can be written as (v, Lu) = (u, Lv). 8.2 Application: Solving the heat equation with Robin bound- ary conditions We consider the function u(x, t), which models the temperature distribution in a heat-conductive rod of length L that is perfectly insulated along its sides. The left end of the rod is also perfectly insulated, meaning no heat escapes or enters at x = 0. Meanwhile, the right end at x = L loses thermal energy at a rate proportional to its temperature at that point. The initial temperature distribution along the rod is given by the function f (x). 155 Example 8.2.1 The problem is stated as follows: ut = α2uxx, 0 < x < L, t > 0 ux(0, t) = 0, ux(L, t) + κu(L, t) = 0, u(x, 0) = f (x), (8.12) where κ > 0. We use the method of separation of variables to solve the problem (8.12). We ﬁrst seek separated solutions of the form u(x, t) = X(x)T (t) satisfying all of the homogeneous linear requirements, i.e. the ﬁrst three conditions. Substituting the separated solution into the PDE yields XT ′ = c2X ′′T ⇒ X ′′ X = T ′ c2T = λ( constant ) since the two sides of the latter equation are functions of distinct independent variables. This gives us the pair of separated ODEs X ′′ − λX = 0, T ′ − λc 2T = 0 From the ﬁrst boundary condition we obtain X ′(0)T (t) = 0 ⇒ X ′(0) = 0 since we do not want T ≡ 0. The second boundary condition requires that X ′(L)T (t) = −κX(L)T (t) ⇒ X ′(L) = −κX(L) We have thus obtained an ODE boundary value problem in X that requires a case by case analysis of the possible values of k for which there are nontrivial (nonzero) solutions. Case 1: λ = µ2 > 0. In this situation the ODE for X becomes X ′′ −µ2X = 0 with characteristic equation r2 − µ2 = 0, whose roots are r = ±µ. The solutions are then given by X = c1e µx + c2e −µx The boundary conditions require that µ ( c1e µ·0 − c2e−µ·0) = 0 ⇒ c1 − c2 = 0 µ ( c1e µL − c2e −µL) = −κ ( c1eµL + c2e −µL) ⇒ c1(κ + µ)e µL + c2(κ − µ)e−µL = 0 or in matrix form ( 1 −1 (κ + µ)eµL (κ − µ)e −µL ) ( c1 c2 ) = ( 0 0 ) . The determinant of the coeﬃcient matrix is (κ − µ)e −µL + (κ + µ)eµL = κ ( eµL + e−µL) + µ ( eµL − e −µL) > 0, which means that the only possibility is that c1 = c2 = 0, or in other words X ≡ 0. 156 Case 2: λ = 0. Now the ODE in X simpliﬁes to X ′′ = 0 which means that X = ax + b. The ﬁrst boundary condition immediately implies a = 0 and the second then becomes 0 = −κb ⇒ b = 0 which once again tells us that X ≡ 0. So we move on. Case 3: λ = −µ2 < 0. Things ﬁnally get interesting. The ODE becomes X ′′ + µ2X = 0 whose characteristic equation has roots ±iµ, so that X(x) = c1 cos(µx) + c2 sin(µx). From the ﬁrst boundary condition we ﬁnd −c1 sin 0 + c2 cos 0 = 0 ⇒ c2 = 0 The second boundary condition is then −µc1 sin(µL) = −κc1 cos(µL) ⇒ tan µL = κ µ since we do not want c1 = 0 at this point. This equation has an increasing sequence of positive solutions (We have proved it using a graphic), which we label 0 < µ1 < µ2 < µ3 < · · · Finally, we obtain the nontrivial solutions Xn = cos(µnx), n ∈ N Since −µ2 = λ, for each n ∈ N the ODE for T becomes T ′ + (cµn) 2 T = 0 ⇒ T ′ = − (cµn) 2 T ⇒ T = Tn = cne −(cµn) 2t. We have ﬁnally obtained our separated solutions: un(x, t) = cne−(cµn) 2t cos(µnx), n ∈ N, where µn is the nth positive solution to tan µL = κ/µ. Hence, u(x, t) = ∞∑ n=1 un(x, t) = ∞∑ n=1 cne −(cµn) 2t cos(µnx). Using the initial condition, we have f (x) = u(x, 0) = ∞∑ n=1 un(x, t) = ∞∑ n=1 cne −(cµn) 2·0 cos(µnx) = ∞∑ n=1 cn cos(µnx). (8.13) Now, we want to show that the functions cos µnx are pairwise orthogonal on the interval [0, L]. Assume that for diﬀerent eigenvalues µ2 n and µ2 m the corresponding eigenfunctions are given by Xn(x) = cos(µnx) and Xm(x) = cos(µmx), with µn ̸= µm. 157 Our goal is to show that these eigenfunctions are orthogonal: ∫ L 0 Xn(x)Xm(x) dx = 0 for n ̸= m. We know that Xn(x) satisﬁes X ′′ n(x) + µ2 nXn(x) = 0. If we multiply the latter identity by Xm(x) and integrate over [0, L] we obtain: ∫ L 0 Xm(x)X ′′ n(x) dx + µ2 n ∫ L 0 Xm(x)Xn(x) dx = 0. (8.14) Let the ﬁrst term in (8.14) be: I = ∫ L 0 Xm(x)X ′′ n(x) dx. An integration by parts gives: I = [Xm(x)X ′ n(x)]L 0 − ∫ L 0 X ′ m(x)X ′ n(x) dx. Using the boundary conditions, we get I = −κXm(L)Xn(L) − ∫ L 0 X ′ m(x)X ′ n(x) dx. Returning to (8.14) we now have: −κXm(L)Xn(L) − ∫ L 0 X ′ m(x)X ′ n(x) dx + µ2 n ∫ L 0 Xm(x)Xn(x) dx = 0. A second integration by parts along with the equation solved by Xm lead us to (µ2 n − µ2 m) ∫ L 0 Xn(x)Xm(x) dx = 0. Since µ2 n ̸= µ2 m for n ̸= m, we must have ∫ L 0 Xn(x)Xm(x) dx = 0. Finally, we multiply equation (8.13) by cos(µmx) and integrate over [0, L]. Thanks to the orthogonality relation, we obtain cn = ∫ L 0 f (x) cos(µnx)dx ∫ L 0 cos2(µnx)dx . 158 Bibliography [1] Lectures notes of Prof. Anthony Peirce, https://personal.math.ubc.ca/~peirce/math257_ 316_2018_NA.htm. [2] Abada Nadjet. Equations diﬀ´erentielles du second ordre. Cours 4 `eme ann´ee, 2019-2020. [3] Boyce & DiPrima. Elementary Diﬀerential Equations and Boundary Value Problems. 159","libVersion":"0.2.1","langs":""}