{"path":".obsidian/plugins/text-extractor/cache/8f963789b80dc39aff87a1b29f18204f.json","text":"CPSC 340: Machine Learning and Data Mining MLE and MAP Last Time: Maximum Likelihood Estimation (MLE) â€¢ Maximum likelihood estimation (MLE) for fitting probabilistic models. â€“ We have a dataset D. â€“ We want to pick parameters â€˜wâ€™. â€“ We define the likelihood as a probability mass/density function p(D | w). â€“ We choose the model à·ð‘¤ that maximizes the likelihood: â€¢ Appealing â€œconsistencyâ€ properties as n goes to infinity (take STAT 4XX). â€“ â€œThis is a reasonable thing to do for large data setsâ€. â€¢ Gives naÃ¯ve Bayes â€œcountingâ€ estimates we used. Least Squares is Gaussian MLE â€¢ It turns out that most objectives have an MLE interpretation: â€“ For example, consider minimizing the squared error: â€“ This gives MLE of a linear model with IID noise from a normal distribution: â€¢ â€œGaussianâ€ is another name for the â€œnormalâ€ distribution. â€“ Remember that least squares solution is called the â€œnormal equationsâ€. Least Squares is Gaussian MLE â€¢ It turns out that most objectives have an MLE interpretation: â€“ For example, consider minimizing the squared error: Minimizing the Negative Log-Likelihood (NLL) â€¢ To compute maximize likelihood estimate (MLE), usually we equivalently minimize the negative â€œlog-likelihoodâ€ (NLL): â€¢ â€œLog-likelihoodâ€ is short for â€œlogarithm of the likelihoodâ€. â€¢ Why are these equivalent? â€“ Logarithm is strictly monotonic: if Î± > Î², then log(Î±) > log(Î²). â€¢ So location of maximum doesnâ€™t change if we take logarithm. â€“ Changing sign flips max to min. â€¢ See Max and Argmax notes if this seems strange. Minimizing the Negative Log-Likelihood (NLL) â€¢ We use log-likelihood because it turns multiplication into addition: â€¢ More generally: Least Squares is Gaussian MLE (Gory Details) â€¢ Letâ€™s assume that yi = wTxi + Îµi, with Îµi following standard normal: â€¢ This leads to a Gaussian likelihood for example â€˜iâ€™ of the form: â€¢ Finding MLE (minimizing NLL) is least squares: Digression: â€œGenerativeâ€ vs. â€œDiscriminativeâ€ â€¢ Notice, that we maximized conditional p(y | X, w), not the likelihood p(y, X | w). â€“ We did MLE â€œconditionedâ€ on the features â€˜Xâ€™ being fixed (no â€œlikelihood of Xâ€). â€“ This is called a â€œdiscriminativeâ€ supervised learning model. â€“ A â€œgenerativeâ€ model (like naÃ¯ve Bayes) would optimize p(y, X | w). â€¢ Discriminative probabilistic models: â€“ Least squares, robust regression, logistic regression. â€“ Can use complicated features because you donâ€™t model â€˜Xâ€™. â€¢ Example of generative probabilistic models: â€“ NaÃ¯ve Bayes, linear discriminant analysis (makes Gaussian assumption). â€“ Often need strong assumption because they model â€˜Xâ€™. â€¢ â€œFolkâ€ belief: generative models are often better with small â€˜nâ€™. Loss Functions and Maximum Likelihood Estimation â€¢ So least squares is MLE under Gaussian likelihood. â€¢ With a Laplace likelihood you would get absolute error. â€¢ Other likelihoods lead to different errors (â€œsigmoidâ€ -> logistic loss). â€œHeavyâ€ Tails vs. â€œLightâ€ Tails â€¢ We know that L1-norm is more robust than L2-norm. â€“ What does this mean in terms of probabilities? â€“ Gaussian has â€œlight tailsâ€: assumes everything is close to mean. â€“ Laplace has â€œheavy tailsâ€: assumes some data is far from mean. â€“ Student â€˜tâ€™ is even more heavy-tailed/robust, but NLL is non-convex. http://austinrochford.com/posts/2013-09-02-prior-distributions-for-bayesian-regression-using-pymc.html â€¢ Recall we got probabilities from binary linear models with sigmoid: 1. The linear model wTxi gives us a number in oi in (-âˆž, âˆž). 2. We mapped oi=wTxi to a probability with the sigmoid function. â€¢ We can show that MLE with this model gives logistic loss. Sigmoid: transforming wTxi to a ProbabilitySigmoid: transforming oi to a Probability â€¢ Weâ€™ll define p(yi = +1 | oi) = h(oi), where â€˜hâ€™ is the sigmoid function. â€¢ With yi in {-1,+1}, we can write both cases as p(yi | oi) = h(yioi). â€¢ So we convert oi=wTxi into â€œprobability of yiâ€ using: â€¢ MLE with this likelihood is equivalent to minimizing logistic loss. MLE Interpretation of Logistic Regression â€¢ For IID regression problems the conditional NLL can be written: â€¢ Logistic regression assumes sigmoid(wTxi) conditional likelihood: â€¢ Plugging in the sigmoid likelihood, the NLL is the logistic loss: MLE Interpretation of Logistic Regression â€¢ We just derived the logistic loss from the perspective of MLE. â€“ Instead of â€œsmooth convex approximation of 0-1 lossâ€, we now have that logistic regression is doing MLE in a probabilistic model. â€“ The training and prediction would be the same as before. â€¢ We still minimize the logistic loss in terms of â€˜wâ€™. â€“ But MLE justifies sigmoid for â€œprobability that e-mail is importantâ€: â€“ Similarly, NLL under the softmax likelihood is the softmax loss (for multi-class). Next Topic: MAP Estimation Maximum Likelihood Estimation and Overfitting â€¢ In our abstract setting with data D the MLE is: â€¢ But conceptually MLE is a bit weird: â€“ â€œFind the â€˜wâ€™ that makes â€˜Dâ€™ have the highest probability given â€˜wâ€™.â€ â€¢ And MLE often leads to overfitting: â€“ Data could be very likely for some very unlikely â€˜wâ€™. â€“ For example, a complex model that overfits by memorizing the data. â€¢ What we really want: â€“ â€œFind the â€˜wâ€™ that has the highest probability given the data D.â€ Maximum a Posteriori (MAP) Estimation â€¢ Maximum a posteriori (MAP) estimate maximizes the reverse probability: â€“ This is what we want: the probability of â€˜wâ€™ given our data. â€¢ MLE and MAP are connected by Bayes rule: â€¢ So MAP maximizes the likelihood p(D|w) times the prior p(w): â€“ Prior is our â€œbeliefâ€ that â€˜wâ€™ is correct before seeing data. â€“ Prior can reflect that complex models are likely to overfit. MAP Estimation and Regularization â€¢ From Bayes rule, the MAP estimate with IID examples Di is: â€¢ By again taking the negative of the logarithm as before we get: â€¢ So we can view the negative log-prior as a regularizer: â€“ Many regularizers are equivalent to negative log-priors. L2-Regularization and MAP Estimation â€¢ We obtain L2-regularization under an independent Gaussian assumption: â€¢ This implies that: â€¢ So we have that: â€¢ With this prior, the MAP estimate with IID training examples would be MAP Estimation and Regularization â€¢ MAP estimation gives link between probabilities and loss functions. â€“ Gaussian likelihood (Ïƒ = 1) + Gaussian prior gives L2-regularized least squares. â€“ Laplace likelihood (Ïƒ = 1) + Gaussian prior gives L2-regularized robust regression: â€“ As â€˜nâ€™ goes to infinity, effect of prior/regularizer goes to zero. â€“ Unlike with MLE, the choice of Ïƒ changes the MAP solution for these models. Summarizing the past few slides â€¢ Many of our loss functions and regularizers have probabilistic interpretations. â€“ Laplace likelihood leads to absolute error. â€“ Laplace prior leads to L1-regularization. â€¢ The choice of likelihood corresponds to the choice of loss. â€“ Our assumptions about how the yi-values can come from the xi and â€˜wâ€™. â€¢ The choice of prior corresponds to the choice of regularizer. â€“ Our assumptions about which â€˜wâ€™ values are plausible. Regularizing Other Models â€¢ We can view priors in other models as regularizers. â€¢ Remember the problem with MLE for naÃ¯ve Bayes: â€¢ The MLE of p(â€˜lactaseâ€™ = 1| â€˜spamâ€™) is: count(spam,lactase)/count(spam). â€¢ But this caused problems if count(spam,lactase) = 0. â€¢ Our solution was Laplace smoothing: â€“ Add â€œ+1â€ to our estimates: (count(spam,lactase)+1)/(counts(spam)+2). â€“ This corresponds to a â€œBetaâ€ prior so Laplace smoothing is a regularizer. Why do we care about MLE and MAP? â€¢ Unified way of thinking about many of our tricks? â€“ Probabilitic interpretation of logistic loss. â€“ Laplace smoothing and L2-regularization are doing the same thing. â€¢ Remember our two ways to reduce overfitting in complicated models: â€“ Model averaging (ensemble methods). â€“ Regularization (linear models). â€¢ â€œFullyâ€-Bayesian methods (CPSC 440) combine both of these. â€“ Average over all models, weighted by posterior (including regularizer). â€“ Can use extremely-complicated models without overfitting. Losses for Other Discrete Labels â€¢ MLE/MAP gives loss for classification with basic labels: â€“ Least squares and absolute loss for regression. â€“ Logistic regression for binary labels {â€œspamâ€, â€œnot spamâ€}. â€“ Softmax regression for multi-class {â€œspamâ€, â€œnot spamâ€, â€œimportantâ€}. â€¢ But MLE/MAP lead to losses with other discrete labels (bonus): â€“ Ordinal: {1 star, 2 stars, 3 stars, 4 stars, 5 stars}. â€“ Counts: 602 â€˜likesâ€™. â€“ Survival rate: 60% of patients were still alive after 3 years. â€“ Unbalanced classes: 99.9% of examples are classified as +1. â€¢ Define likelihood of labels, and use NLL as the loss function. â€¢ We can also use ratios of probabilities to define more losses (bonus): â€“ Binary SVMs, multi-class SVMs, and â€œpairwise preferencesâ€ (ranking) models. Summary â€¢ Maximum likelihood estimate viewpoint of common models. â€“ Objective functions are equivalent to maximizing p(y, X | w) or p(y | X, w). â€¢ MAP estimation directly models p(w | X, y). â€“ Gives probabilistic interpretation to regularization. â€¢ Losses for weird scenarios are possible using MLE/MAP: â€“ Ordinal labels, count labels, censored labels, unbalanced labels. â€¢ Next time: â€“ What â€˜partsâ€™ are your personality made of? Loss vs. Objective vs. Error vs. CostDiscussion: Least Squares and Gaussian Assumption â€¢ Classic justifications for the Gaussian assumption underlying least squares: â€“ Your noise might really be Gaussian. (It probably isn't, but maybe it's a good enough approximation.) â€“ The central limit theorem (CLT) from probability theory. (If you add up enough IID random variables, the estimate of their mean converges to a Gaussian distribution.) â€¢ I think the CLT justification is wrong as we've never assumed that the xij are IID across â€˜jâ€™ values. We only assumed that the examples xi are IID across â€˜iâ€™ values, so the CLT implies that our estimate of â€˜wâ€™ would be a Gaussian distribution under different samplings of the data, but this says nothing about the distribution of yi given wTxi. â€¢ On the other hand, there are reasons *not* to use a Gaussian assumption, like it's sensitivity to outliers. This was (apparently) what lead Laplace to propose the Laplace distribution as a more robust model of the noise. â€¢ The \"student t\" distribution (published anonymously by Gosset while working at the Guiness beer company) is even more robust, but doesn't lead to a convex objective. Binary vs. Multi-Class Logistic â€¢ How does multi-class logistic generalize the binary logistic model? â€¢ We can re-parameterize softmax in terms of (k-1) values of oc: â€“ This is due to the â€œsum to 1â€ property (one of the oc values is redundant). â€“ So if k=2, we donâ€™t need a o2 and only need a single â€˜oâ€™. â€“ Further, when k=2 the probabilities can be written as: â€“ Renaming â€˜2â€™ as â€˜-1â€™, we get the binary logistic regression probabilities. Ordinal Labels â€¢ Ordinal data: categorical data where the order matters: â€“ Rating hotels as {â€˜1 starâ€™, â€˜2 starsâ€™, â€˜3 starsâ€™, â€˜4 starsâ€™, â€˜5 starsâ€™}. â€“ Softmax would ignore order. â€¢ Can use â€˜ordinal logistic regressionâ€™. Count Labels â€¢ Count data: predict the number of times something happens. â€“ For example, yi = â€œ602â€ Facebook likes. â€¢ Softmax requires finite number of possible labels. â€¢ We probably donâ€™t want separate parameter for â€˜654â€™ and â€˜655â€™. â€¢ Poisson regression: use probability from Poisson count distribution. â€“ Many variations exist, a lot of people think this isnâ€™t the best likelihood. Censored Survival Analysis (Cox Partial Likelihood) â€¢ Censored survival analysis: â€“ Target yi is last time at which we know person is alive. â€¢ But some people are still alive (so they have the same yi values). â€¢ The yi values (time at which they die) are â€œcensoredâ€. â€“ We use vi=0 is they are still alive and otherwise we set vi = 1. â€¢ Cox partial likelihood assumes â€œinstantaneousâ€ rate of dying depends on xi but not on total time theyâ€™ve been alive (not that realistic). Leads to likelihood of the â€œcensoredâ€ data of the form: â€¢ There are many extensions and alternative likelihoods. Other Parsimonious Parameterizations â€¢ Sigmoid isnâ€™t the way to model a binary p(yi | xi, w): â€“ Probit (uses CDF of normal distribution, very similar to logistic). â€“ Noisy-Or (simpler to specify probabilities by hand). â€“ Extreme-value loss (good with class imbalance). â€“ Cauchit, Gosset, and many others existâ€¦ Unbalanced Training Sets â€¢ Consider the case of binary classification where your training set has 99% class -1 and only 1% class +1. â€“ This is called an â€œunbalancedâ€ training set â€¢ Question: is this a problem? â€¢ Answer: it depends! â€“ If these proportions are representative of the test set proportions, and you care about both types of errors equally, then â€œnoâ€ itâ€™s not a problem. â€¢ You can get 99% accuracy by just always predicting -1, so ML can only help with the 1%. â€“ But itâ€™s a problem if the test set is not like the training set (e.g. your data collection process was biased because it was easier to get -1â€™s) â€“ Itâ€™s also a problem if you care more about one type of error, e.g. if mislabeling a +1 as a -1 is much more of a problem than the opposite â€¢ For example if +1 represents â€œtumorâ€ and -1 is â€œno tumorâ€ 33 Unbalanced Training Sets â€¢ This issue comes up a lot in practice! â€¢ How to fix the problem of unbalanced training sets? â€“ Common strategy is to build a â€œweightedâ€ model: â€¢ Put higher weight on the training examples with yi=+1. â€¢ This is equivalent to replicating those examples in the training set. â€¢ You could also subsample the majority class to make things more balanced. â€¢ Boostrap: create a dataset of size â€˜nâ€™ where n/2 are sampled from +1, n/2 from -1. â€“ Another approach is to try to make â€œfakeâ€ data to fill in minority class. â€“ Another option is to change to an asymmetric loss function (next slides) that penalizes one type of error more than the other. â€“ Some discussion of different methods here. 34 Unbalanced Data and Extreme-Value Loss â€¢ Consider binary case where: â€“ One class overwhelms the other class (â€˜unbalancedâ€™ data). â€“ Really important to find the minority class (e.g., minority class is tumor). Unbalanced Data and Extreme-Value Loss â€¢ Extreme-value distribution: Unbalanced Data and Extreme-Value Loss â€¢ Extreme-value distribution: Loss Functions from Probability Ratios â€¢ Weâ€™ve seen that loss functions can come from probabilities: â€“ Gaussian => squared loss, Laplace => absolute loss, sigmoid => logistic. â€¢ Most other loss functions can be derived from probability ratios. â€“ Example: sigmoid => hinge. Loss Functions from Probability Ratios â€¢ Weâ€™ve seen that loss functions can come from probabilities: â€“ Gaussian => squared loss, Laplace => absolute loss, sigmoid => logistic. â€¢ Most other loss functions can be derived from probability ratios. â€“ Example: sigmoid => hinge. Loss Functions from Probability Ratios â€¢ Weâ€™ve seen that loss functions can come from probabilities: â€“ Gaussian => squared loss, Laplace => absolute loss, sigmoid => logistic. â€¢ Most other loss functions can be derived from probability ratios. â€“ Example: sigmoid => hinge. Loss Functions from Probability Ratios â€¢ Weâ€™ve seen that loss functions can come from probabilities: â€“ Gaussian => squared loss, Laplace => absolute loss, sigmoid => logistic. â€¢ Most other loss functions can be derived from probability ratios. â€“ Example: sigmoid => hinge. Loss Functions from Probability Ratios â€¢ General approach for defining losses using probability ratios: 1. Define constraint based on probability ratios. 2. Minimize violation of logarithm of constraint. â€¢ Example: softmax => multi-class SVMs. Supervised Ranking with Pairwise Preferences â€¢ Ranking with pairwise preferences: â€“ We arenâ€™t given any explicit yi values. â€“ Instead weâ€™re given list of objects (i,j) where yi > yj.","libVersion":"0.2.1","langs":""}