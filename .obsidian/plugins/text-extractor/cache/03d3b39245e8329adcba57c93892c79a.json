{"path":".obsidian/plugins/text-extractor/cache/03d3b39245e8329adcba57c93892c79a.json","text":"CPSC 340: Machine Learning and Data Mining Principal Component Analysis Last Time: MAP Estimation â€¢ MAP estimation maximizes posterior: â€¢ Likelihood measures probability of labels â€˜yâ€™ given parameters â€˜wâ€™. â€¢ Prior measures probability of parameters â€˜wâ€™ before we see data. â€¢ For IID training data and independent priors, equivalent to using: â€¢ So log-likelihood is an error function, and log-prior is a regularizer. â€“ Squared error comes from Gaussian likelihood. â€“ L2-regularization comes from Gaussian prior. End of Part 3: Key Concepts â€¢ Linear models predict based on linear combination(s) of features: â€¢ We model non-linear effects using a change of basis: â€“ Replace d-dimensional xi with k-dimensional zi and use vTzi. â€“ Examples include polynomial basis and (non-parametric) RBFs. â€¢ Regression is supervised learning with continuous labels. â€“ Logical error measure for regression is squared error: â€“ Can be solved as a system of linear equations. End of Part 3: Key Concepts â€¢ Gradient descent finds local minimum of smooth objectives. â€“ Converges to a global optimum for convex functions. â€“ Can use smooth approximations (Huber, log-sum-exp) â€¢ Stochastic gradient methods allow huge/infinite â€˜nâ€™. â€“ Though very sensitive to the step-size. â€¢ Kernels let us use similarity between examples, instead of features. â€“ Lets us use some exponential- or infinite-dimensional features. â€¢ Feature selection is a messy topic. â€“ Classic method is forward selection based on L0-norm. â€“ L1-regularization simultaneously regularizes and selects features. End of Part 3: Key Concepts â€¢ We can reduce over-fitting by using regularization: â€¢ Squared error is not always right measure: â€“ Absolute error is less sensitive to outliers. â€“ Logistic loss and hinge loss are better for binary yi. â€“ Softmax loss is better for multi-class yi. â€¢ MLE/MAP perspective: â€“ We can view loss as log-likelihood and regularizer as log-prior. â€“ Allows us to define losses based on probabilities. The Story So Farâ€¦ â€¢ Part 1: Supervised Learning. â€“ Methods based on counting and distances. â€¢ Part 2: Unsupervised Learning. â€“ Methods based on counting and distances. â€¢ Part 3: Supervised Learning (just finished). â€“ Methods based on linear models and gradient descent. â€¢ Part 4: Unsupervised Learning. â€“ Methods based on linear models and gradient descent. Motivation: Human vs. Machine Perception â€¢ Huge difference between what we see and what computer sees: â€¢ But maybe images shouldnâ€™t be written as combinations of pixels. â€“ Can we learn a better representation? â€“ In other words, can we learn good features? What we see: What the computer â€œseesâ€: Motivation: Pixels vs. Parts â€¢ Can view 28x28 image as weighted sum of â€œsingle pixel onâ€ images: â€“ We have one image/feature for each pixel. â€“ The weights specify â€œhow much of this pixel is in the imageâ€. â€¢ A weight of zero means that pixel is white, a weight of 1 means itâ€™s black. â€¢ This is non-intuitive, isnâ€™t a â€œ3â€ made of small number of â€œpartsâ€? â€“ Now the weights are â€œhow much of this part is in the imageâ€. Motivation: Pixels vs. Parts â€¢ We could represent other digits as different combinations of â€œpartsâ€: â€¢ Consider replacing images xi by the weights zi of the different parts: â€“ The 784-dimensional xi for the â€œ5â€ image is replaced by 7 numbers: zi = [1 0 1 1 1 0 1]. â€“ Features like this could make learning much easier. Part 4: Latent-Factor Models â€¢ The â€œpart weightsâ€ are a change of basis from xi to some zi. â€“ But in high dimensions, it can be hard to find a good basis. â€¢ Part 4 is about learning the basis from the data. â€¢ Why? â€“ Supervised learning: we could use â€œpart weightsâ€ as our features. â€“ Outlier detection: it might be an outlier if isnâ€™t a combination of usual parts. â€“ Dimension reduction: compress data into limited number of â€œpart weightsâ€. â€“ Visualization: if we have only 2 â€œpart weightsâ€, we can view data as a scatterplot. â€“ Interpretation: we can try and figure out what the â€œpartsâ€ represent. Previously: Vector Quantization â€¢ Recall using k-means for vector quantization: â€“ Run k-means to find a set of â€œmeansâ€ wc. â€“ This gives a cluster à·œğ‘¦i for each object â€˜iâ€™. â€“ Replace features xi by mean of cluster: â€¢ This can be viewed as a (really bad) latent-factor model. Vector Quantization (VQ) as Latent-Factor Model â€¢ When d=3, we could write xi exactly as: â€“ In this â€œpointlessâ€ latent-factor model we have zi = [xi1 xi2 xi3]. â€¢ If xi is in cluster 2, VQ approximates xi by mean w2 of cluster 2: â€¢ So in this example we would have zi = [0 1 0 0]. â€“ The â€œpartsâ€ are the means from k-means. â€“ VQ only uses one part (the â€œpartâ€ from the cluster). Vector Quantization vs. PCA â€¢ Viewing vector quantization as a latent-factor model: â€¢ Suppose weâ€™re doing supervised learning, and the colours are the true labels â€˜yâ€™: â€“ Classification would be really easy with this â€œk-means basisâ€ â€˜Zâ€™. Vector Quantization vs. PCA â€¢ Viewing vector quantization as a latent-factor model: â€¢ But it only uses 1 part, itâ€™s just memorizing â€˜kâ€™ points in xi space. â€“ What we want is combinations of parts. â€¢ PCA is a generalization that allows continuous â€˜ziâ€™: â€“ It can have more than 1 non-zero. â€“ It can use fractional weights and negative weights. Principal Component Analysis (PCA) Applications â€¢ Principal component analysis (PCA) has been invented many times: https://en.wikipedia.org/wiki/Principal_component_analysis PCA Notation (MEMORIZE) â€¢ PCA takes in a matrix â€˜Xâ€™ and an input â€˜kâ€™, and outputs two matrices: â€¢ For row â€˜câ€™ of W, we use the notation wc. â€“ Each wc is a â€œpartâ€ (also called a â€œfactorâ€ or â€œprincipal componentâ€). â€¢ For row â€˜iâ€™ of Z, we use the notation zi. â€“ Each zi is a set of â€œpart weightsâ€ (or â€œfactor loadingsâ€ or â€œfeaturesâ€). â€¢ For column â€˜jâ€™ of W, we use the notation wj. â€“ Index â€˜jâ€™ of all the â€˜kâ€™ â€œpartsâ€ (value of pixel â€˜jâ€™ in all the different parts). PCA Notation (MEMORIZE) â€¢ PCA takes in a matrix â€˜Xâ€™ and an input â€˜kâ€™, and outputs two matrices: â€¢ With this notation, we can write our approximation of one xij as: â€“ K-means: â€œtake index â€˜jâ€™ of closest meanâ€. â€“ PCA: â€œzi gives weights for index â€˜jâ€™ of all meansâ€. â€¢ We can write approximation of the vector xi as: Different views (MEMORIZE) â€¢ PCA approximates each xij by the inner product < wj, zi >. â€¢ PCA approximates vector xi by the matrix-vector product WTzi. â€¢ PCA approximates matrix â€˜Xâ€™ by the matrix-matrix product ZW. â€“ PCA is also called a â€œmatrix factorizationâ€ model. â€“ Both â€˜Zâ€™ and â€˜Wâ€™ are variables. â€¢ This can be viewed as a â€œchange of basisâ€ from xi to zi values. â€“ The â€œbasis vectorsâ€ are the rows of W, the wc. â€“ The â€œcoordinatesâ€ in the new basis of each xi are the zi. â€¢ Applications of PCA: â€“ Dimensionality reduction: replace â€˜Xâ€™ with lower-dimensional â€˜Zâ€™. â€¢ If k << d, then compresses data. â€¢ Often better approximation than vector quantization. PCA Applications â€¢ Applications of PCA: â€“ Dimensionality reduction: replace â€˜Xâ€™ with lower-dimensional â€˜Zâ€™. â€¢ If k << d, then compresses data. â€¢ Often better approximation than vector quantization. PCA Applications â€¢ Applications of PCA: â€“ Dimensionality reduction: replace â€˜Xâ€™ with lower-dimensional â€˜Zâ€™. â€¢ If k << d, then compresses data. â€¢ Often better approximation than vector quantization. PCA Applications https://monsterlegacy.net/2013/03/04/rancor-star-wars/ â€¢ Applications of PCA: â€“ Dimensionality reduction: replace â€˜Xâ€™ with lower-dimensional â€˜Zâ€™. â€¢ If k << d, then compresses data. â€¢ Often better approximation than vector quantization. PCA Applications https://monsterlegacy.net/2013/03/04/rancor-star-wars/ â€¢ Applications of PCA: â€“ Outlier detection: if PCA gives poor approximation of xi, could be â€˜outlierâ€™. â€¢ Though we will see PCA uses squared error so PCA training is sensitive to outliers. PCA Applications â€¢ Applications of PCA: â€“ Partial least squares: uses PCA features as basis for linear model. PCA Applications â€¢ Applications of PCA: â€“ Data visualization: plot zi with k = 2 to visualize high-dimensional objects. http://infoproc.blogspot.ca/2008/11/european-genetic-substructure.html PCA Applications â€¢ Applications of PCA: â€“ Data visualization: plot zi with k = 2 to visualize high-dimensional objects. â€¢ Can augment other visualizations: https://www.sciencedaily.com/releases/2018/01/180125140943.htm PCA Applications â€¢ Applications of PCA: â€“ Data interpretation: we can try to assign meaning to latent factors wc. â€¢ Hidden â€œfactorsâ€ that influence all the variables. https://new.edu/resources/big-5-personality-traits PCA Applications \"Most Personality Quizzes Are Junk Science. I Found One That Isn't.\" What is PCA actually doing? When should PCA work well? Today I just want to show geometry, weâ€™ll talk about implementation next time. Doom Overhead Map and Latent-Factor Models â€¢ Original â€œDoomâ€ video game included an â€œoverhead mapâ€ feature: â€¢ This map can be viewed as a latent-factor model of player location. https://en.wikipedia.org/wiki/Doom_(1993_video_game) https://forum.minetest.net/viewtopic.php?f=5&t=9666 Overhead Map and Latent-Factor Models â€¢ Actual player location at time â€˜iâ€™ can be described by 3 coordinates: â€¢ The overhead map approximates these 3 coordinates with only 2: â€¢ Our k=2 latent factors are the following: â€¢ So our approximation of xi is: Overhead Map and Latent-Factor Models â€¢ The â€œoverhead mapâ€ approximation just ignores the â€œheightâ€. â€“ This is a good approximation if the world is flat. â€¢ Even if the character jumps, the first two features will approximate location. â€“ But itâ€™s a poor approximation if heights are different. Overhead Map and Latent-Factor Models â€¢ Consider these crazy goats trying to get some salt: â€“ Ignoring height gives poor approximation of goat location. â€¢ But the â€œgoat spaceâ€ is basically a two-dimensional plane. â€“ Better k=2 approximation: define â€˜Wâ€™ so that combinations give the plane. www.momtastic.com/webecoist/2010/11/07/some-fine-dam-climbing-goats-scaling-steep-vertical-wall https://www.quora.com/What-is-a-simplified-explanation-and-proof-of-the-Johnson-Lindenstrauss-lemma Summary â€¢ Latent-factor models: â€“ Try to learn basis Z from training examples X. â€“ Usually, the zi are â€œpart weightsâ€ for â€œpartsâ€ wc. â€“ Useful for dimensionality reduction, visualization, factor discovery, etc. â€¢ Principal component analysis: â€“ Writes each training examples as linear combination of parts. â€¢ We learn both the â€œpartsâ€ â€˜Wâ€™ and the â€œfeaturesâ€ Z. â€“ We can view â€˜Wâ€™ as best lower-dimensional hyper-plane. â€“ We can view â€˜Zâ€™ as the coordinates in the lower-dimensional hyper-plane. â€¢ Next time: PCA in 4 lines of code.","libVersion":"0.2.1","langs":""}