{"path":".obsidian/plugins/text-extractor/cache/03d3b39245e8329adcba57c93892c79a.json","text":"CPSC 340: Machine Learning and Data Mining Principal Component Analysis Last Time: MAP Estimation • MAP estimation maximizes posterior: • Likelihood measures probability of labels ‘y’ given parameters ‘w’. • Prior measures probability of parameters ‘w’ before we see data. • For IID training data and independent priors, equivalent to using: • So log-likelihood is an error function, and log-prior is a regularizer. – Squared error comes from Gaussian likelihood. – L2-regularization comes from Gaussian prior. End of Part 3: Key Concepts • Linear models predict based on linear combination(s) of features: • We model non-linear effects using a change of basis: – Replace d-dimensional xi with k-dimensional zi and use vTzi. – Examples include polynomial basis and (non-parametric) RBFs. • Regression is supervised learning with continuous labels. – Logical error measure for regression is squared error: – Can be solved as a system of linear equations. End of Part 3: Key Concepts • Gradient descent finds local minimum of smooth objectives. – Converges to a global optimum for convex functions. – Can use smooth approximations (Huber, log-sum-exp) • Stochastic gradient methods allow huge/infinite ‘n’. – Though very sensitive to the step-size. • Kernels let us use similarity between examples, instead of features. – Lets us use some exponential- or infinite-dimensional features. • Feature selection is a messy topic. – Classic method is forward selection based on L0-norm. – L1-regularization simultaneously regularizes and selects features. End of Part 3: Key Concepts • We can reduce over-fitting by using regularization: • Squared error is not always right measure: – Absolute error is less sensitive to outliers. – Logistic loss and hinge loss are better for binary yi. – Softmax loss is better for multi-class yi. • MLE/MAP perspective: – We can view loss as log-likelihood and regularizer as log-prior. – Allows us to define losses based on probabilities. The Story So Far… • Part 1: Supervised Learning. – Methods based on counting and distances. • Part 2: Unsupervised Learning. – Methods based on counting and distances. • Part 3: Supervised Learning (just finished). – Methods based on linear models and gradient descent. • Part 4: Unsupervised Learning. – Methods based on linear models and gradient descent. Motivation: Human vs. Machine Perception • Huge difference between what we see and what computer sees: • But maybe images shouldn’t be written as combinations of pixels. – Can we learn a better representation? – In other words, can we learn good features? What we see: What the computer “sees”: Motivation: Pixels vs. Parts • Can view 28x28 image as weighted sum of “single pixel on” images: – We have one image/feature for each pixel. – The weights specify “how much of this pixel is in the image”. • A weight of zero means that pixel is white, a weight of 1 means it’s black. • This is non-intuitive, isn’t a “3” made of small number of “parts”? – Now the weights are “how much of this part is in the image”. Motivation: Pixels vs. Parts • We could represent other digits as different combinations of “parts”: • Consider replacing images xi by the weights zi of the different parts: – The 784-dimensional xi for the “5” image is replaced by 7 numbers: zi = [1 0 1 1 1 0 1]. – Features like this could make learning much easier. Part 4: Latent-Factor Models • The “part weights” are a change of basis from xi to some zi. – But in high dimensions, it can be hard to find a good basis. • Part 4 is about learning the basis from the data. • Why? – Supervised learning: we could use “part weights” as our features. – Outlier detection: it might be an outlier if isn’t a combination of usual parts. – Dimension reduction: compress data into limited number of “part weights”. – Visualization: if we have only 2 “part weights”, we can view data as a scatterplot. – Interpretation: we can try and figure out what the “parts” represent. Previously: Vector Quantization • Recall using k-means for vector quantization: – Run k-means to find a set of “means” wc. – This gives a cluster ො𝑦i for each object ‘i’. – Replace features xi by mean of cluster: • This can be viewed as a (really bad) latent-factor model. Vector Quantization (VQ) as Latent-Factor Model • When d=3, we could write xi exactly as: – In this “pointless” latent-factor model we have zi = [xi1 xi2 xi3]. • If xi is in cluster 2, VQ approximates xi by mean w2 of cluster 2: • So in this example we would have zi = [0 1 0 0]. – The “parts” are the means from k-means. – VQ only uses one part (the “part” from the cluster). Vector Quantization vs. PCA • Viewing vector quantization as a latent-factor model: • Suppose we’re doing supervised learning, and the colours are the true labels ‘y’: – Classification would be really easy with this “k-means basis” ‘Z’. Vector Quantization vs. PCA • Viewing vector quantization as a latent-factor model: • But it only uses 1 part, it’s just memorizing ‘k’ points in xi space. – What we want is combinations of parts. • PCA is a generalization that allows continuous ‘zi’: – It can have more than 1 non-zero. – It can use fractional weights and negative weights. Principal Component Analysis (PCA) Applications • Principal component analysis (PCA) has been invented many times: https://en.wikipedia.org/wiki/Principal_component_analysis PCA Notation (MEMORIZE) • PCA takes in a matrix ‘X’ and an input ‘k’, and outputs two matrices: • For row ‘c’ of W, we use the notation wc. – Each wc is a “part” (also called a “factor” or “principal component”). • For row ‘i’ of Z, we use the notation zi. – Each zi is a set of “part weights” (or “factor loadings” or “features”). • For column ‘j’ of W, we use the notation wj. – Index ‘j’ of all the ‘k’ “parts” (value of pixel ‘j’ in all the different parts). PCA Notation (MEMORIZE) • PCA takes in a matrix ‘X’ and an input ‘k’, and outputs two matrices: • With this notation, we can write our approximation of one xij as: – K-means: “take index ‘j’ of closest mean”. – PCA: “zi gives weights for index ‘j’ of all means”. • We can write approximation of the vector xi as: Different views (MEMORIZE) • PCA approximates each xij by the inner product < wj, zi >. • PCA approximates vector xi by the matrix-vector product WTzi. • PCA approximates matrix ‘X’ by the matrix-matrix product ZW. – PCA is also called a “matrix factorization” model. – Both ‘Z’ and ‘W’ are variables. • This can be viewed as a “change of basis” from xi to zi values. – The “basis vectors” are the rows of W, the wc. – The “coordinates” in the new basis of each xi are the zi. • Applications of PCA: – Dimensionality reduction: replace ‘X’ with lower-dimensional ‘Z’. • If k << d, then compresses data. • Often better approximation than vector quantization. PCA Applications • Applications of PCA: – Dimensionality reduction: replace ‘X’ with lower-dimensional ‘Z’. • If k << d, then compresses data. • Often better approximation than vector quantization. PCA Applications • Applications of PCA: – Dimensionality reduction: replace ‘X’ with lower-dimensional ‘Z’. • If k << d, then compresses data. • Often better approximation than vector quantization. PCA Applications https://monsterlegacy.net/2013/03/04/rancor-star-wars/ • Applications of PCA: – Dimensionality reduction: replace ‘X’ with lower-dimensional ‘Z’. • If k << d, then compresses data. • Often better approximation than vector quantization. PCA Applications https://monsterlegacy.net/2013/03/04/rancor-star-wars/ • Applications of PCA: – Outlier detection: if PCA gives poor approximation of xi, could be ‘outlier’. • Though we will see PCA uses squared error so PCA training is sensitive to outliers. PCA Applications • Applications of PCA: – Partial least squares: uses PCA features as basis for linear model. PCA Applications • Applications of PCA: – Data visualization: plot zi with k = 2 to visualize high-dimensional objects. http://infoproc.blogspot.ca/2008/11/european-genetic-substructure.html PCA Applications • Applications of PCA: – Data visualization: plot zi with k = 2 to visualize high-dimensional objects. • Can augment other visualizations: https://www.sciencedaily.com/releases/2018/01/180125140943.htm PCA Applications • Applications of PCA: – Data interpretation: we can try to assign meaning to latent factors wc. • Hidden “factors” that influence all the variables. https://new.edu/resources/big-5-personality-traits PCA Applications \"Most Personality Quizzes Are Junk Science. I Found One That Isn't.\" What is PCA actually doing? When should PCA work well? Today I just want to show geometry, we’ll talk about implementation next time. Doom Overhead Map and Latent-Factor Models • Original “Doom” video game included an “overhead map” feature: • This map can be viewed as a latent-factor model of player location. https://en.wikipedia.org/wiki/Doom_(1993_video_game) https://forum.minetest.net/viewtopic.php?f=5&t=9666 Overhead Map and Latent-Factor Models • Actual player location at time ‘i’ can be described by 3 coordinates: • The overhead map approximates these 3 coordinates with only 2: • Our k=2 latent factors are the following: • So our approximation of xi is: Overhead Map and Latent-Factor Models • The “overhead map” approximation just ignores the “height”. – This is a good approximation if the world is flat. • Even if the character jumps, the first two features will approximate location. – But it’s a poor approximation if heights are different. Overhead Map and Latent-Factor Models • Consider these crazy goats trying to get some salt: – Ignoring height gives poor approximation of goat location. • But the “goat space” is basically a two-dimensional plane. – Better k=2 approximation: define ‘W’ so that combinations give the plane. www.momtastic.com/webecoist/2010/11/07/some-fine-dam-climbing-goats-scaling-steep-vertical-wall https://www.quora.com/What-is-a-simplified-explanation-and-proof-of-the-Johnson-Lindenstrauss-lemma Summary • Latent-factor models: – Try to learn basis Z from training examples X. – Usually, the zi are “part weights” for “parts” wc. – Useful for dimensionality reduction, visualization, factor discovery, etc. • Principal component analysis: – Writes each training examples as linear combination of parts. • We learn both the “parts” ‘W’ and the “features” Z. – We can view ‘W’ as best lower-dimensional hyper-plane. – We can view ‘Z’ as the coordinates in the lower-dimensional hyper-plane. • Next time: PCA in 4 lines of code.","libVersion":"0.2.1","langs":""}