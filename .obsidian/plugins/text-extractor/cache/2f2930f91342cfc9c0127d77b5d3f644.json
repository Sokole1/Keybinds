{"path":".obsidian/plugins/text-extractor/cache/2f2930f91342cfc9c0127d77b5d3f644.json","text":"CPSC 340 Final (Fall 2016) Name: Student Number: Please enter your information above, turn oﬀ cellphones, space yourselves out throughout the room, and wait until the oﬃcial start of the exam to begin. You can raise your hand to ask a question, and please look up occasionally in case there are clariﬁcations written on the projector (the time remaining will also be written on the projector). You are welcome to (quietly) leave early if you ﬁnish before the ﬁnal 5 minutes of the exam, but please do not leave in the last 5 minutes as it is very distracting to those who want to work up to the last minute. The ﬁnal consists of 10 questions, and they will all be equally weighted in the marking scheme. Note that some question have multiple parts, written as (a), (b), and (c). Clearly mark where you are answering each part, show your work/reasoning for each part, and make sure to check that you answered all parts before handing in your ﬁnal. May the force be with you! 1 2 3 4 5 6 7 8 9 10 Total 1 1 Training/Validation/Testing You are asked by a client to build a system that solves a regression problem. They give you 3000 training examples and a set of 100 features for each example. You have been assured that the examples have been generated in way that makes them IID, and the examples have been given to you sorted based on the values of the ﬁrst feature. The client not only wants an accurate model, but they want an unbiased estimate of the accuracy of the ﬁnal model. Assume that the data is stored in a 3000 by 100 matrix X, and the targets are stored in a 3000 by 1 vector y. Assume that you have a ‘model’ function that depends on two parameters k1 and k2 with the following interface: • model = train(X,y,k1,k2); % Train model on {X, y} with hyper-parameters k1 and k2. • yhat = predict(model,Xhat); % Predicts using the model on Xhat. Assume that the two parameters k1 and k2 can take any integer value from 1 to 5. Give pseudo-code for a training/validation/testing procedure that chooses good values of k1 and k2, and then reports an unbiased estimate of the accuracy of the ﬁnal model. 2 2 Data Standardization Consider the following Matlab code for loading a dataset, standardizing the features, training a model, and making predictions on a test set: % Load d a t a {X, y , X t e s t } load data . mat [ n , d ] = s i z e (X ) ; [ t , ˜ ] = s i z e ( X t e s t ) ; % S t a n d a r d i z e f e a t u r e s mu = zeros ( d , 1 ) ; sigma = zeros ( d , 1 ) ; for j = 1 : d mu( j ) = mean(X( : , j ) ) ; sigma ( j ) = std (X( : , j ) ) ; X( : , j ) = (X( : , j ) − mu( j ) ) / sigma ( j ) ; end % F i t model model = f i t (X, y ) ; % Make p r e d i c t i o n s on t e s t s e t y t e s t = model . p r e d i c t ( model , X t e s t ) ; Unfortunately, there is a major error in the code above. This error causes the predictions ytest to be terrible, even though the training error is low and you don’t think that the model has overﬁt. (a) What is the major error in this code? (b) Give the code needed to ﬁx the error, and where it needs to be located in the above script. 3 3 Terminlogy and Key Properties Consider the following list of tools we discussed in the course: 1. Median. 2. Scatterplot. 3. Cross-validation. 4. KNN. 5. Density-based clustering. 6. Polynomial basis. 7. L1-regularization. 8. Softmax probability. 9. Multi-dimensional scaling (MDS). 10. PageRank. Match each one of the above items to one of the key properties below: 1. Assigning a score to a node in a graph. 2. Choosing regularization parameter. 3. Feature selection. 4. Finding non-convex clusters. 5. Multi-class loss function. 6. Non-linear regression. 7. Non-parametric classiﬁer. 8. Non-parametric visualization. 9. Robust alternative to mean. 10. Visualizing dependency between 2 variables. 4 4 Softmax and Decision Stumps Consider the dataset below, which has 10 training examples and 2 features: X =                 0 1 0 0 1 0 1 1 1 1 0 0 1 0 1 0 1 1 1 0                 , y =                 1 1 1 2 2 2 3 3 3 3                 , and consider a single test example: ˆx = [ 1 1 ] . (a) Suppose we ﬁt a multi-class linear classiﬁer using the softmax loss, and we obtain the following weight matrix: W = [ −1 +2 0 +2 0 +3 ] Under this model, what class label would we assign to the test example? (Show your work.) (b) What is the cost of prediction on t test examples with an already-trained softmax model? State your result in O() notation in terms of the number of training examples n, the number of test examples t, the number of features d, and the number of classes k? (c) What is the decision stump that minimizes the classiﬁcation error? 5 5 Parametric vs. Non-Parametric Next to each of the methods below, state whether it is parametric or non-parametric (where n is the number of training examples and d is the number of features): 1. Histogram with 10 bins. 2. Decision tree learner of depth √n. 3. 10-nearest neighbours. 4. K-means with √d means. 5. Density-based clustering. 6. Linear regression with polynomial basis of degree n. 7. Softmax multi-class classiﬁcation (linear basis). 8. Robust PCA. 9. ISOMAP (with MDS to compute zi). 10. Deep neural network. 6 6 Linear Regression Variations The tilted least squares objective function has the form f (w) = 1 2 n∑ i=1(wT xi − yi) 2 + λ d∑ j=1 wjvj, for a vector v with real-valued elements vj and where λ can be any real number. (a) Show that this objective function is convex. (b) Write the minimizer of the above objective function as the solution of a linear system. (c) Consider the L∞-regularized weighted absolute loss f (w) = n∑ i=1 zi|wT xi − yi| + λ max j |wj|, where the max is taken over j in {1, 2, . . . , d} and zi ≥ 0 for all i. Write this objective in matrix and norm notation. (You can use Z as a diagonal matrix with the zi values along the diagonal.) 7 7 MLE for Survival Analysis with Censoring Suppose we have a set of training examples (xi, yi, vi) where • yi is a positive number giving the last time that user i used Microsoft Windows. • vi = 1 if user i has quit using Microsoft Windows and vi = 0 if they are still using it. If we assume that the “instantaneous rate” that users quit using Windows depends on the features xi but not the total time that they’ve been using Windows, then a standard censored survival analysis likelihood is given by p(yi, vi|xi, w) = exp(viwT xi) exp(−yi exp(wT xi)). (a) If a dataset consists of n IID examples (xi, yi, vi), show how ﬁnding the maximum likelihood estimate (MLE) for w corresponds to minimizing an additive loss function, f (w) = n∑ i=1 g(yi, vi, w, xi), and derive the form of the loss function f (simplifying as much as possible). (b) If the largest value of yi in the training set is k, what is the cost of evaluating this objective function in terms of n, d, and k? (c) Show that the function derived in part (a) is convex (you can assume yi ≥ 0). 8 8 PCA and Regularization Someone gives you a code for PCA, and when you run the code it returns a matrix W whose rows have a norm of 1 and are orthogonal. However, when you run the code again it returns −W . (a) Why is this not necessarily a bug? (b) Consider a regularized objective for ﬁlling in missing entries xij of a matrix given a set of examples xij values (denoted by R) from the matrix. f (Z, W ) = 1 2 ∑ (i,j)∈R(wT j zi − xij)2 + λW 2 ∥W ∥ 2 F , with λW > 0. (Here, we’re not assuming that the rows are normalized or that they are orthogonal.) What is the eﬀect of λW on the fundamental trade-oﬀ ? (c) Consider the following variation on the objective function, f (Z, W ) = 1 2 ∑ (i,j)∈R(wT j zi − xij) 2 + λW 2 ∥W ∥ 2 F + λZ 2 ∥Z∥2 F , for λW > 0 and λZ > 0. Would the answer to (b) change? (d) Consider another variation on the objective function, f (Z, W ) = 1 2 ∑ (i,j)∈R(wT j zi − xij)2 + λW 2 ∥W ∥0, for λW > 0. Would the answer to (b) change? 9 9 Grouped PageRank In the last assignment we implemented the PageRank algorithm using a random walk strategy. Consider a variation of the webpage ranking problem where we have groups of webpages, and instead of ranking individual webpages to want to give rankings to the groups. As an example with 6 webpages, group 1 could consist of webpages {1, 2, 5} and gets a rank of 0.20, group 2 could consist of {3, 4} and get a rank of 0.70, and group 3 could consist of {6, } and get a rank of 0.10. The groups might represent webpages that come from the same server, that have the same theme, or that have some other logical organization. Write pseudo-code for a random walk algorithm that computes a PageRank-like score for groups of webpages. Remember that links are between individual webpages, and if you need it please use the following notation: • Use t to denote the “timestep” (iteration number) of the random walk. • Use i to refer to an individual webpage. • Use Ni to denote the outlinks of webpage i, and |Ni| to denote the number of outlinks. • Use gi to refer to the group of webpage i (assume each webpage is assigned to exacty one group). • Use ci to denote a count of the number of times we’ve visited page i. • Use kg to denote a count of the number of times we’ve visited group g. Deﬁne any extra notation that you use. 10 10 Neural Networks Consider a two-layer neural network model with L2-regularization, f (W (1), W (2), w) = 1 2 n∑ i=1(wT h(W (2)h(W (1)xi)) − yi) 2 + λ 2 ∥w∥2 + λ1 2 ∥W (1)∥ 2 F + λ2 2 ∥W (2)∥ 2 F , where W 1 is k1 by d, W 2 is k2 by k1, and w is k2 by 1. (a) Describe how the following factors aﬀect the two parts of the fundamental trade-oﬀ: 1. The parameter k1. 2. The parmaeter λ. 3. Increasing the depth (number of hidden layers) from 2 to a larger number m (b) Given only a training set, how can we choose values of the above parameters? 11","libVersion":"0.2.1","langs":""}