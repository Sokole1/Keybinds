{"path":".obsidian/plugins/text-extractor/cache/36bd6417c688273542a6a2fe47b75fb8.json","text":"CPSC 302, Fall 2022 Midterm 2 Solutions Question 1. (8 points) Determine whether the following statements are true or false. There is no need to justify your answer; just write T/F at a visible place. (a)1 pt Even if a matrix A is perfectly conditioned, i.e., κ(A) = 1, Gaussian Elimination without pivoting for solving a linear system Ax = b may still break down due to division by zero. Answer: True. ( Example : A = ( 0 1 1 0 )) (b)1 pt Ignoring roundoﬀ errors, if during the kth step of Gaussian elimination with partial piv- oting for solving a linear system Ax = b, i.e., during the step of zeroing the entries below the kth diagonal, a row of zeros is generated in the interim matrix A(k) (in the notation used in the lectures/textbook/slides), then the original matrix A is necessarily singular and the linear system does not have a unique solution. Answer: True. (Elementary row operations maintain singularity/nonsingularity.) (c)1 pt When numerically solving a sparse linear system Ax = b using Gaussian elimination with partial pivoting, the required computational work and storage are always equivalent in big O terms to the computational work and storage required for solving the linear system by computing A−1 explicitly ﬁrst, and then computing x = A−1b. Answer: False. (Counterexample: banded matrices.) (d)1 pt For a dense symmetric positive deﬁnite matrix A, thanks to symmetry and because no pivoting is required, computing the Cholesky decomposition and using it to solve the linear system Ax = b is computationally cheaper by an order of magnitude than standard Gaussian elimination with partial pivoting – it takes only O(n2) ﬂoating point operations. Answer: False. (Still O(n3)) (e)1 pt Suppose we numerically solve a linear system Ax = b and the computed solution is ˜x. Even if the norm of the relative residual, ∥b−A˜x∥ ∥b∥ is small, the norm of the relative error ∥x−˜x∥ ∥x∥ may be large. Answer: True. (f)1 pt In the solution of the linear least-squares problem minx ∥b−Ax∥2, the residual r = b−Ax is orthogonal to Ax. Answer: True. (Since AT r = 0, also xT AT r = (Ax)T r = 0.) (g)1 pt The matrix H = I − 2uuT , with ∥u∥2 = 1, is not always invertible, i.e., one can ﬁnd a vector u with with ∥u∥2 = 1 such that H is singular. Answer: False. (H is orthogonal, hence nonsingular.) (h)1 pt The matrix G = I − uuT , where where ∥u∥2 = 1, is symmetric and it satisﬁes G2 = G. Answer: True. Question 2. (6 points) Consider an ‘almost tridiagonal’ n × n matrix A, whose nonzero pattern is like the nonzero pattern of a tridiagonal matrix except an,1 ̸= 0. Here is a 6 × 6 example: A =         × × 0 0 0 0 × × × 0 0 0 0 × × × 0 0 0 0 × × × 0 0 0 0 × × × × 0 0 0 × ×  | | | | | |  . (a)2 pts If no pivoting is required, what are the nonzero patterns of the factors L and U of the LU decomposition of A? Just write down the nonzero pattern for the 6 × 6 example using × and 0; there is no need to provide a justiﬁcation. Answer: L =         × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 × × × × × ×  | | | | | |  ; U =         × × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 0 0 0 0 × × 0 0 0 0 0 ×  | | | | | |  (b)4 pts In answering this question, assume that n is large and you may ignore pivoting. State how much additional computational work and how much additional storage are required, if any, for solving the linear system Ax = b, compared to solving a linear system with a tridiagonal matrix. Refer speciﬁcally to the following and write your answers on the margins; there is no need to justify your answers. (i) Additional computational work for computing the LU decomposition A = LU , if any Answer: O(n) (due to new nonzeros generated in last row of L) (ii) Additional computational work for solving Ly = b, if any Answer: O(n) (last row of L) (iii) Additional computational work for solving U x = y, if any Answer: No additional work (iv) The overall additional storage that is required in the solution process, if any Answer: O(n) (last row of L) Make sure to refer to the above as a comparison with a standard tridiagonal system and state only the additional computational work and storage. Question 3. (6 points) Consider the linear least-squares problem min x ∥b − Ax∥2, where A is a full rank m × n matrix, with m > n. Suppose we are using the normal equations, and the matrix of the linear system has been slightly perturbed, as follows: instead of solving AT Ax = AT b, we solve (AT A + E)(x + y) = AT b, where E is a matrix and y is a vector, with ∥E∥ and ∥y∥ very small. That is, we refer to (x + y) as the solution of this modiﬁed linear system. Page 2 (a)2 pts Suppose Ey is so small that it can be neglected. Show that y = −(AT A) −1Ex. Answer: Opening the parenthesis and neglecting the small term Ey we get AT Ax + AT Ay + Ex = AT b. Using AT Ax = AT b, the above equation simpliﬁes to AT Ay + Ex = 0. moving Ex to the right-hand side and multiplying both sides by AT A)−1 gives us y = −(AT A)−1Ex, as required. (b)2 pts Denote B = AT A and suppose that ∥E∥ ∥B∥ < ε and that the condition number of B satisﬁes κ(B) = ∥B∥∥B−1∥ < M. Find a constant β that depends on ε and M such that the relative error satisﬁes ∥y∥ ∥x∥ < β. Answer: Writing B = AT A and taking norms of the solution for y obtained in part (a) we get ∥y∥ ≤ ∥B−1∥∥E∥∥x∥. Since B = AT A is nonsingular, x can only be zero if AT b = 0 and that’s a trivial situation that we ignore (in that case y would also be zero and this is of no interest whatsoever). We thus know that ∥x∥ > 0 and we can divide by it, multiply and divide by ∥B∥ on the right-hand side, and obtain ∥y∥ ∥x∥ ≤ ∥B−1∥∥B∥ | {z } κ(B) ∥E∥ ∥B∥ | {z } <ε . Therefore, β = M ε. (c)2 pts When A is ill-conditioned or does not have full rank, a common approach is to replace the normal equations AT Ax = AT b by (AT A + αI)˜x = A T b, where α > 0 is a positive scalar. There is a corresponding linear least-squares problem here, which you are not required to ﬁnd. Denote C = AT A + αI. Show that the matrix C is symmetric positive deﬁnite (and therefore Cholesky can be used). Do this in two steps, as follows. (There is no need to discuss the solution method, just show what you are being asked to show below.) Page 3 (i) Show that C is symmetric Answer: CT = (A T A + αI)T = (AT A)T + αI = AT (AT )T + αI = AT A + αI = C. (ii) Show that C is positive deﬁnite Answer: Given a vector x ̸= 0 we have xT Cx = xT (AT A + αI)x = xT AT Ax| {z } ∥Ax∥2 2≥0 + αxT x| {z } α∥x∥2 2>0 > 0. Page 4","libVersion":"0.2.1","langs":""}