{"path":".obsidian/plugins/text-extractor/cache/f5df682ec0be05c8dd6044402bd0851c.json","text":"3.3 Softmax Classifier Gradient [7 points] Using a one-ve-al classfer can hurt performance because the classfes are £ independently, so theee s 20 atempt to calibrate the columns of the matrix V. As ve discssed in lecture, an alternative to this {ndependent model s to us the softmax loss, which is gven by 0=y [,,,.;,, +log (Zcxp[m;’,z,))] . Show that the partial derivatives of this function, which make up it gradient, are given by the folloving expression: PP W{(’ =3l = | Wz 105 = 0l where, 13 =) is the indicator function (it i 1 when s = ¢ and 0 ofherwise) o bl = e W) i the prodicted probability of example being clss c, deined as . exp(ul) Pl = | Wia) = 2 HE2— S exp(utz)","libVersion":"0.2.1","langs":"eng"}