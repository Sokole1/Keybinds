{"path":".obsidian/plugins/text-extractor/cache/178c4b057b4eb823da90edba2f060289.json","text":"CPSC 340: Machine Learning and Data Mining Robust Regression Last Time: Gradient Descent and Convexity â€¢ We introduced gradient descent: â€“ Uses sequence of iterations of the form: â€“ Converges to a stationary point where âˆ‡ f(w) = 0 under weak conditions. â€¢ Will be a global minimum if the function is convex. â€¢ We discussed ways to show a function is convex: â€“ Second derivative is non-negative (1D functions). â€“ Closed under addition, multiplication by non-negative, maximization. â€“ Any [squared-]norm is convex. â€“ Composition of convex function with linear function is convex. Least Squares with Outliers â€¢ Height vs. weight of NBA players: https://www.youtube.com/watch?v=i4eYWl1ewFo Least Squares with Outliers â€¢ Consider least squares problem with outliers in â€˜yâ€™: http://setosa.io/ev/ordinary-least-squares-regression Least Squares with Outliers â€¢ Consider least squares problem with outliers in â€˜yâ€™: â€¢ Least squares is very sensitive to outliers. Least Squares with Outliers â€¢ Squaring error shrinks small errors, and magnifies large errors: â€¢ Outliers (large error) influence â€˜wâ€™ much more than other points. https://seeing-theory.brown.edu/regression-analysis/index.html Least Squares with Outliers â€¢ Squaring error shrinks small errors, and magnifies large errors: â€¢ Outliers (large error) influence â€˜wâ€™ much more than other points. â€“ Good if outlier means â€˜plane crashesâ€™, bad if it means â€˜data entry errorâ€™. Robust Regression â€¢ Robust regression objectives focus less on large errors (outliers). â€¢ For example, use absolute error instead of squared error: â€¢ Now decreasing â€˜smallâ€™ and â€˜largeâ€™ errors is equally important. â€¢ Instead of minimizing L2-norm, minimizes L1-norm of residuals: Least Squares with Outliers â€¢ Absolute error is more robust to outliers: Regression with the L1-Norm â€¢ Unfortunately, minimizing the absolute error is harder. â€“ We donâ€™t have â€œnormal equationsâ€ for minimizing the L1-norm. â€“ Absolute value is non-differentiable at 0. â€“ Generally, harder to minimize non-smooth than smooth functions. â€¢ Unlike smooth functions, the gradient may not get smaller near a minimizer. â€“ To apply gradient descent, weâ€™ll use a smooth approximation. Smooth Approximations to the L1-Norm â€¢ There are differentiable approximations to absolute value. â€“ Common example is Huber loss: â€“ Note that â€˜hâ€™ is differentiable: hâ€™(Îµ) = Îµ and hâ€™(-Îµ) = -Îµ. â€“ This â€˜fâ€™ is convex but setting ğ›»f(x) = 0 does not give a linear system. â€¢ But we can minimize the Huber loss using gradient descent. Very Robust Regression â€¢ Non-convex errors can be very robust: â€“ Not influenced by outlier groups. Very Robust Regression â€¢ Non-convex errors can be very robust: â€“ Not influenced by outlier groups. â€“ But non-convex, so finding global minimum is hard. â€“ Absolute value is â€œmost robustâ€ convex loss function. Very Robust RegressionMotivation for Modeling Outliers https://xkcd.com/937/ â€¢ What if the â€œoutlierâ€ is the only non-male person in your dataset? â€“ Do you want to be robust to the outlier? â€“ Will the model work for everyone if it has good average case performance? Accuracy vs. Fairness Trade-Off? â€¢ Improving test error might need to make fairness worse? â€“ If you chase you outliers you might worsen generalization. â€“ If you do not chase the outliers you might worsen fairness. â€¢ Recent related paper: https://www.jmlr.org/papers/v23/21-1427.html â€œBrittleâ€ Regression â€¢ What if you really care about getting the outliers right? â€“ You want to minimize size of worst error across examples. â€¢ For example, if in worst case the plane can crash or you perform badly on a group. â€¢ In this case you could use something like the infinity-norm: â€¢ Very sensitive to outliers (â€œbrittleâ€), but minimizes highest error. â€“ Different than previous errors, which were all some sort of average. Log-Sum-Exp Function â€¢ As with the L1-norm, the Lâˆ-norm is convex but non-smooth: â€“ We can again use a smooth approximation and fit it with gradient descent. â€¢ Convex and smooth approximation to max function is log-sum-exp function: â€“ We will use this several times in the course. â€“ Notation alert: when I write â€œlogâ€ I always mean â€œnaturalâ€ logarithm: log(e) = 1. â€¢ Intuition behind log-sum-exp: â€“ âˆ‘ğ‘– exp ğ‘§ğ‘– â‰ˆ max ğ‘– exp(ğ‘§ğ‘–), as largest element is magnified exponentially (if no ties). â€“ And notice that log(exp(zi)) = zi. Log-Sum-Exp Function Examples â€¢ Log-sum-exp function as smooth approximation to max: â€¢ If there arenâ€™t â€œcloseâ€ values, itâ€™s really close to the max. â€¢ Comparison of max{0,w} and smooth log(exp(0) + exp(w)): Part 3 Key Ideas: Linear Models, Least Squares â€¢ Focus of Part 3 is linear models: â€“ Supervised learning where prediction is linear combination of features: â€¢ Regression: â€“ Target yi is numerical, testing ( à·œğ‘¦i == yi) doesnâ€™t make sense. â€¢ Squared error: â€“ Can find optimal â€˜wâ€™ by solving â€œnormal equationsâ€. Part 3 Key Ideas: Change of Basis, Gradient Descent â€¢ Change of basis: replaces features xi with non-linear transforms zi: â€“ Add a bias variable (feature that is always one). â€“ Polynomial basis. â€“ Other basis functions (logarithms, trigonometric functions, etc.). â€¢ For large â€˜dâ€™ we often use gradient descent: â€“ Iterations only cost O(nd). â€“ Converges to a critical point of a smooth function. â€“ For convex functions, it finds a global optimum. Part 3 Key Ideas: Error Functions, Smoothing â€¢ Error functions: â€¢ Squared error is sensitive to outliers. â€¢ Absolute (L1) error and Huber error are more robust to outliers. â€¢ Brittle (Lâˆ) error is more sensitive to outliers. â€¢ L1 and Lâˆ error functions are convex but non-differentiable: â€¢ Finding â€˜wâ€™ minimizing these errors is harder than squared error. â€¢ We can approximate these with differentiable functions: â€¢ L1 can be approximated with Huber. â€¢ Lâˆ can be approximated with log-sum-exp. â€¢ With these smooth (convex) approximations, we can find global optimum with gradient descent. End of Scope for Midterm Material. (we are not done Part 3, but nothing after this point will be tested on the midterm) Finding the â€œTrueâ€ Model â€¢ To measure performance we have focused on minimize test error. â€“ This is good if our goal is to predict on well on new IID data â€“ But this is a weird way to do science! â€¢ â€œIt's true there's been a lot of work on trying to apply statistical models to various linguistic problems. I think there have been some successes, but a lot of failures. There is a notion of success ... which I think is novel in the history of science. It interprets success as approximating unanalyzed data.â€ Noam Chomsky. Finding the â€œTrueâ€ Model â€¢ To measure performance we have focused on minimize test error. â€“ This is good if our goal is to predict on well on new IID data â€“ But this is a weird way to do science! â€¢ We normally want to design simple models that explain the world. â€“ Might work even if new data is not IID! â€“ Next topic: finding theâ€œtrueâ€ model. â€¢ Hint: it is hard unless things are simple! https://www.discovermagazine.com/the-sciences/the-5-most-important-scientific-equations-of-all-time Finding the â€œTrueâ€ Model â€¢ Consider a simple case of trying to find the â€œtrueâ€ model? â€“ We believe that yi really is a polynomial function of xi. â€“ We want to find the degree of the polynomial â€˜pâ€™. â€¢ Should we choose the â€˜pâ€™ with the lowest training error? â€“ No, this will pick a â€˜pâ€™ that is too large. (training error always decreases as you increase â€˜pâ€™) http://www.cs.ubc.ca/~arnaud/stat535/slides5_revised.pdf Finding the â€œTrueâ€ Model â€¢ Consider a simple case of trying to find the â€œtrueâ€ model? â€“ We believe that yi really is a polynomial function of xi. â€“ We want to find the degree of the polynomial â€˜pâ€™. â€¢ Should we choose the â€˜pâ€™ with the lowest validation error? â€“ This will also often choose a â€˜pâ€™ that is too large (due to optimization bias). â€“ Consider a case where the true model has p=2 (quadratic function). â€¢ We fit a quadratic function yi = 2xi 2 â€“ 5, and a cubic function yi = 0.00001xi 3 + 2xi 2 - 5. â€¢ Cubic is wrong, but by chance might get a lower error on a particular validation set. â€“ If we try many models, there are more â€œchancesâ€ to randomly get a lower validation error. Complexity Penalties â€¢ There are a lot of â€œscoresâ€ people use to find the â€œtrueâ€ model. â€¢ Basic idea behind them: put a penalty on the model complexity. â€“ Want to fit the data and have a simple model. â€¢ For example, minimize training error plus the degree of polynomial. â€“ If we use p=4, use â€œtraining error plus 4â€ as error. â€¢ If two â€˜pâ€™ values have similar error, this prefers the smaller â€˜pâ€™. Choosing Degree of Polynomial Basis â€¢ How can we optimize this score? â€“ Form Z0, solve for â€˜vâ€™, compute score(0) = Â½||Z0v â€“ y||2 + 0. â€“ Form Z1, solve for â€˜vâ€™, compute score(1) = Â½||Z1v â€“ y||2 + 1. â€“ Form Z2, solve for â€˜vâ€™, compute score(2) = Â½||Z2v â€“ y||2 + 2. â€“ Form Z3, solve for â€˜vâ€™, compute score(3) = Â½||Z3v â€“ y||2 + 3. â€“ Choose the degree with the lowest score. â€¢ â€œYou need to decrease training error by at least 1 to increase degree by 1.â€ Information Criteria â€¢ There are many scores, usually with the form: â€“ The value â€˜kâ€™ is the â€œnumber of estimated parametersâ€ (â€œdegrees of freedomâ€). â€¢ For polynomial basis, we have k = (p+1). â€“ The parameter Î» > 0 controls how strong we penalize complexity. â€¢ â€œYou need to decrease the training error by least Î» to increase â€˜kâ€™ by 1â€. â€¢ Using (Î» = 1) is called Akaike information criterion (AIC). â€¢ Other choices of Î» (not necessarily integer) give other criteria: â€“ Mallowâ€™s Cp. â€“ Adjusted R2. â€“ ANOVA-based model selection. Choosing Degree of Polynomial Basis â€¢ How can we optimize this score in terms of â€˜pâ€™? â€“ Form Z0, solve for â€˜vâ€™, compute score(0) = Â½||Z0v â€“ y||2 + Î». â€“ Form Z1, solve for â€˜vâ€™, compute score(1) = Â½||Z1v â€“ y||2 + 2Î». â€“ Form Z2, solve for â€˜vâ€™, compute score(2) = Â½||Z2v â€“ y||2 + 3Î». â€“ Form Z3, solve for â€˜vâ€™, compute score(3) = Â½||Z3v â€“ y||2 + 4Î». â€“ So we need to improve by â€œat least Î»â€ to justify increasing degree. â€¢ If Î» is big, weâ€™ll choose a small degree. If Î» is small, weâ€™ll choose a large degree. Summary â€¢ Outliers in â€˜yâ€™ can cause problem for least squares. â€¢ Robust regression using L1-norm is less sensitive to outliers. â€¢ Brittle regression using Linf-norm is more sensitive to outliers. â€¢ Smooth approximations: â€“ Let us apply gradient descent to non-smooth functions. â€“ Huber loss is a smooth approximation to absolute value. â€“ Log-Sum-Exp is a smooth approximation to maximum. â€¢ Information criteria are scores that penalize number of parameters. â€“ When we want to find the â€œtrueâ€ model. â€¢ Next time: â€“ Can we find the â€œtrueâ€ features? Random Sample Consensus (RANSAC) â€¢ In computer vision, a widely-used generic framework for robust fitting is random sample consensus (RANSAC). â€¢ This is designed for the scenario where: â€“ You have a large number of outliers. â€“ Majority of points are â€œinliersâ€: itâ€™s really easy to get low error on them. https://en.wikipedia.org/wiki/Random_sample_consensus Random Sample Consensus (RANSAC) â€¢ RANSAC: â€“ Sample a small number of training examples. â€¢ Minimum number needed to fit the model. â€¢ For linear regression with 1 feature, just 2 examples. â€“ Fit the model based on the samples. â€¢ Fit a line to these 2 points. â€¢ With â€˜dâ€™ features, youâ€™ll need â€˜d+1â€™ examples. â€“ Test how many points are fit well based on the model. â€“ Repeat until we find a model that fits at least the expected number of â€œinliersâ€. â€¢ You might then re-fit based on the estimated â€œinliersâ€. https://en.wikipedia.org/wiki/Random_sample_consensus Log-Sum-Exp for Brittle Regression â€¢ To use log-sum-exp for brittle regression: Log-Sum-Exp Numerical Trick â€¢ Numerical problem with log-sum-exp is that exp(zi) might overflow. â€“ For example, exp(100) has more than 40 digits. â€¢ Implementation â€˜trickâ€™: Gradient Descent for Non-Smooth? â€¢ â€œYou are unlikely to land on a non-smooth point, so gradient descent should work for non-smooth problems?â€ â€“ Consider just trying to minimize the absolute value function: â€“ Norm(gradient) is constant when not at 0, so unless you are lucky enough to hit exactly 0, you will just bounce back and forth forever. â€“ We didnâ€™t have this problem for smooth functions, since the gradient gets smaller as you approach a minimizer. â€“ You could fix this problem by making the step-size slowly go to zero, but you need to do this carefully to make it work, and the algorithm gets much slower. Gradient Descent for Non-Smooth? â€¢ Counter-example from Bertsekasâ€™ â€œNonlinear Programmingâ€ where gradient descent for a non-smooth convex problem does not converge to a minimum.","libVersion":"0.2.1","langs":""}