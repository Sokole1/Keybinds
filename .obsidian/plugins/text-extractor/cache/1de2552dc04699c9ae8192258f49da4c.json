{"path":".obsidian/plugins/text-extractor/cache/1de2552dc04699c9ae8192258f49da4c.json","text":"MIDTERM EXAM 2 CPSC 302: Numerical Computation for Algebraic Problems November 16, 2022 TIME: 45 minutes Name: Signature: Student number: CWL ID: ‹ This is a closed-book examination. No books, notes, calculators, laptops, or any other materials or accessories may be used. ‹ Please take a moment to make sure that this exam has ﬁve pages: this cover page and four exam pages following it. ‹ If you need more space, you may use the space on the other side of the page or a booklet (provided at your request), which you will be required to hand in with your exam. ‹ Please be prepared to present a library card for identiﬁcation. ‹ GOOD LUCK! Question: 1 2 3 Total Points: 8 6 6 20 Score: CPSC 302: Numerical Computation for Algebraic Problems Midterm Exam 2 Question 1. (8 points) Determine whether the following statements are true or false. There is no need to justify your answer; just write T/F at a visible place. (a)1 pt Even if a matrix A is perfectly conditioned, i.e., κ(A) = 1, Gaussian Elimination without pivoting for solving a linear system Ax = b may still break down due to division by zero. (b)1 pt Ignoring roundoﬀ errors, if during the kth step of Gaussian elimination with partial piv- oting for solving a linear system Ax = b, i.e., during the step of zeroing the entries below the kth diagonal, a row of zeros is generated in the interim matrix A(k) (in the notation used in the lectures/textbook/slides), then the original matrix A is necessarily singular and the linear system does not have a unique solution. (c)1 pt When numerically solving a sparse linear system Ax = b using Gaussian elimination with partial pivoting, the required computational work and storage are always equivalent in big O terms to the computational work and storage required for solving the linear system by computing A−1 explicitly ﬁrst, and then computing x = A−1b. (d)1 pt For a dense symmetric positive deﬁnite matrix A, thanks to symmetry and because no pivoting is required, computing the Cholesky decomposition and using it to solve the linear system Ax = b is computationally cheaper by an order of magnitude than standard Gaussian elimination with partial pivoting – it takes only O(n2) ﬂoating point operations. (e)1 pt Suppose we numerically solve a linear system Ax = b and the computed solution is ˜x. Even if the norm of the relative residual, ∥b−A˜x∥ ∥b∥ is small, the norm of the relative error ∥x−˜x∥ ∥x∥ may be large. (f)1 pt In the solution of the linear least-squares problem minx ∥b−Ax∥2, the residual r = b−Ax is orthogonal to Ax. (g)1 pt The matrix H = I − 2uuT , with ∥u∥2 = 1, is not always invertible, i.e., one can ﬁnd a vector u with with ∥u∥2 = 1 such that H is singular. (h)1 pt The matrix G = I − uuT , where where ∥u∥2 = 1, is symmetric and it satisﬁes G2 = G. You may use the space below to write your own notes, but only your T/F answers will be marked. Page 1 of 4 CPSC 302: Numerical Computation for Algebraic Problems Midterm Exam 2 Question 2. (6 points) Consider an ‘almost tridiagonal’ n × n matrix A, whose nonzero pattern is like the nonzero pattern of a tridiagonal matrix except an,1 ̸= 0. Here is a 6 × 6 example: A =         × × 0 0 0 0 × × × 0 0 0 0 × × × 0 0 0 0 × × × 0 0 0 0 × × × × 0 0 0 × ×  | | | | | |  . (a)2 pts If no pivoting is required, what are the nonzero patterns of the factors L and U of the LU decomposition of A? Just write down the nonzero pattern for the 6 × 6 example using × and 0; there is no need to provide a justiﬁcation. (b)4 pts In answering this question, assume that n is large and you may ignore pivoting. State how much additional computational work and how much additional storage are required, if any, for solving the linear system Ax = b, compared to solving a linear system with a tridiagonal matrix. Refer speciﬁcally to the following and write your answers on the margins; there is no need to justify your answers. (i) Additional computational work for computing the LU decomposition A = LU , if any (ii) Additional computational work for solving Ly = b, if any (iii) Additional computational work for solving U x = y, if any (iv) The overall additional storage that is required in the solution process, if any Make sure to refer to the above as a comparison with a standard tridiagonal system and state only the additional computational work and storage. Page 2 of 4 CPSC 302: Numerical Computation for Algebraic Problems Midterm Exam 2 Question 3. (6 points) Consider the linear least-squares problem min x ∥b − Ax∥2, where A is a full rank m × n matrix, with m > n. Suppose we are using the normal equations, and the matrix of the linear system has been slightly perturbed, as follows: instead of solving AT Ax = AT b, we solve (AT A + E)(x + y) = AT b, where E is a matrix and y is a vector, with ∥E∥ and ∥y∥ very small. That is, we refer to (x + y) as the solution of this modiﬁed linear system. (a)2 pts Suppose Ey is so small that it can be neglected. Show that y = −(AT A) −1Ex. (b)2 pts Denote B = AT A and suppose that ∥E∥ ∥B∥ < ε and that the condition number of B satisﬁes κ(B) = ∥B∥∥B−1∥ < M. Find a constant β that depends on ε and M such that the relative error satisﬁes ∥y∥ ∥x∥ < β. Page 3 of 4 CPSC 302: Numerical Computation for Algebraic Problems Midterm Exam 2 (c)2 pts When A is ill-conditioned or does not have full rank, a common approach is to replace the normal equations AT Ax = AT b by (AT A + αI)˜x = A T b, where α > 0 is a positive scalar. There is a corresponding linear least-squares problem here, which you are not required to ﬁnd. Denote C = AT A + αI. Show that the matrix C is symmetric positive deﬁnite (and therefore Cholesky can be used). Do this in two steps, as follows. (There is no need to discuss the solution method, just show what you are being asked to show below.) (i) Show that C is symmetric (ii) Show that C is positive deﬁnite End of exam Page 4 of 4 End of exam","libVersion":"0.2.1","langs":""}