{"path":".obsidian/plugins/text-extractor/cache/eff2f9fcdf3170270de74e0766724068.json","text":"Conjugate Priors Hierarchical Bayes CPSC 440: Advanced Machine Learning Conjugate Priors Mark Schmidt University of British Columbia Winter 2021 Conjugate Priors Hierarchical Bayes Last Time: Bayesian Predictions and Empirical Bayes We’ve discussed making predictions using posterior predictive, ˆy ∈ argmax ˜y ∫ w p(˜y | ˜x, w)p(w | X, y, λ)dw, which gives optimal predictions given your assumptions. We considered empirical Bayes (type II MLE), ˆλ ∈ argmax λ p(y | X, λ), where p(y | X, λ) = ∫ w p(y | X, w)p(w | λ)dw, where we optimize marginal likelihood to select model and/or hyper-parameters. Allows a huge number of hyper-parameters with less over-ﬁtting than MLE. Can use gradient descent to optimize continuous hyper-parameters. Ratio of marginal likelihoods (Bayes factor) can be used for hypothesis testing. In many settings, naturally encourages sparsity (in parameters, data, clusters, etc.). Conjugate Priors Hierarchical Bayes Beta-Bernoulli Model Consider again a coin-ﬂipping example with a Bernoulli variable, x ∼ Ber(θ). Previously we considered that either θ = 1 or θ = 0.5. Today: θ is a continuous variable coming from a beta distribution, θ ∼ B(α, β). The parameters α and β of the prior are called hyper-parameters. Similar to λ in regression, α and β are parameters of the prior. Conjugate Priors Hierarchical Bayes Beta-Bernoulli Prior Why the beta as a prior distribution? “It’s a ﬂexible distribution that includes uniform as special case”. “It makes the integrals easy”. https://en.wikipedia.org/wiki/Beta_distribution Uniform distribution if α = 1 and β = 1. “Laplace smoothing” corresponds to MAP with α = 2 and β = 2. Biased towards 0.5. Conjugate Priors Hierarchical Bayes Beta-Bernoulli Posterior The PDF for the beta distribution has similar form to Bernoulli, θ | α, β ∼ B(α, β) ⇒ p(θ | α, β) ∝ θα−1(1 − θ) β−1. Observing HTH under Bernoulli likelihood and beta prior gives posterior of p(θ | HT H, α, β) ∝ p(HT H | θ, α, β)p(θ | α, β) ∝ (θ2(1 − θ) 1θα−1(1 − θ)β−1) = θ(2+α)−1(1 − θ)(1+β)−1. Since proportionality (∝) constant is unique for probabilities, posterior is a beta: θ | HT H, α, β ∼ B(2 + α, 1 + β). When the prior and posterior come from same family, it’s called a conjugate prior. Conjugate Priors Hierarchical Bayes Conjugate Priors Conjugate priors make Bayesian inference easier: 1 Posterior is prior with “updated” parameters. For Bernoulli-beta, if we observe h heads and t tails then posterior is B(α + h, β + t). Hyper-parameters α and β are “pseudo-counts” in our mind before we ﬂip. 2 We can update posterior sequentially as data comes in. For Bernoulli-beta, just update counts h and t. Conjugate Priors Hierarchical Bayes Conjugate Priors Conjugate priors make Bayesian inference easier: 3 Marginal likelihood has closed-form, proportional to ratio of normalizing constants. The beta distribution is written in terms of the beta function B, p(θ | α, β) = 1 B(α, β) θα−1(1 − θ) β−1, where B(α, β) = ∫ θ θα−1(1 − θ)β−1dθ. and using the form of the posterior the marginal likelihood p(HT H | α, β) = ∫ θ 1 B(α, β) θ(h+α)−1(1 − θ)(t+β)−1dθ = B(h + α, t + β) B(α, β) . Empirical Bayes (type II MLE) would optimize this in terms of α and β. 4 In many cases posterior predictive also has a nice form... Conjugate Priors Hierarchical Bayes Bernoulli-Beta Posterior Predictive If we observe ‘HHH’ then our diﬀerent estimates are: MAP with uniform Beta(1,1) prior (maximum likelihood), ˆθ = (3 + α) − 1 (3 + α) + β − 2 = 3 3 = 1. MAP Beta(2,2) prior (Laplace smoothing), ˆθ = (3 + α) − 1 (3 + α) + β − 2 = 4 5 . Conjugate Priors Hierarchical Bayes Bernoulli-Beta Posterior Predictive If we observe ‘HHH’ then our diﬀerent estimates are: Posterior predictive (Bayesian) with uniform Beta(1,1) prior, p(H | HHH) = ∫ 1 0 p(H | θ)p(θ | HHH)dθ = ∫ 1 0 Ber(H | θ)Beta(θ | 3 + α, β)dθ = ∫ 1 0 θBeta(θ | 3 + α, β)dθ = E[θ] = 4 5 . (mean of beta is α/(α + β)) Notice Laplace smoothing is not needed to avoid degeneracy under uniform prior. Here we had a “pseudo-count” of 1 head and 1 tail before we did our 3 ﬂips. Conjugate Priors Hierarchical Bayes Eﬀect of Prior and Improper Priors We obtain diﬀerent predictions under diﬀerent priors: B(3, 3) prior is like seeing 3 heads and 3 tails (stronger prior towards 0.5), For HHH, posterior predictive is 0.667. B(100, 1) prior is like seeing 100 heads and 1 tail (biased), For HHH, posterior predictive is 0.990. B(.01, .01) biases towards having unfair coin (head or tail), For HHH, posterior predictive is 0.997. Called “improper” prior (does not integrate to 1), but posterior can be “proper”. We might hope to use an uninformative prior to not bias results. But this is often hard/ambiguous/impossible to do (bonus slide). Conjugate Priors Hierarchical Bayes Back to Conjugate Priors Basic idea of conjugate priors for abstract prior D and likelihood P : x ∼ D(θ), θ ∼ P (λ) ⇒ θ | x ∼ P (λ ′). Beta-bernoulli example (beta is also conjugate for binomial and geometric): x ∼ Ber(θ), θ ∼ B(α, β), ⇒ θ | x ∼ B(α′, β′), Gaussian-Gaussian example: x ∼ N (µ, Σ), µ ∼ N (µ0, Σ0), ⇒ µ | x ∼ N (µ′, Σ ′), and posterior predictive is also a Gaussian. If Σ is also a random variable: Conjugate prior is normal-inverse-Wishart, posterior predictive is a student t. For the conjugate priors of many standard distributions, see: https://en.wikipedia.org/wiki/Conjugate_prior#Table_of_conjugate_distributions Conjugate Priors Hierarchical Bayes Back to Conjugate Priors Conjugate priors make things easy because we have closed-form posterior. Some “non-named” conjugate priors: Discrete priors are “conjugate” to all likelihoods: Posterior will be discrete, although it still might be NP-hard to use. Mixtures of conjugate priors are also conjugate priors. Posterior will have simple form, though again some comptuations may be diﬃcult. Do conjugate priors always exist? No, they only exist for exponential family likelihoods (next slides). Bayesian inference is ugly when you leave exponential family (e.g., student t). Can use numerical integration for low-dimensional integrals. For high-dimensional integrals, need Monte Carlo methods or variational inference. Conjugate Priors Hierarchical Bayes Digression: Exponential Family Exponential family distributions can be written in the form p(x | w) ∝ h(x) exp(wT F (x)). We often have h(x) = 1, or an indicator that x satisﬁes constraints. F (x) is called the suﬃcient statistics. F (x) tells us everything that is relevant about data x. If F (x) = x, we say that the w are cannonical parameters. Exponential family distributions can be derived from maximum entropy principle. Distribution that is “most random” that agrees with the suﬃcient statistics F (x). Argument is based on “convex conjugate” of − log p. Conjugate Priors Hierarchical Bayes Digression: Bernoulli Distribution as Exponential Family We often deﬁne linear models by setting wT xi equal to cannonical parameters. If we start with the Gaussian (ﬁxed variance), we obtain least squares. For Bernoulli, the cannonical parameterization is in terms of “log-odds”, p(x | θ) = θx(1 − θ) 1−x = exp(log(θx(1 − θ)1−x)) = exp(x log θ + (1 − x) log(1 − θ)) ∝ exp ( x log ( θ 1 − θ )) . Setting wT xi = log(yi/(1 − yi)) and solving for yi yields logistic regression. You can obtain regression models for other settings using this approach. Conjugate Priors Hierarchical Bayes Conjugate Graphical Models DAG computations simplify if parents are conjugate to children. Examples: Bernoulli child with Beta parent. Gaussian belief networks. Discrete DAG models. Hybrid Gaussian/discrete, where discrete nodes can’t have Gaussian parents. Gaussian graphical model with normal-inverse-Wishart parents. Conjugate Priors Hierarchical Bayes Outline 1 Conjugate Priors 2 Hierarchical Bayes Conjugate Priors Hierarchical Bayes Hierarchical Bayesian Models Type II maximum likelihood is not really Bayesian: We’re dealing with w using the rules of probability. But we’re treating λ as a parameter, not a nuissance variable. You could overﬁt λ. Hierarchical Bayesian models introduce a hyper-prior p(λ | γ). We can be “very Bayesian” and treat the hyper-parameter as a nuissance parameter. Now use Bayesian inference for dealing with λ: Work with posterior over λ, p(λ | X, y, γ), if integral over w is easy. Or work with posterior over w and λ. You could also consider a Bayes factor for comparing λ values: p(λ1 | X, y, γ)/p(λ2 | X, y, γ), which now account for belief in diﬀerent hyper-parameter settings. Conjugate Priors Hierarchical Bayes Model Selection and Averaging: Hyper-Parameters as Variables Bayesian model selection (“type II MAP”): maximizes hyper-parameter posterior, ˆλ = argmax λ p(λ | X, y, γ) = argmax λ p(y | X, λ)p(λ | γ), further taking us away from overﬁtting (thus allowing more complex models). We could do the same thing to choose order of polynomial basis, σ in RBFs, etc. Bayesian model averaging considers posterior predictive over hyper-parameters, ˆyi = argmax ˆy ∫ λ ∫ w p(ˆy | ˆxi, w)p(w, λ | X, y, γ)dwdλ. Could maximize marginal likelihood of hyper-hyper-parameter γ, (“type III ML”), ˆγ = argmax γ p(y | X, γ) = argmax γ ∫ λ ∫ w p(y | X, w)p(w | λ)p(λ | γ)dwdλ. Conjugate Priors Hierarchical Bayes Application: Automated Statistician Hierarchical Bayes approach to regression: 1 Put a hyper-prior over possible hyper-parameters. 2 Use type II MAP to optimize hyper-parameters of your regression model. Can be viewed as an automatic statistician: http://www.automaticstatistician.com/examples Conjugate Priors Hierarchical Bayes Discussion of Hierarchical Bayes “Super Bayesian” approach: Go up the hierarchy until model includes all assumptions about the world. Some people try to do this, and have argued that this may be how humans reason. Key advantage: Mathematically simple to know what to do as you go up the hierarchy: Same math for w, z, λ, γ, and so on (all are nuissance parameters). Key disadvantages: It can be hard to exactly encode your prior beliefs. The integrals get ugly very quickly. Conjugate Priors Hierarchical Bayes Summary Conjugate priors are priors that lead to posteriors of the same form. They make Bayesian inference much easier. Posterior is prior distribution with “updated” parameters. Marginal likelihood proportional to ratio of normalizing constants. Exponential family distributions are the only distributions with conjugate priors. Most standard distributions are exponential family, or integral of exponential family. Hierarchical Bayes goes even more Bayesian with prior on hyper-parameters. Leads to Bayesian model selection and Bayesian model averaging. Next time: modeling cancer mutation signatures. Conjugate Priors Hierarchical Bayes Uninformative Priors and Jeﬀreys Prior We might want to use an uninformative prior to not bias results. But this is often hard/impossible to do. We might think the uniform distribution, B(1, 1), is uninformative. But posterior will be biased towards 0.5 compared to MLE. And if you re-parameterize distribution it won’t stay uniform. We might think to use “pseudo-count” of 0, B(0, 0), as uninformative. But posterior isn’t a probability until we see at least one head and one tail. Some argue that the “correct” uninformative prior is B(0.5, 0.5). This prior is invariant to the parameterization, which is called a Jeﬀreys prior.","libVersion":"0.2.1","langs":""}