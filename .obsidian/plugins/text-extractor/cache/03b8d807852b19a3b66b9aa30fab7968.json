{"path":".obsidian/plugins/text-extractor/cache/03b8d807852b19a3b66b9aa30fab7968.json","text":"In this dataset, we have 2 features and each colour represents one of the classes. Note that the classes are highly structured: the colours each roughly follow a Gausian distribution plus some noisy samples. Since we have an idea of what the features look like for each class, we might consider classifying inputs = using a generative classifier. In particular, we are going to use Bayes rule to write p(zly=¢0) ply=c|O) =c|lz,@) =\" \"2 \"2 1 77 p(y=c|z,0) 2@ 0) , where © represents the parameters of our model. To classify a new example Z, we get §= argmax p(y=c|#6) = argmax p(y=c| O)p( |y=c,6)= argmax TN (F | pe,To), ce{1,2,....,k} c€{1,2,....k} ce{1,2,....,k} where we’ve assumed that y | © ~ Categorical(m,...,mt), and z ~ (y = ¢) ~ N (¢, X¢); the notation N(z | p, E) means the density of N'(p, X) evaluated at z. Recall that the maximum likelihood estimate for 7, is simply n./n, where n, = S 1(y@ = ¢) is the number of data points in class c¢. Given a dataset (X,y), the MLE for the mean and covarance parameters is then n arg max Hwy(,> N(z® | Py Zy)) [TENSTYS SN > sl n T A = arg minz (log |2y(,) ‘ + (z(l) - \"‘y“)) 2;(}) (z(z) — ;Lym)> i=1 k T = arg minz Z (log || + (z(’) - uc) =t (1(1) - ;,zc)) . e=1iyl=c In class, we discussed the general GDA/QDA model, which allows any value for the p. and 3. The objective above then separates for the different ¢, and the MLE becomes just the per-class MLE. We also discussed linear discriminant analysis (LDA), which enforces that all the covariance matrices agree, 3. = X. In this case the classifier becomes linear, with 3 corresponding to a “pooled covariance.”","libVersion":"0.2.1","langs":"eng"}