{"path":".obsidian/plugins/text-extractor/cache/55f786728a33eb1241e8865c848a3ceb.json","text":"CPSC 340: Machine Learning and Data Mining Kernel Trick Motivation: Automatic Brain Tumor Segmentation â€¢ Task: labeling tumors and normal tissue in multi-modal MRI data. â€¢ Applications: â€“ Radiation therapy target planning, quantifying treatment responses. â€“ Mining growth patterns, image-guided surgery. â€¢ Challenges: â€“ Variety of tumor appearances, similarity to normal tissue. â€“ Grumbly scientist to me in 2003: â€œyou are never going to solve this problem.â€ Input: Output: Motivation: Automatic Brain Tumor Segmentation â€¢ Convolutions (later) can be used to engineer good features. â€¢ Good performance was obtained with linear classifiers (SVMs/logistic). â€“ Provided you did feature selection or used regularization. â€¢ One of the only methods that worked better: â€“ Regularized linear classifier with a low-order polynomial basis (p=2 or p=3). â€¢ Makes the data â€œcloser to separableâ€ in the higher-dimensional space. Input: Output: Support Vector Machines for Non-Separable â€¢ Can we use linear models for data that is not close to separable? http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable â€¢ Can we use linear models for data that is not close to separable? â€“ It may be separable under non-linear transform (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable â€¢ Can we use linear models for data that is not close to separable? â€“ It may be separable under change of basis (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable â€¢ Can we use linear models for data that is not close to separable? â€“ It may be separable under change of basis (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Multi-Dimensional Polynomial Basis â€¢ Recall fitting polynomials when we only have 1 feature: â€¢ We can fit these models using a change of basis: â€¢ How can we do this when we have a lot of features? Multi-Dimensional Polynomial Basis â€¢ Polynomial basis for d=2 and p=2: â€¢ With d=4 and p=3, the polynomial basis would include: â€“ Bias variable and the xij: 1, xi1, xi2, xi3, xi4. â€“ The xij squared and cubed: (xi1)2, (xi2)2, (xi3)2, (xi4)2, (xi1)3, (xi2)3, (xi3)3, (xi4)3. â€“ Two-term interactions: xi1xi2, xi1xi3, xi1xi4, xi2xi3, xi2xi4, xi3xi4. â€“ Cubic interactions: xi1xi2xi3, xi2xi3xi4, xi1xi3,xi4, xi1xi2xi4, xi1 2xi2, xi1 2xi3, xi1 2xi4, xi1xi2 2, xi2 2xi3, xi2 2xi4, xi1xi3 2, xi2xi3 2,xi3 2xi4, xi1xi4 2, xi2xi4 2, xi3xi4 2. Multi-Dimensional Polynomial Basis â€¢ If we go to degree p=5, weâ€™ll have O(d5) quintic terms: â€¢ For large â€˜dâ€™ and â€˜pâ€™, storing a polynomial basis is intractable! â€“ â€˜Zâ€™ has k=O(dp) columns, so it does not fit in memory. â€¢ Could try to search for a good subset of these. â€“ â€œHierarchical forward selectionâ€ (bonus). â€¢ Alternating, you can use all of them with the â€œkernel trickâ€. â€“ For special case of L2-regularized linear models. How can you use an exponential-sized basis? â€¢ Which of these two expressions would you rather compute? â€“ Expressions are equal, but left way costs O(p) while right costs O(1). â€¢ Which of these two expressions would you rather compute? â€“ Expressions are equal, but left way has infinite terms and right costs O(1). â€¢ Can we add weights to the terms in sum, and use these tricks? Kernel Trick â€“ Big Picture â€¢ Many people find the kernel trick confusing. â€“ But the core ideas are simple. â€¢ Motivation for kernel trick: â€“ Want to transform feature xi to features zi that are very high-dimensional. â€¢ Mechanics of kernel trick: â€“ Write training and prediction to only depend on zi Tzj for different â€˜iâ€™ and â€˜jâ€™. â€¢ Need to avoid any other operations involving xi or zi. â€“ Replace all instances of zi Tzj with the cheap â€œkernelâ€ function k(xi, xj). â€¢ Which gives you cheap training and prediction, with very high-dimensional features. Digression: The â€œOtherâ€ Normal Equations â€¢ Recall the L2-regularized least squares objective with basis â€˜Zâ€™: â€¢ We showed that the minimum is given by (in practice you still solve the linear system, since inverse is less numerically unstable â€“ see CPSC 302) â€¢ With some work (bonus slide), this can equivalently be written as: â€¢ This is faster if n << k: â€“ After forming â€˜Zâ€™, cost is O(n2k + n3) instead of O(nk2 + k3). â€“ But for the polynomial basis, this is still too slow since k = O(dp). Kernel Trick for the â€œOtherâ€ Normal Equations â€¢ With the â€œotherâ€ normal equations we have â€¢ Given test data à·¨ğ‘‹, predict à·œğ‘¦ by forming à·¨ğ‘ and then using: â€¢ Notice that if you can from K and à·¨ğ¾ then you do not need Z and à·¨ğ‘. â€¢ Key idea behind â€œkernel trickâ€ for certain bases (like polynomials): â€“ We can efficiently compute K and à·©ğ¾ even though forming Z and à·¨ğ‘ is intractable. â€¢ In the same way we can comptue (x+1)9 instead of x9 + 9x8 + 36x7 + 84x6â€¦ Gram Matrix â€¢ The matrix K = ZZT is called the Gram matrix K. â€¢ K contains the dot products between all training examples. â€“ Similar to â€˜Zâ€™ in RBFs, but using dot product as â€œsimilarityâ€ instead of distance. Gram Matrix â€¢ The matrix à·©ğ¾ = à·¨ğ‘ZT has dot products between train and test examples: â€¢ Kernel function: k(xi, xj) = ziTzj. â€“ Computes dot product in basis (ziTzj) using original features xi and xj. The Kernel TrickThe Kernel TrickDegenerate Example: â€œLinear Kernelâ€ â€¢ Consider two examples xi and xj for a 2-dimensional dataset: â€¢ As an example kernel, the â€œlinear kernelâ€ just uses original features: â€¢ In this case the inner product zi Tzj is k(xi,xj) = xi Txj: â€“ But in this case model is still a linear function of original features. Example: Degree-2 Kernel â€¢ Consider two examples xi and xj for a 2-dimensional dataset: â€¢ Now consider a particular degree-2 basis: â€¢ In this case the inner product zi Tzj is k(xi,xj) = (xi Txj)2: Polynomial Kernel with Higher Degrees â€¢ Letâ€™s add a bias and linear terms to our degree-2 basis: â€¢ In this case the inner product zi Tzj is k(xi,xj) = (1 + xi Txj)2: Polynomial Kernel with Higher Degrees â€¢ To get all degree-4 â€œmonomialsâ€ I can use: â€¢ To also get lower-order terms use k(xi,xj) = (1 + xi Txj)4 â€¢ The general degree-p polynomial kernel function: â€“ Works for any number of features â€˜dâ€™. â€“ But cost of computing one k(xi,xj) is O(d) instead of O(dp) to compute zi Tzj. â€“ Take-home message: I can compute dot-products without the features. Kernel Trick with Polynomials â€¢ Using polynomial basis of degree â€˜pâ€™ with the kernel trick: â€“ Compute K and à·©ğ¾ using: â€“ Make predictions using: â€¢ Training cost is only O(n2d + n3), despite using k=O(dp) features. â€“ We can form â€˜Kâ€™ in O(n2d), and we need to â€œinvertâ€ an â€˜n x nâ€™ matrix. â€“ Testing cost is only O(ndt), cost to form à·©ğ¾. Linear Regression vs. Kernel RegressionAn Infinite-Dimensional Basis? â€¢ Suppose d=1 and I want to use this infinite set of features (d = âˆ): â€¢ The kernel function has a simple form: â€¢ For these features, even though d=âˆ, cost of kernel is O(1). Gaussian-RBF Kernel â€¢ Previous slide is a special case of the Gaussian RBF kernel: â€“ Where we have introduced a variance hyper-parameter ğœ2. â€“ This is the most popular kernel function. â€¢ Same formula as Gaussian RBF features, but not equivalent: â€“ Before we used Gaussian RBFs as a set of â€˜nâ€™ features. â€“ Now we are using Gaussian RBFs as a dot product (for infinite features). â€¢ In practice, Gaussian RBFs as features or as kernels gives similar performance. Motivation: Finding Gold â€¢ Kernel methods first came from mining engineering (â€œKrigingâ€): â€“ Mining company wants to find gold. â€“ Drill holes, measure gold content. â€“ Build a kernel regression model (typically use RBF kernels). http://www.bisolutions.us/A-Brief-Introduction-to-Spatial-Interpolation.php Kernel Trick for Non-Vector Data â€¢ Consider data that doesnâ€™t look like this: â€¢ But instead looks like this: â€¢ We can interpret k(xi,xj) as a â€œsimilarityâ€ between objects xi and xj. â€“ We donâ€™t need features if we can compute â€œsimilarityâ€ between objects. â€“ Kernel trick lets us fit regression models without explicit features. â€“ There are â€œstring kernelsâ€, â€œimage kernelsâ€, â€œgraph kernelsâ€, and so on. Kernel Trick for Non-Vector Data â€¢ Recent list of types of data where people have defined kernels: â€¢ Bonus slide overviews a particular â€œstringâ€ kernel. https://arxiv.org/pdf/1802.04784.pdf Valid Kernels â€¢ What kernel functions k(xi,xj) can we use? â€¢ Kernel â€˜kâ€™ must be an inner product in some space: â€“ There must exist a mapping from the xi to some zi such that k(xi,xj) = zi Tzj. â€¢ It can be hard to show that a function satisfies this. â€“ Infinite-dimensional eigenfunction problem. â€¢ But like convex functions, there are some simple rules for constructing â€œvalidâ€ kernels from other valid kernels (bonus slide). Kernel Trick for Other Methods â€¢ Besides L2-regularized least squares, when can we use kernels? â€“ We can compute Euclidean distance with kernels: â€“ All of our distance-based methods have kernel versions: â€¢ Kernel k-nearest neighbours. â€¢ Kernel clustering k-means (allows non-convex clusters) â€¢ Kernel density-based clustering. â€¢ Kernel hierarchical clustering. â€¢ Kernel distance-based outlier detection. â€¢ Kernel â€œAmazon Product Recommendationâ€. Kernel Trick for Other Methods â€¢ Besides L2-regularized least squares, when can we use kernels? â€“ â€œRepresenter theoremsâ€ (bonus slide) have shown that any L2-regularized linear model can be kernelized (see bonus): â€¢ Kernel robust regression with L2-regularization. â€¢ Kernel brittle regression with L2-regularization. â€¢ Kernel hinge loss (SVM) or logistic loss with L2-regularization. â€¢ Kernel multi-class SVM or multi-class logistic with L2-regularization. Logistic Regression with KernelsSummary â€¢ Common convolution filters for computer vision: â€“ Gaussian, Laplacian of Gaussian, and Gabor filters. â€¢ Filter banks: make features by taking a bunch of convolutions. â€¢ High-dimensional bases allows us to separate non-separable data. â€¢ â€œOtherâ€ normal equations are faster when n < d. â€¢ Kernel trick allows us to use high-dimensional bases efficiently. â€“ Write model to only depend on inner products between features vectors. â€¢ Kernels let us use similarity between objects, rather than features. â€“ Allows some exponential- or infinite-sized feature sets. â€“ Applies to distance-based and linear models with L2-regularization. â€¢ Next time: â€“ How do we train on all of Gmail? Image Convolution ExamplesImage Coordinates â€¢ Should we use the image coordinates? â€“ E.g., the pixel is at location (124, 78) in the image. â€¢ Considerations: â€“ Is the interpretation different in different areas of the image? â€“ Are you using a linear model? â€¢ Would â€œdistance to centerâ€ be more logical? â€“ Do you have enough data to learn about all areas of the image? Alignment-Based Features â€¢ The position in the image is important in brain tumour application. â€“ But we didnâ€™t have much data, so coordinates didnâ€™t make sense. â€¢ We aligned the images with a â€œtemplate imageâ€. Alignment-Based Features â€¢ The position in the image is important in brain tumour application. â€“ But we didnâ€™t have much data, so coordinates didnâ€™t make sense. â€¢ We aligned the images with a â€œtemplate imageâ€. â€“ Allowed â€œalignment-basedâ€ features: Motivation: Automatic Brain Tumor Segmentation â€¢ Final features for brain tumour segmentation: â€“ Gaussian convolution of original/template/priors/symmetry, Laplacian of Gaussian on original. â€¢ All with 3 variances. â€¢ Max(Gabor) with sine and cosine on orginal (3 variances). Motivation: Automatic Brain Tumour Segmentation â€¢ Logistic regression and SVMs among best methods. â€“ When using these 72 features from last slide. â€“ If you used all features I came up with, it overfit. â€¢ Possible solutions to overfitting: â€“ Forward selection was too slow. â€¢ Just one image gives 8 million training examples. â€“ I did manual feature selection (â€œguess and checkâ€). â€“ L2-regularization with all features also worked. â€¢ But this is slow at test time. â€¢ L1-regularization gives best of regularization and feature selection. SIFT Features â€¢ Scale-invariant feature transform (SIFT): â€“ Features used for object detection (â€œis particular object in the imageâ€?) â€“ Designed to detect unique visual features of objects at multiple scales. â€“ Proven useful for a variety of object detection tasks. http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html Why is inner product a similarity? â€¢ It seems weird to think of the inner-product as a similarity. â€¢ But consider this decomposition of squared Euclidean distance: â€¢ If all training examples have the same norm, then minimizing Euclidean distance is equivalent to maximizing inner product. â€“ So â€œhigh similarityâ€ according to inner product is like â€œsmall Euclidean distanceâ€. â€“ The only difference is that the inner product is biased by the norms of the training examples. â€“ Some people explicitly normalize the xi by setting xi = (1/||xi||)xi, so that inner products act like the negation of Euclidean distances. â€¢ E.g., Amazon product recommendation. A String Kernel â€¢ A classic â€œstring kernelâ€: â€“ We want to compute k(â€œcatâ€, â€œcartâ€). â€“ Find all common subsequences: â€˜câ€™, â€˜aâ€™, â€˜tâ€™, â€˜caâ€™, â€˜atâ€™, â€˜ctâ€™, â€˜catâ€™. â€“ Weight them by total length in original strings: â€¢ â€˜câ€™ has length (1,1), â€˜caâ€™ has lengths (2,2), â€˜ctâ€™ has lengths (3,4), and so on. â€“ Add up the weighted lengths of common subsequences to get a similarity: where Î³ is a hyper-parameter controlling influence of length. â€¢ Corresponds to exponential feature set (counts/lengths of all subsequences). â€“ But kernel can be computed in polynomial time by dynamic programming. â€¢ Many variations exist. Kernel Trick for Other Methods â€¢ Besides L2-regularized least squares, when can we use kernels? â€“ â€œRepresenter theorems have shown that any L2-regularized linear model can be kernelized: Kernel Trick for Other Methods â€¢ Besides L2-regularized least squares, when can we use kernels? â€“ â€œRepresenter theoremsâ€ have shown that any L2-regularized linear model can be kernelized. â€“ Linear models without regularization fit with gradient descent. â€¢ If you starting at v=0 or with any other value in span of rows of â€˜Zâ€™.","libVersion":"0.2.1","langs":""}