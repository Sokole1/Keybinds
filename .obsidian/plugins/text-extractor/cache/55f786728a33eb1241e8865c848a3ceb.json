{"path":".obsidian/plugins/text-extractor/cache/55f786728a33eb1241e8865c848a3ceb.json","text":"CPSC 340: Machine Learning and Data Mining Kernel Trick Motivation: Automatic Brain Tumor Segmentation • Task: labeling tumors and normal tissue in multi-modal MRI data. • Applications: – Radiation therapy target planning, quantifying treatment responses. – Mining growth patterns, image-guided surgery. • Challenges: – Variety of tumor appearances, similarity to normal tissue. – Grumbly scientist to me in 2003: “you are never going to solve this problem.” Input: Output: Motivation: Automatic Brain Tumor Segmentation • Convolutions (later) can be used to engineer good features. • Good performance was obtained with linear classifiers (SVMs/logistic). – Provided you did feature selection or used regularization. • One of the only methods that worked better: – Regularized linear classifier with a low-order polynomial basis (p=2 or p=3). • Makes the data “closer to separable” in the higher-dimensional space. Input: Output: Support Vector Machines for Non-Separable • Can we use linear models for data that is not close to separable? http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable • Can we use linear models for data that is not close to separable? – It may be separable under non-linear transform (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable • Can we use linear models for data that is not close to separable? – It may be separable under change of basis (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Support Vector Machines for Non-Separable • Can we use linear models for data that is not close to separable? – It may be separable under change of basis (or closer to separable). http://math.stackexchange.com/questions/353607/how-do-inner-product-space-determine-half-planes Multi-Dimensional Polynomial Basis • Recall fitting polynomials when we only have 1 feature: • We can fit these models using a change of basis: • How can we do this when we have a lot of features? Multi-Dimensional Polynomial Basis • Polynomial basis for d=2 and p=2: • With d=4 and p=3, the polynomial basis would include: – Bias variable and the xij: 1, xi1, xi2, xi3, xi4. – The xij squared and cubed: (xi1)2, (xi2)2, (xi3)2, (xi4)2, (xi1)3, (xi2)3, (xi3)3, (xi4)3. – Two-term interactions: xi1xi2, xi1xi3, xi1xi4, xi2xi3, xi2xi4, xi3xi4. – Cubic interactions: xi1xi2xi3, xi2xi3xi4, xi1xi3,xi4, xi1xi2xi4, xi1 2xi2, xi1 2xi3, xi1 2xi4, xi1xi2 2, xi2 2xi3, xi2 2xi4, xi1xi3 2, xi2xi3 2,xi3 2xi4, xi1xi4 2, xi2xi4 2, xi3xi4 2. Multi-Dimensional Polynomial Basis • If we go to degree p=5, we’ll have O(d5) quintic terms: • For large ‘d’ and ‘p’, storing a polynomial basis is intractable! – ‘Z’ has k=O(dp) columns, so it does not fit in memory. • Could try to search for a good subset of these. – “Hierarchical forward selection” (bonus). • Alternating, you can use all of them with the “kernel trick”. – For special case of L2-regularized linear models. How can you use an exponential-sized basis? • Which of these two expressions would you rather compute? – Expressions are equal, but left way costs O(p) while right costs O(1). • Which of these two expressions would you rather compute? – Expressions are equal, but left way has infinite terms and right costs O(1). • Can we add weights to the terms in sum, and use these tricks? Kernel Trick – Big Picture • Many people find the kernel trick confusing. – But the core ideas are simple. • Motivation for kernel trick: – Want to transform feature xi to features zi that are very high-dimensional. • Mechanics of kernel trick: – Write training and prediction to only depend on zi Tzj for different ‘i’ and ‘j’. • Need to avoid any other operations involving xi or zi. – Replace all instances of zi Tzj with the cheap “kernel” function k(xi, xj). • Which gives you cheap training and prediction, with very high-dimensional features. Digression: The “Other” Normal Equations • Recall the L2-regularized least squares objective with basis ‘Z’: • We showed that the minimum is given by (in practice you still solve the linear system, since inverse is less numerically unstable – see CPSC 302) • With some work (bonus slide), this can equivalently be written as: • This is faster if n << k: – After forming ‘Z’, cost is O(n2k + n3) instead of O(nk2 + k3). – But for the polynomial basis, this is still too slow since k = O(dp). Kernel Trick for the “Other” Normal Equations • With the “other” normal equations we have • Given test data ෨𝑋, predict ො𝑦 by forming ෨𝑍 and then using: • Notice that if you can from K and ෨𝐾 then you do not need Z and ෨𝑍. • Key idea behind “kernel trick” for certain bases (like polynomials): – We can efficiently compute K and ෩𝐾 even though forming Z and ෨𝑍 is intractable. • In the same way we can comptue (x+1)9 instead of x9 + 9x8 + 36x7 + 84x6… Gram Matrix • The matrix K = ZZT is called the Gram matrix K. • K contains the dot products between all training examples. – Similar to ‘Z’ in RBFs, but using dot product as “similarity” instead of distance. Gram Matrix • The matrix ෩𝐾 = ෨𝑍ZT has dot products between train and test examples: • Kernel function: k(xi, xj) = ziTzj. – Computes dot product in basis (ziTzj) using original features xi and xj. The Kernel TrickThe Kernel TrickDegenerate Example: “Linear Kernel” • Consider two examples xi and xj for a 2-dimensional dataset: • As an example kernel, the “linear kernel” just uses original features: • In this case the inner product zi Tzj is k(xi,xj) = xi Txj: – But in this case model is still a linear function of original features. Example: Degree-2 Kernel • Consider two examples xi and xj for a 2-dimensional dataset: • Now consider a particular degree-2 basis: • In this case the inner product zi Tzj is k(xi,xj) = (xi Txj)2: Polynomial Kernel with Higher Degrees • Let’s add a bias and linear terms to our degree-2 basis: • In this case the inner product zi Tzj is k(xi,xj) = (1 + xi Txj)2: Polynomial Kernel with Higher Degrees • To get all degree-4 “monomials” I can use: • To also get lower-order terms use k(xi,xj) = (1 + xi Txj)4 • The general degree-p polynomial kernel function: – Works for any number of features ‘d’. – But cost of computing one k(xi,xj) is O(d) instead of O(dp) to compute zi Tzj. – Take-home message: I can compute dot-products without the features. Kernel Trick with Polynomials • Using polynomial basis of degree ‘p’ with the kernel trick: – Compute K and ෩𝐾 using: – Make predictions using: • Training cost is only O(n2d + n3), despite using k=O(dp) features. – We can form ‘K’ in O(n2d), and we need to “invert” an ‘n x n’ matrix. – Testing cost is only O(ndt), cost to form ෩𝐾. Linear Regression vs. Kernel RegressionAn Infinite-Dimensional Basis? • Suppose d=1 and I want to use this infinite set of features (d = ∞): • The kernel function has a simple form: • For these features, even though d=∞, cost of kernel is O(1). Gaussian-RBF Kernel • Previous slide is a special case of the Gaussian RBF kernel: – Where we have introduced a variance hyper-parameter 𝜎2. – This is the most popular kernel function. • Same formula as Gaussian RBF features, but not equivalent: – Before we used Gaussian RBFs as a set of ‘n’ features. – Now we are using Gaussian RBFs as a dot product (for infinite features). • In practice, Gaussian RBFs as features or as kernels gives similar performance. Motivation: Finding Gold • Kernel methods first came from mining engineering (“Kriging”): – Mining company wants to find gold. – Drill holes, measure gold content. – Build a kernel regression model (typically use RBF kernels). http://www.bisolutions.us/A-Brief-Introduction-to-Spatial-Interpolation.php Kernel Trick for Non-Vector Data • Consider data that doesn’t look like this: • But instead looks like this: • We can interpret k(xi,xj) as a “similarity” between objects xi and xj. – We don’t need features if we can compute “similarity” between objects. – Kernel trick lets us fit regression models without explicit features. – There are “string kernels”, “image kernels”, “graph kernels”, and so on. Kernel Trick for Non-Vector Data • Recent list of types of data where people have defined kernels: • Bonus slide overviews a particular “string” kernel. https://arxiv.org/pdf/1802.04784.pdf Valid Kernels • What kernel functions k(xi,xj) can we use? • Kernel ‘k’ must be an inner product in some space: – There must exist a mapping from the xi to some zi such that k(xi,xj) = zi Tzj. • It can be hard to show that a function satisfies this. – Infinite-dimensional eigenfunction problem. • But like convex functions, there are some simple rules for constructing “valid” kernels from other valid kernels (bonus slide). Kernel Trick for Other Methods • Besides L2-regularized least squares, when can we use kernels? – We can compute Euclidean distance with kernels: – All of our distance-based methods have kernel versions: • Kernel k-nearest neighbours. • Kernel clustering k-means (allows non-convex clusters) • Kernel density-based clustering. • Kernel hierarchical clustering. • Kernel distance-based outlier detection. • Kernel “Amazon Product Recommendation”. Kernel Trick for Other Methods • Besides L2-regularized least squares, when can we use kernels? – “Representer theorems” (bonus slide) have shown that any L2-regularized linear model can be kernelized (see bonus): • Kernel robust regression with L2-regularization. • Kernel brittle regression with L2-regularization. • Kernel hinge loss (SVM) or logistic loss with L2-regularization. • Kernel multi-class SVM or multi-class logistic with L2-regularization. Logistic Regression with KernelsSummary • Common convolution filters for computer vision: – Gaussian, Laplacian of Gaussian, and Gabor filters. • Filter banks: make features by taking a bunch of convolutions. • High-dimensional bases allows us to separate non-separable data. • “Other” normal equations are faster when n < d. • Kernel trick allows us to use high-dimensional bases efficiently. – Write model to only depend on inner products between features vectors. • Kernels let us use similarity between objects, rather than features. – Allows some exponential- or infinite-sized feature sets. – Applies to distance-based and linear models with L2-regularization. • Next time: – How do we train on all of Gmail? Image Convolution ExamplesImage Coordinates • Should we use the image coordinates? – E.g., the pixel is at location (124, 78) in the image. • Considerations: – Is the interpretation different in different areas of the image? – Are you using a linear model? • Would “distance to center” be more logical? – Do you have enough data to learn about all areas of the image? Alignment-Based Features • The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. • We aligned the images with a “template image”. Alignment-Based Features • The position in the image is important in brain tumour application. – But we didn’t have much data, so coordinates didn’t make sense. • We aligned the images with a “template image”. – Allowed “alignment-based” features: Motivation: Automatic Brain Tumor Segmentation • Final features for brain tumour segmentation: – Gaussian convolution of original/template/priors/symmetry, Laplacian of Gaussian on original. • All with 3 variances. • Max(Gabor) with sine and cosine on orginal (3 variances). Motivation: Automatic Brain Tumour Segmentation • Logistic regression and SVMs among best methods. – When using these 72 features from last slide. – If you used all features I came up with, it overfit. • Possible solutions to overfitting: – Forward selection was too slow. • Just one image gives 8 million training examples. – I did manual feature selection (“guess and check”). – L2-regularization with all features also worked. • But this is slow at test time. • L1-regularization gives best of regularization and feature selection. SIFT Features • Scale-invariant feature transform (SIFT): – Features used for object detection (“is particular object in the image”?) – Designed to detect unique visual features of objects at multiple scales. – Proven useful for a variety of object detection tasks. http://opencv-python-tutroals.readthedocs.io/en/latest/py_tutorials/py_feature2d/py_sift_intro/py_sift_intro.html Why is inner product a similarity? • It seems weird to think of the inner-product as a similarity. • But consider this decomposition of squared Euclidean distance: • If all training examples have the same norm, then minimizing Euclidean distance is equivalent to maximizing inner product. – So “high similarity” according to inner product is like “small Euclidean distance”. – The only difference is that the inner product is biased by the norms of the training examples. – Some people explicitly normalize the xi by setting xi = (1/||xi||)xi, so that inner products act like the negation of Euclidean distances. • E.g., Amazon product recommendation. A String Kernel • A classic “string kernel”: – We want to compute k(“cat”, “cart”). – Find all common subsequences: ‘c’, ‘a’, ‘t’, ‘ca’, ‘at’, ‘ct’, ‘cat’. – Weight them by total length in original strings: • ‘c’ has length (1,1), ‘ca’ has lengths (2,2), ‘ct’ has lengths (3,4), and so on. – Add up the weighted lengths of common subsequences to get a similarity: where γ is a hyper-parameter controlling influence of length. • Corresponds to exponential feature set (counts/lengths of all subsequences). – But kernel can be computed in polynomial time by dynamic programming. • Many variations exist. Kernel Trick for Other Methods • Besides L2-regularized least squares, when can we use kernels? – “Representer theorems have shown that any L2-regularized linear model can be kernelized: Kernel Trick for Other Methods • Besides L2-regularized least squares, when can we use kernels? – “Representer theorems” have shown that any L2-regularized linear model can be kernelized. – Linear models without regularization fit with gradient descent. • If you starting at v=0 or with any other value in span of rows of ‘Z’.","libVersion":"0.2.1","langs":""}