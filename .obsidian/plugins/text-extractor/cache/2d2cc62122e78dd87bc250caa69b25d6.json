{"path":".obsidian/plugins/text-extractor/cache/2d2cc62122e78dd87bc250caa69b25d6.json","text":"CPSC 340 Assignment 6 (due 2023-12-06 at 11:59pm) Important: Submission Format [5 points] Please make sure to follow the submission instructions posted on the course website. We will deduct marks if the submission format is incorrect, or if you’re not using LATEX and your submission is at all diﬃcult to read – at least these 5 points, more for egregious issues. Compared to assignment 1, your name and student number are no longer necessary (though it’s not a bad idea to include them just in case, especially if you’re doing the assignment with a partner). 1 Robust PCA for Background Subtraction If you run python main.py 1, the program will load a dataset X where each row contains the pixels from a single frame of a video of a highway. The demo applies PCA to this dataset and then uses this to reconstruct the original image. It then shows the following 3 images for each frame: 1. The original frame. 2. The reconstruction based on PCA. 3. A binary image showing locations where the reconstruction error is non-trivial. Recently, latent-factor models have been proposed as a strategy for “background subtraction”: trying to separate objects from their background. In this case, the background is the highway and the objects are the cars on the highway. Why does this make any sense? Remember that PCA tries to ﬁnd principal components that reconstruct the dataset well. Here the training examples are highway images. Now, we need to reconstruct these highway images out of a small number of PC images. Since we want to reconstruct them well, we’re going to create PCs that capture the common features across the entire dataset. In other words, the PCs will try to reconstruct the highway and slight variations of the highway, because that is always there. If a car appears in the top-left of only a couple images, we might not want a PC for that case, since that PC is useless for the majority of the training set. Thus, when we try to reconstruct the images we are left to see only the parts of the image that can be made out of the PCs, or in other words that are common across all the images. Hence why the reconstructions look like the background. We can then subtract this reconstruction (background) from the original image to get objects of interest (foreground). Cool, right? In this demo, we see that PCA does an OK job of identifying the cars on the highway in that it does tend to identify the locations of cars. However, the results aren’t great as it identiﬁes quite a few irrelevant parts of the image as objects. Who likes good news — everyone? Well good news, everyone! We can use robust PCA to do even better. Robust PCA is a variation on PCA where we replace the L2-norm with the L1-norm, f (Z, W ) = n∑ i=1 d∑ j=1 |⟨wj, zi⟩ − xij|, and it has recently been proposed as a more eﬀective model for background subtraction. 1 1.1 Why Robust PCA [5 points] In a few sentences, explain why using a robust loss might help PCA perform better for this background subtraction application. Some conversation-starters: what is an outlier here — a car or the highway? What does it mean to be robust to outliers — that we’re good at reconstructing them or that we’re back at reconstructing them? 1.2 Robust PCA Approximation and Gradient [5 points] If you run python main.py 1.3 , the program will repeat the same procedure as in the above section, but will attempt to use the robust PCA method, whose objective functions are yet to be implemented. You will need to implement it (yay!). We’ll use a gradient-based approach to PCA and a smooth approximation to the L1-norm. In this case the log-sum-exp approximation to the absolute value may be hard to get working due to numerical issues. Perhaps the Huber loss would work. We’ll use the “multi-quadric” approximation: |α| ≈ √α2 + ϵ, where ϵ controls the accuracy of the approximation (a typical value of ϵ is 0.0001). Note that when ϵ = 0 we revert back to the absolute value, but when ϵ > 0 the function becomes smooth. Our smoothed loss is: f (Z, W ) = n∑ i=1 d∑ j=1 √(⟨wj, zi⟩ − xij)2 + ϵ The partial derivatives of this loss with respect to the elements of W are: ∂ ∂wcj f (Z, W ) = ∂ ∂wcj n∑ i=1 d∑ j′=1 ((⟨wj′, zi⟩ − xij′)2 + ϵ ) 1 2 = n∑ i=1 ∂ ∂wcj ((⟨wj, zi⟩ − xij) 2 + ϵ) 1 2 (since the j′ ̸= j terms have no wcj in them) = n∑ i=1 1 2 ((⟨wj, zi⟩ − xij) 2 + ϵ)− 1 2 ∂ ∂wcj ((⟨wj, zi⟩ − xij) 2 + ϵ) = n∑ i=1 1 2 ((⟨wj, zi⟩ − xij) 2 + ϵ)− 1 2 2 (⟨wj, zi⟩ − xij) ∂ ∂wcj ⟨wj, zi⟩ = n∑ i=1 ((⟨wj, zi⟩ − xij)2 + ϵ )− 1 2 (⟨wj, zi⟩ − xij) zic The partial derivatives with respect to Z are similar: 2 ∂ ∂zic f (Z, W ) = ∂ ∂zic n∑ i′=1 d∑ j=1 ((⟨wj, zi′⟩ − xi′j)2 + ϵ) 1 2 = d∑ j=1 ((⟨wj, zi⟩ − xij)2 + ϵ )− 1 2 (⟨wj, zi⟩ − xij) ∂ ∂zic ⟨wj, zi⟩ = d∑ j=1 ((⟨wj, zi⟩ − xij)2 + ϵ )− 1 2 (⟨wj, zi⟩ − xij) wcj If we put this into matrix(ish) notation, we get the following: ∇W f (Z, W ) = Z T [R ⊘ (R◦2 + ϵ)◦ 1 2 ] where R ≡ ZW − X, A ⊘ B denotes element-wise division of A and B, A + s for a scalar s denotes element-wise adding s to each entry of A, and A◦p denotes taking A to the element-wise power of p. And, similarly, the gradient with respect to Z is given by: ∇Zf (Z, W ) = [R ⊘ (R◦2 + ϵ)◦ 1 2 ] W T Show that the two parts of the gradient above, ∇W f (Z, W ) and ∇Zf (Z, W ), have the expected dimensions. 1.3 Robust PCA Implementation [10 points] In fun obj.py, you will ﬁnd classes named RobustPCAFactorsLoss and RobustPCAFeaturesLoss which should compute the gradients with respect to W and Z, respectively. Complete the evaluate() method for each using the smooth approximation and gradient given above. Submit (1) your code in fun obj.py, and (2) one of the resulting ﬁgures. Hint: Your code will look similar to the already implemented PCAFactorsLoss and PCAFeaturesLoss classes. Note that the arguments for evaluate() are carefully ordered and shaped to be compatible with our opti- mizers. Note: The robust PCA is somewhat sensitive to initialization, so multiple runs might be required to get a reasonable result. 1.4 Reﬂection [6 points] 1. Very brieﬂy comment on the results from the previous section — does robust PCA seem to do better than regular PCA for this task? 2. How does the number of video frames and the size of each frame relate to n, d, and/or k? 3. What would the eﬀect be of changing the threshold (see code) in terms of false positives (cars we identify that aren’t really there) and false negatives (real cars that we fail to identify)? 3 2 Movie Recommendations If you run python main.py 2, the program will perform the following steps: 1. Loads the small educational version of the MovieLens dataset (https://grouplens.org/datasets/ movielens/). 2. Prints out the ﬁrst 5 rows of the ratings dataframe (which we’ll use) and the movies dataframe (which we won’t use, but is loaded FYI). 3. Transforms the ratings table into the Y matrix described in lecture. 4. Splits Y into train (80% of the ratings) and validation (20% of the ratings) matrices. 5. Prints out some stats about the dataset, including the average rating in the training set. 2.1 Understanding Y [6 points] Answer the following questions: 1. In lecture, we used the “?” symbol to represent missing ratings. How are these missing entries of Y implemented in the code? 2. How many (non-missing) ratings are there in Y train and Y valid, respectively? 3. Does the same user-movie rating ever appear in both Y train and Y valid? Is the result as expected? 2.2 Implementing Collaborative Filtering [15 points] If you run python main.py 2.2, the code will ﬁt a model that doesn’t actually do anything. It uses the same alternating minimization scheme as in the previous question on Robust PCA. Fill in the evaluate methods of the CollaborativeFilteringWLoss and CollaborativeFilteringZLoss classes in fun obj.py. Submit your code. Hint: Since we’re minimizing the regularized PCA loss, your code should be quite similar to the evaluate methods in PCAFeaturesLoss and PCAFactorsLoss. I suggest you copy/paste those methods as a starting point. However, there are two changes you’ll need to make: (1) modify the loss and gradient to account for the regularization, and (2) accounting for the fact that the loss should only sum over the available ratings, not the entire Y matrix. The easiest way to account for this is to modify the R matrix by setting all its NaN values to 0. Since R represents the residuals (or reconstruction errors), setting these to 0 says that these entries don’t contribute to the loss, which is exactly what we want. 2.3 Hyperparameter tuning [5 points] If you run python main.py 2.3, the program will implement a baseline model that predicts the average rating (around 3.5 stars) for all missing ratings. As you’ll see from the output, this baseline has a root mean squared error (RMSE) of slightly above 1 star. The hyperparameters we gave you in the code actually do worse than this (my validation RMSE is around 1.3 stars). Adjust the hyperparameters of your collaborative ﬁltering model, namely k, λW , λZ, until you obtain a better validation error than the baseline. Submit the hyperparameters you found as well as your training and validation RMSE. PS: there is nothing to stop you from just guessing random hyperparameters, but it would be cool if you think about the fact that the current model overﬁts and adjust each of the hyperparameters in the direction that reduces the complexity of the model to combat the overﬁtting. 4 2.4 Regularization with PCA [5 points] In lecture we discussed the fact that with regularized PCA, we need to regularize both W and Z for things to make sense. Test this out empirically by setting λZ = 0 and λW > 0. Modify the code so that it prints out the Frobenius norm of W and Z after each iteration of the alternating minimization (you don’t have to submit this printing code though). Describe your observations and brieﬂy discuss. 3 Neural Networks 3.1 Neural Networks by Hand [7 points] Suppose that we train a neural network with sigmoid activations and one hidden layer and obtain the following parameters (assume that we don’t use any bias variables): W = [ −2 −1 3 1 0 −2 ] , v = [5 3 ] . Assuming that we are doing regression, for a training example with features xT i = [ 4 −2 2 ] what are the values in this network of zi, h(zi), and ˆyi? 3.2 Neural Networks vs. Linear Classiﬁer [7 points] If you run python main.py 3, the program will train a multi-class logistic regression classiﬁer on the MNIST handwritten digits data set using stochastic gradient descent with some pre-tuned hyperparameters. The scikit-learn implementation called by the code uses a minibatch size of 1 and a one-vs-rest strategy for multi-class logistic regression. It is set to train for 10 epochs. The performance of this model is already quite good, with around 8% training and validation errors. Your task is to use a neural networks classiﬁer to outperform this baseline model. If you run python main.py 3.2, the program will train a single-layer neural network (one-layer nonlinear encoder paired with a linear classiﬁer) on the same dataset using some pre-tuned hyperparameters. Modify the code, play around with the hyperparameter values (conﬁgurations of encoder layers, batch size and learning rate of SGD, standardization, etc.) until you achieve validation error that is reliably below 5%. Report (1) the training and validation errors that you obtain with your hyperparameters, and (2) the hyperparameters that you used. 3.3 Neural Disentangling [10 points] An intuitive way to think about a neural network is to think of it as a composition of an encoder (all the layers except the last one, including the ﬁnal nonlinearity) and a linear predictor (the last layer). The predictor signals the encoder to learn a better feature space, in such a way that the ﬁnal linear predictions are meaningful. The following ﬁgures resulted from running python main.py 3.3. For this, a neural network model is used to encode the examples into a 2-dimensional feature space. Here’s the original training data: 5 And here is the learned decision boundary of the neural network: We can look inside and see what the encoder learned. Here is the training data shown in the 2D transformed feature space learned by the encoder (ﬁrst layer): 6 Here is the linear classiﬁer learned by the predictor in this transformed feature space: This particular neural network solution does not classify the given examples very well. However, if we use a 4-dimensional hidden features (with all else equal), we get a better-looking result: 7 Note: in this case, since the hidden feature space has dimension 4, we are only able to plot 2 of the 4 dimensions. The actual linear classiﬁers is a 3D hyperplane dividing the 4D space. However, even with these two dimensions that we can plot, we can already see that the training data looks pretty much linearly separable, and we learn a good classiﬁer. Previously we changed the hidden layer size from 2 to 4. Another change we can make is to increase the number of layers from 1 to 2. Let’s use 2 layers of hidden features of size 2: 8 Answer the following questions and provide a brief explanation for each: 1. Why are the axes (z1 and z2) of the learned feature space scatterplot constrained to [0, 1]? Note/hint: here, our deﬁnition of z is after the nonlinearity is applied, whereas in the lecture z is deﬁned as before the nonlinearity is applied. Sorry about that! 2. Does it make sense to use neural networks on datasets whose examples are linearly separable in the original feature space? 9 3. Why would increasing the dimensionality of the hidden features (which is a hyper-parameter) help improve the performance of our classiﬁer? 4. Why would increasing the number of layers help improve the performance of our classiﬁer? 5. Neural networks are known to suﬀer problems due to a highly non-convex objective function. What is one way we can address this problem and increase the chances of discovering a global optimum? (Hint: look at the code.) 4 Very-Short Answer Questions [14 points] Answer each of the following questions in a sentence or two. 1. Give one reason why you might want sparse solutions with a latent-factor model. 2. Which is better for recommending movies to a new user, collaborative ﬁltering or content-based ﬁlter- ing? Brieﬂy justify your answer. 3. Are neural networks mainly used for supervised or unsupervised learning? Are they parametric or nonparametric? 4. Why might regularization become more important as we add layers to a neural network? 5. Consider using a fully-connected neural network for 5-class classiﬁcation on a problem with d = 10. If the network has one hidden layer of size k = 100, how many parameters (including biases), does the network have? 6. What is the “vanishing gradient” problem with neural networks based on sigmoid non-linearities? 7. Convolutional networks seem like a pain... why not just use regular (“fully connected”) neural networks for image classiﬁcation? 10","libVersion":"0.2.1","langs":""}