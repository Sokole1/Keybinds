{"path":".obsidian/plugins/text-extractor/cache/6f8cd40794708b78814d0c1f7029e40c.json","text":"Question 2. (20 points) Circle all that apply. There may be zero correct choices or multiple correct choices, not necessarily just one!! No need to justify your answers. (a) Which of the following changes would typically reduce training error? Note: For unsupervised learning algorithms, “reducing training error” means reducing the loss. Also, for the neural network questions, assume you always reach a global minimum of the loss. i. using the faster decision tree algorithm based on sorting, instead of the naive approach ii. increasing the amount of Laplace smoothing (higher 8 parameter) for naive Bayes iii. using a smaller k¥ with KNN iv. increasing the maximum tree depth in a random forest v. increasing k in k-means clustering vi. increasing A in regularized linear regression vii. fitting OLS with gradient descent instead of the normal equations viii. running more iterations of gradient descent (assuming « is small enough) ix. using a higher degree polynomial basis for linear regression x. removing some irrelevant features from your model xi. removing some relevant features from your model xii. fitting logistic regression with SGD instead of gradient descent xiii. increasing the width, o, of the RBF kernel xiv. increasing the variance of the prior in the MLE/MAP view of a model xv. fitting PCA with gradient descent instead of SVD xvi. removing the orthogonality constraint from PCA xvil. increasing k in NMF xviii. adding another layer to a neural network xix. increasing the hidden layer size in a 1-hidden-layer neural network xx. using larger filters in a CNN","libVersion":"0.2.1","langs":"eng"}