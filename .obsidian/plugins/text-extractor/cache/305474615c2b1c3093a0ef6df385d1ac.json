{"path":".obsidian/plugins/text-extractor/cache/305474615c2b1c3093a0ef6df385d1ac.json","text":"Last Time: Bayesian Predictions and Empirical Bayes o We've discussed making predictions using posterior predictive, g€ argmaX/ p(7 | 2, w)p(w | X,y, \\)dw, [ w which gives optimal predictions given your assumptions. o We considered empirical Bayes (type Il MLE), A€ arg;naxp(y | X,\\), where p(y|X,\\)= / p(y | X, w)p(w | \\)dw, w where we optimize marginal likelihood to select model and/or hyper-parameters. o Allows a huge number of hyper-parameters with less over-fitting than MLE. o Can use gradient descent to optimize continuous hyper-parameters. o Ratio of marginal likelihoods (Bayes factor) can be used for hypothesis testing. o In many settings, naturally encourages sparsity (in parameters, data, clusters, etc.).","libVersion":"0.2.1","langs":"eng"}