{"path":".obsidian/plugins/text-extractor/cache/cad2a31bc0c28d92f0ee87e46ec30713.json","text":"Convex Sets and Functions Strict-Convexity and Strong-Convexity CPSC 540: Machine Learning Convex Optimization Mark Schmidt University of British Columbia Winter 2020 Convex Sets and Functions Strict-Convexity and Strong-Convexity Admin Registration forms: I will sign them at the end of class (need to submit prereq form ﬁrst). Website/Piazza: https://www.cs.ubc.ca/~schmidtm/Courses/540-W20. https://piazza.com/ubc.ca/winterterm22019/cpsc540. Tutorials: start Monday after class. Assignment 1 due next Friday. All questions now posted, see Piazza update thread for changes. Gradescope submission instructions coming soon. Convex Sets and Functions Strict-Convexity and Strong-Convexity Machine Learning and Optimization In machine learning, training is typically written as an optimization problem: We optimize parameters w of model, given data. There are some exceptions: 1 Methods based on counting and distances (KNN, random forests). See CPSC 340. 2 Methods based on averaging and integration (Bayesian learning). Later in course. But even these models have parameters to optimize. Important class of optimization problems: convex optimization problems. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convex Optimization Consider an optimization problem of the form min w∈C f (w). where we are minimizing a function f subject to w being in the set C. For least squares we have f (w) = ∥Xw − y∥ 2 and C ≡ Rd If we had non-negative constraints, we would have C ≡ {w | w ≥ 0}. Notation: when I write w ≥ 0 for a vector w I mean inequality holds for each row. We say that this is a convex optimization problem if: The set C is a convex set. The function f is a convex function. This lecture is boring, but convexity ideas will show up throughout the course. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convex Optimization Key property of convex optimization problems: All local optima are global optima. Convexity is usually a good indicator of tractability: Minimizing convex functions is usually easy. Minimizing non-convex functions is usually hard. Oﬀ-the-shelf software solves many classes of convex problems (MathProgBase). Convex Sets and Functions Strict-Convexity and Strong-Convexity Deﬁnition of Convex Sets A set C is convex if the line between any two points stays also in the set. Convex Sets and Functions Strict-Convexity and Strong-Convexity Deﬁnition of Convex Sets To formally deﬁne convex sets, we use the notion of convex combination: A convex combination of two variables w and v is given by θw + (1 − θ)v for any 0 ≤ θ ≤ 1, which characterizes the points on the line between w and v. A set C is convex if convex combinations of points in the set are also in the set: For all w ∈ C and v ∈ C we have θw + (1 − θ)v | {z } convex comb ∈ C for 0 ≤ θ ≤ 1. This deﬁnition allows us to prove the convexity of many simple sets. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Examples of Simple Convex Sets Real space Rd. Positive orthant Rd + : {w | w ≥ 0}. Hyper-plane: {w | a⊤w = b}. Half-space: {w | a⊤w ≤ b}. Norm-ball: {w | ∥w∥p ≤ τ }. Norm-cone: {(w, τ ) | ∥w∥p ≤ τ }. Convex Sets and Functions Strict-Convexity and Strong-Convexity Showing a Set is Convex from Intersections Useful property: the intersection of convex sets is convex. We can prove convexity of a set by showing it’s an intersection of convex sets. Example: “linear programs” have constraints of the form Aw ≤ b. Each constraints a ⊤ i bi deﬁnes a half-space. Half-spaces are convex sets. So the set of w satisfying Aw ≤ b is the intersection of convex sets. Convex Sets and Functions Strict-Convexity and Strong-Convexity Showing a Set is Convex from a Convex Function The set C is often the intersection of a set of inequalities of the form {w | g(w) ≤ τ }, for some function g and some number τ . Sets deﬁned like this are convex if g is a convex function (see bonus). This follows from the deﬁnition of a convex function (next topic). Example: The set of w where w2 ≤ 10 forms a convex set by convexity of w2. Speciﬁcally, the set is [−√10, √10]. Convex Sets and Functions Strict-Convexity and Strong-Convexity Digression: k-way Convex Combinations and Diﬀerentiability Classes A convex combintion of k vectors {w1, w2, . . . , wk} is given by k∑ c=1 θcwc where k∑ c=1 θc = 1, θc ≥ 0. We’ll deﬁne convex functions for diﬀerent diﬀerentiability classes: C 0 is the set of continuous functions. C 1 is the set of continuous functions with continuous ﬁrst-derivatives. C 2 is the set of continuous functions with continuous ﬁrst- and second-derivatives. Convex Sets and Functions Strict-Convexity and Strong-Convexity Deﬁnitions of Convex Functions Four quivalent deﬁnitions of convex functions (depending on diﬀerentiability): 1 A C 0 function is convex if the area above the function is a convex set. 2 A C 0 function is convex if the function is always below its “chords” between points. 3 A C 1 function is convex if the function is always above its tangent planes. 4 A C 2 function is convex if it is curved upwards everwhere. If the function is univariate this means f ′′(w) ≥ 0 for all w. Univariate examples where you can show f ′′(w) ≥ 0 for all w: Quadratic w2 + bw + c with a ≥ 0. Linear: aw + b. Constant: b. Exponential: exp(aw). Negative logarithm: − log(w). Negative entropy: w log w, for w > 0. Logistic loss: log(1 + exp(−w)). Convex Sets and Functions Strict-Convexity and Strong-Convexity C 0 Deﬁnitions of Convex Functions A function f is convex iﬀ the area above the function is a convex set. Equivalently, the function is always below its “chords” between points. f (θw + (1 − θ)v | {z } convex comb ) ≤ θf (w) + (1 − θ)f (v) | {z } “chord” , for all w ∈ C, v ∈ C, 0 ≤ θ ≤ 1. Implies all local minima of convex functions are global minima. Indeed, ∇f (w) = 0 means w is a global minima. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convexity of Norms The C0 deﬁnition can be used to show that all norms are convex: If f (w) = ∥w∥p for a generic norm, then we have f (θw + (1 − θ)v) = ∥θw + (1 − θ)v∥p ≤ ∥θw∥p + ∥(1 − θ)v∥p (triangle inequality) = |θ| · ∥w∥p + |1 − θ| · ∥v∥p (absolute homogeneity) = θ∥w∥p + (1 − θ)∥v∥p (0 ≤ θ ≤ 1) = θf (w) + (1 − θ)f (v), (deﬁnition of f ) so f is always below the “chord”. See course webpage notes on norms if the above steps aren’t familiar. Also note that all squared norms are convex. These are all convex: |w|, ∥w∥, ∥w∥1, ∥w∥ 2, ∥w1∥2, ∥w∥∞,... Convex Sets and Functions Strict-Convexity and Strong-Convexity Operations that Preserve Convexity There are a few operations that preserve convexity. Can show convexity by writing as sequence of convexity-preserving operations. If f and g are convex functions, the following preserve convexity: 1 Non-negative scaling: h(w) = αf (w). 2 Sum: h(w) = f (w) + g(w). 3 Maximum: h(w) = max{f (w), g(w)}. 4 Composition with linear: h(w) = f (Aw), where A is a matrix (or another linear operator). But note that composition f (g(w)) of convex f and g is not convex in general. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convexity of SVMs If f and g are convex functions, the following preserve convexity: 1 Non-negative scaling. 2 Sum. 3 Maximum. 4 Composition with linear. We can use these to quickly show that SVMs are convex, f (w) = n∑ i=1 max{0, 1 − yiw⊤xi} + λ 2 ∥w∥ 2. Second term is squared norm multiplied by non-negative λ 2 . Squared norms are convex, and non-negative scaling perserves convexity. First term is sum(max(linear)). Linear is convex and sum/max preserve convexity. Since both terms are convex, and sums preserve convexity, SVMs are convex. Convex Sets and Functions Strict-Convexity and Strong-Convexity C 1 Deﬁnition of Convex Functions Convex functions must be continuous, and have a domain that is a convex set. But they may be non-diﬀerentiable. A diﬀerentiable (C1) function f is convex iﬀ f is always above tangent planes. f (v) ≥ f (w) + ∇f (w)⊤(v − w), ∀w ∈ C, v ∈ C. Notice that ∇f (w) = 0 implies f (v) ≥ f (w) for all v, so w is a global minimizer. Convex Sets and Functions Strict-Convexity and Strong-Convexity C 2 Deﬁnition of Convex Functions The multivariate C2 deﬁnition is based on the Hessian matrix, ∇2f (w). The matrix of second partial derivatives, ∇2f (w) =      ∂ ∂w1∂w1 f (w) ∂ ∂w1∂w2 f (w) · · · ∂ ∂w1∂wd f (w) ∂ ∂w2∂w1 f (w) ∂ ∂w2∂w2 f (w) · · · ∂ ∂w2∂wd f (w) ... ... . . . ... ∂ ∂wd∂w1 f (w) ∂ ∂wd∂w2 f (w) · · · ∂ ∂wd∂wd f (w)      In the case of least squares, we can write the Hessian for any w as ∇2f (w) = X ⊤X, see course webpage notes on the gradients/Hessians of linear/quadratic functions. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convexity of Twice-Diﬀerentiable Functions A C2 function is convex iﬀ: ∇2f (w) ≽ 0, for all w in the domain (“curved upwards” in every direction). This notation A ≽ 0 means that A is positive semideﬁnite. Two equivalent deﬁnitions of a positive semideﬁnite matrix A: 1 All eigenvalues of A are non-negative. 2 The quadratic v⊤Av is non-negative for all vectors v. Convex Sets and Functions Strict-Convexity and Strong-Convexity Example: Convexity and Least Squares We can use twice-diﬀerentiable condition to show convexity of least squares, f (w) = 1 2 ∥Xw − y∥2. The Hessian of this objective for any w is given by ∇2f (w) = X ⊤X. So we want to show that X ⊤X ≽ 0 or equivalently that v⊤X ⊤Xv ≥ 0 for all v. We can show this by non-negativity of norms, v⊤X ⊤Xv = (v⊤X ⊤) | {z } (Xv)⊤ Xw = (Xv) ⊤(Xv) | {z } u⊤u = ∥Xv∥2 | {z } ∥u∥2 ≥ 0, so least squares is convex (and solving ∇f (w) = 0 gives global minimum). Convex Sets and Functions Strict-Convexity and Strong-Convexity Showing that Function is Convex Most common approaches for showing that a function is convex: 1 Show that f is constructed from operations that preserve convexity. Non-negative scaling, sum, max, composition with linear. 2 Show that ∇2f (w) is positive semi-deﬁnite for all w (for C 2 functions), ∇2f (w) ≽ 0 (zero matrix). 3 Show that f is below chord for any convex combination of points. f (θw + (1 − θ)v ≤ θf (w) + (1 − θ)f (v). Post-lecture slides: convexity of logistic regression from C2 deﬁnition. And how to write logistic regression gradient and Hessian in matrix notation. Convex Sets and Functions Strict-Convexity and Strong-Convexity Outline 1 Convex Sets and Functions 2 Strict-Convexity and Strong-Convexity Convex Sets and Functions Strict-Convexity and Strong-Convexity Positive Semi-Deﬁnite, Positive Deﬁnite, Generalized Inequality The notation A ≽ 0 indicates that A is positive semi-deﬁnite. The eigenvalues of A are all non-negative. v⊤Av ≥ 0 for all vectors v. The notation A ≻ 0 indicates that A is positive deﬁnite. The eigenvalues of A are all positive. v⊤Av > 0 for all vectors v ̸= 0. This implies that A is invertible (bonus). The notation A ≽ B indicates that A − B is positive semi-deﬁnite. The eigenvalues of A − B are all non-negative. v⊤Av ≥ v⊤Bv for all vectors v. MEMORIZE! Convex Sets and Functions Strict-Convexity and Strong-Convexity More Examples of Convex Functions Some convex sets based on these deﬁntions that we’ll use (for covariances): The set of positive semideﬁnite matrices, {W | W ≽ 0}. The set of positive deﬁnite matrices, {W | W ≻ 0}. Some more exotic examples of convex functions we’ll use in this course: f (W ) = − log detW for W ≻ 0 (negative log-determinant). f (W, v) = v⊤W −1v for W ≻ 0. f (w) = log(∑d j=1 exp(wj)) (log-sum-exp function). Convex Sets and Functions Strict-Convexity and Strong-Convexity Positive Semi-Deﬁnite, Positive Deﬁnite, Generalized Inequality Note that not every matrix can be compared. With these matrices: A = [ 1 0 0 0 ] and B = [ 0 0 0 1 ] , neither A ≽ B nor B ≽ A (the “generalized inequality” deﬁnes a “partial order”). It’s often useful to compare to the identity matrix I, which has eigenvalues 1. So a matrix of the form µI for a scalar µ has all eigenvalues equal to µ. Writing LI ≽ A ≽ µI means “eigenvalues of A are between µ and L”. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convexity, Strict Convexity, and Strong Convexity We say that a C2 function is convex if for all w, ∇2f (w) ≽ 0, and this implies any stationary point (∇f (w) = 0) is a global minimum. We say that a C2 function is strictly convex if for all w, ∇2f (w) ≻ 0, and this implies there is at most one stationary point (and ∇2f (w) is invertible). We say that a C2 function is strongly convex if for all w. ∇2f (w) ≽ µI, for some µ > 0, and this implies there exists a stationary point (if domain C is closed). Strong convxity aﬀects speed of gradient descent, and how much data you need. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convexity, Strict Convexity, and Strong Convexity These deﬁnitions simplify for univariate functions: Convex: f ′′(w) ≥ 0. Strictly convex: f ′′(w) > 0. Strongly convex: f ′′(w) ≥ µ for µ > 0. Examples: Convex: f (w) = w. Since f ′′(w) = 0. Strictly convex: f (w) = exp(w). Since f ′′(w) = exp(w) > 0. Strongly convex: f (w) = 1 2 w2. Since f ′′(w) = 1 so it is strongly convex with µ = 1. Convex Sets and Functions Strict-Convexity and Strong-Convexity Strict Convexity of L2-Regularized Least Squares In L2-regularized least squares, the Hessian matrix is ∇2f (w) = (X ⊤X + λI). We can show that this is positive-deﬁnite, so the problem is strictly convex, v⊤∇2f (w)v = v⊤(X ⊤X + λI)v = ∥Xv∥2 | {z } ≥0 + λ∥v∥ 2 | {z } >0 > 0, where we used that λ > 0 and ∥v∥ > 0 for v ̸= 0. This implies that the matrix (X ⊤X + λI) is invertible, and solution is unique. Similar argument shows it’s strongly-convex with µ = λ. Value µ can be larger if columns of X are independent (no collinearity). In this case, ∥Xv∥ ̸= 0 for v ̸= 0 so even least squares is strongly-convex. Convex Sets and Functions Strict-Convexity and Strong-Convexity Strong-Convexity Discussion We can also deﬁne strict and strong convexity for C1 and C0 functions (bonus). For example, we say that a C0 function f is strongly convex if the function f (w) − µ 2 ∥w∥ 2, is a convex function for some µ > 0. “If you ‘un-regularize’ by µ then it’s still convex.” If we have a convex loss f , adding L2-regularization makes it strongly-convex, f (w) + λ 2 ∥w∥2, with µ being at least λ. So L2-regularization guarantees a solution exists, and that it is unique. Convex Sets and Functions Strict-Convexity and Strong-Convexity Summary Convex optimization problems are a class that we can usually eﬃciently solve. Showing functions and sets are convex. Either from deﬁnitions or convexity-preserving operations. C2 deﬁnition of convex functions that the Hessian is positive semideﬁnite. ∇2f (w) ≽ 0. Strict and strong convexity guarantee uniqueness and existense of solutions. Adding L2-regularization to a convex function gives you these. Post-lecture slides: matrix notationa and convexity of logistic regerssion. This will help with your assignments. How much data do we need? Convex Sets and Functions Strict-Convexity and Strong-Convexity Example: Convexity of Logistic Regression Consider the binary logistic regression model, f (w) = n∑ i=1 log(1 + exp(−yiwT xi)). With some tedious manipulations, gradient in matrix notation is ∇f (w) = X T r. where the vector r has elements ri = −yih(−yiwT xi). And h is the sigmoid function, h(α) = 1/1 + exp(−α). We know the gradient has this form from the multivariate chain rule. Functions for the form f (Xw) always have ∇f (w) = X T r (see bonus slide). Convex Sets and Functions Strict-Convexity and Strong-Convexity Example: Convexity of Logistic Regression With ome more tedious manipulations we get the Hessian in matrix notation as ∇2f (w) = X T DX. where D is a diagonal matrix with dii = h(yiwT xi)h(−yiwT xi). The f (Xw) structure leads to a X T DX Hessian structure. For other problems D may not be diagonal. Since the sigmoid function h is non-negative, we can compute D 1 2 , and vT X T DXv = vT X T D 1 2 D 1 2 Xv = (D 1 2 Xv) T (D 1 2 Xv) = ∥XD 1 2 v∥ 2 ≥ 0, so X T DX is positive semideﬁnite and logistic regression is convex. Convex Sets and Functions Strict-Convexity and Strong-Convexity Showing that Hyper-Planes are Convex Hyper-plane: C = {w | a⊤w = b}. If w ∈ C and v ∈ C, then we have a ⊤w = b and a⊤v = b. To show C is convex, we can show that a ⊤u = b for u between w and v. a ⊤u = a ⊤(θw + (1 − θ)v) = θ(a ⊤w) + (1 − θ)(a ⊤v) = θb + (1 − θ)b = b. Alternately, if you knew that linear functions a⊤w are convex, then C is the intersection of {w | a⊤w ≤ b} and {w | a⊤w ≥ b}. Convex Sets and Functions Strict-Convexity and Strong-Convexity Convex Sets from Functions For sets of the form C = {w | g(w) ≤ τ }, If g is a convex function, then C is a convex set: g(θw + (1 − θ)v | {z } convex comb ) ≤ θg(w) + (1 − θ)g(v) | {z } by convexity ≤ θτ + (1 − θ)τ | {z } deﬁnition of g = τ, which means convex combinations are in the set. Convex Sets and Functions Strict-Convexity and Strong-Convexity Multivariate Chain Rule If g : Rd 7→ Rn and f : Rn 7→ R, then h(x) = f (g(x)) has gradient ∇h(x) = ∇g(x) T ∇f (g(x)), where ∇g(x) is the Jacobian (since g is multi-output). If g is an aﬃne map x 7→ Ax + b so that h(x) = f (Ax + b) then we obtain ∇h(x) = AT ∇f (Ax + b). Further, for the Hessian we have ∇2h(x) = AT ∇2f (Ax + b)A. Convex Sets and Functions Strict-Convexity and Strong-Convexity Positive-Deﬁnite implies Invertibility If A ≻ 0, then all the eigenvalues of A are positive. If each eigenvalue is positive, the product of the eigenvalues is positive. The product of the eigenvalues is equal to the determinant. Thus, the determinant is positive. The determinant not being 0 implies the matrix is invertible. Convex Sets and Functions Strict-Convexity and Strong-Convexity Strong Convexity of L2-Regularized Least Squares In L2-regularized least squares, the Hessian matrix is ∇2f (w) = (X ⊤X + λI). v⊤∇2f (w)v = v⊤(X⊤X + λI)v = ∥Xv∥ 2 | {z } +v⊤(λI)v ≥ v⊤(λI)v, so we’ve shown that ∇2f (w) ≽ λI, which implies strong-convexity with µ = λ. This implies that a solution exists, and that the solution is unique. Note that we have strong convexity with µ > λ if X ⊤X is positive deﬁnite. Which happens iﬀ the features are independent (not collinear). Convex Sets and Functions Strict-Convexity and Strong-Convexity Strictly-Convex Functions A function is strictly-convex if the convexity deﬁnitions hold strictly: f (θw + (1 − θ)v) < θf (w) + (1 − θ)f (v), 0 < θ < 1 (C0) f (v) > f (w) + ∇f (w)⊤(v − w) (C1) ∇2f (w) ≻ 0 (C2) Function is always strictly below any chord, strictly above any tangent, and curved upwards in every direction. Strictly-convex function have at most one global minimum: w and v can’t both be global minima if w ̸= v: it would imply convex combinations u of w and v would have f (u) below the global minimum. Convex Sets and Functions Strict-Convexity and Strong-Convexity A C 0 Deﬁnition of Strict and Strong Convexity There are many equivalent deﬁnitions of the convexities, here is one set for C0 functions: Convex (usual deﬁnition): f (θw + (1 − θ)v) ≤ θf (w) + (1 − θ)f (v). Strictly convex (strict version, exclusindg θ = 0 or θ = 1): f (θw + (1 − θ)v) < θf (w) + (1 − θ)f (v). Strong convexity (need an “extra” bit of decrease as you move away from endpoints): f (θw + (1 − θ)v) ≤ θf (w) + (1 − θ)f (v) − θ(1 − θ)µ 2 ∥u − v∥ 2.","libVersion":"0.2.1","langs":""}