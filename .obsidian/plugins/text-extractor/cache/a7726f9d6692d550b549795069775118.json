{"path":".obsidian/plugins/text-extractor/cache/a7726f9d6692d550b549795069775118.json","text":"CPSC 302, Fall 2023, Assignment 2 Solutions Released October 6, 2023 1. This question is concerned with the function of the form f (x) = α cosh(x/4) − x, where α is a parameter. All parts of this question pertain to this function, and α is speciﬁed as necessary. For this question and other questions, to make sure you see what is going on, make sure to write format long at the start of your program; it will allow you to see enough digits of accuracy. (a) Conﬁrm graphically by generating two plots that the function has two zeros for α = 1 and none for α = 5 on the interval [−15, 15]. Include your Matlab code and the plots. Answer: We generate a simple script here with no sophistication whatsoever and get two plots that show what’s required: c l o s e a l l % Get r i d o f any p r e v i o u s f i g u r e s x=l i n s p a c e ( = 1 5 , 1 5 , 1 0 0 ) ; % A n i c e command t o g e n e r a t e a g r i d f i g u r e ; % Generate a new f i g u r e p l o t ( x , c o s h ( x/4)= x ) x l a b e l ( ’ x ’ ) y l a b e l ( ’ y ’ ) t i t l e ( ’ y=c o s h ( x/4)= x ’ ) f i g u r e ; % Generate a new f i g u r e p l o t ( x , 5 * c o s h ( x/4)= x ) x l a b e l ( ’ x ’ ) y l a b e l ( ’ y ’ ) t i t l e ( ’ y=5c o s h ( x/4)= x ’ ) -15 -10 -5 0 5 10 15 x -5 0 5 10 15 20 25 30 35 40y y=cosh(x/4)-x -15 -10 -5 0 5 10 15 x 0 20 40 60 80 100 120 140y y=5cosh(x/4)-x 1 (b) Compute the zero for α = 1, namely the zero of the function f (x) = cosh(x/4) − x, over the same interval speciﬁed in (a), using the secant method. Use x0 = 0.5 and x1 = 2.0 as your two initial guesses. Your program should use only one hyperbolic func- tion evaluation per iteration; marks will be deducted for an ineﬃcient implementation that is wasteful in terms of function evaluations. Print out the iterates and the absolute function values until the absolute value of the function, i.e., |f (xk)|, goes below 10−10. Answer: We write a function, presented below. (There are several ways to do it and we are not too particular about the exact format or very speciﬁc coding conventions; what we mainly care about is numerical eﬃciency, robustness, and correctness of the solver.) f u n c t i o n [ x i t e r , f i t e r ] = s e c a n t ( f , x0 , x1 , f t o l ) % S e c a n t = s o l v e a n o n l i n e a r e q u a t i o n f ( x)=0 u s i n g t h e s e c a n t method % Given i n i t i a l g u e s s e s x0 and x1 , compute an % approximate z e r o o f t h e f u n c t i o n f , u s i n g s e c a n t . E x i t once % t h e a b s o l u t e v a l u e o f f f o r t h e c u r r e n t i t e r a t e g o e s below t o l % Input : % f : a f u n c t i o n % x0 : f i r s t i n i t i a l g u e s s % x1 : s e c o n d i n i t i a l g u e s s % f t o l : t o l e r a n c e f o r a b s o l u t e f u n c t i o n v a l u e % Output : % x i t e r = a v e c t o r t h a t w i l l h o l d t h e i t e r a t e s % f i t e r = f u n c t i o n v a l u e s % i n i t i a l i z e o l d x=x0 ; o l d f=f ( o l d x ) ; c u r r x=x1 ; x i t e r =[ x0 ; x1 ] ; % i t e r a t e s x k f i t e r =o l d f ; % f u n c t i o n v a l u e s f ( x k ) % w h i l e t r u e c u r r f=f ( c u r r x ) ; % check c o n v e r g e n c e i f abs ( c u r r f )< f t o l f i t e r =[ f i t e r ; c u r r f ] ; r e t u r n end new x=c u r r x = c u r r f * ( c u r r x = o l d x ) / ( c u r r f = o l d f ) ; o l d x=c u r r x ; o l d f=c u r r f ; c u r r x=new x ; x i t e r =[ x i t e r ; new x ] ; % s t o r e i t e r a t e f i t e r =[ f i t e r ; o l d f ] ; % s t o r e f u n c t i o n v a l u e % A s i m p l e check t o e x i t g r a c e f u l l y i f something g o e s wrong i f l e n g t h ( x i t e r )>50 % e x i t i f not c o n v e r g e d a f t e r 50 i t e r a t i o n s f p r i n t f ( ’ E x i t i n g , t o o many i t e r a t i o n s ’ ) r e t u r n end end We then run it with the desired function and get satisfying results. [x_iter,f_iter]=secant(@(x) cosh(x/4)-x,0.5,2,1e-10); 2 After some minor output beautiﬁcation (please see answer to 2(a) if you’d like to see in general how to print the output in a nice way; we omit it in this answer) we print x iter and abs(f iter) and get k x |f(x)| == === ====== 0 0.500000000000000 0.507822677825711 1 2.000000000000000 0.872374034793619 2 1.051902500400071 0.017124676718383 3 1.032918723300612 0.000608246573621 4 1.033569874675092 0.000000385708140 5 1.033569462021354 0.000000000008673 (c) Comment brieﬂy on the convergence behaviour. There is no need here for a lengthy discussion - just state whether the speed of convergence matches your expectations based on the discussion in the lecture; recall that in the lecture we said that secant converges superlinearly, and that typically with initial guesses suﬃciently close to the solution we would expect to see convergence close enough to the rounding unit within 6-7 iterations. Answer: In order to know how well we have done, we can run the zero ﬁnder of Matlab (you were not expected to think of doing that, but we are doing something similar here to what we are doing in question 2 with nthroot, for the purpose of further illustration): x_star=fzero(@(x) cosh(x/4)-x,1) Let us identify the variable x star with the zero we are seeking, and call it x∗. We get x∗ = 1.033569462012075, fully accurate on this ﬂoating point system (double precision). (If we compute f (x∗) with this value of x star we get 0 on this computer.) We see that in terms of number of iterations, convergence is as expected. Typically it would take 6-7 iterations until we get to machine precision domain. Here we get for the function value an error of below 10−10 when we get to x5 (including the two initial guesses, which require the function to be evaluated, for a total of six function evaluations). The trend tells us that we would probably need one more iteration to get closer to machine precision (see more on the trend below). We see the superlinear convergence in action: as we get closer to the solution, we start gaining more digits in comparison with the beginning of the iteration. There was no requirement here to try to estimate ρk or do anything too sophisticated, but a welcome way of looking at things could to be to look at the ratios of each two adjacent iterations. (This is not being done here, for brevity, but it does give useful insight.) We have two coinciding decimal digits for x2 compared to x∗ (both start with 1.0), and then three for x3 (1.03), seven for x4 (1.033569), and eleven coinciding decimal digits for x5 (1.0335694620) in comparison with x∗. (d) Write a program using a ﬁxed-point iteration of the form xk+1 = cosh(xk/4). Use x0 = 0.5 as an initial guess, and then repeat your experiment with x0 = 2.0. As a convergence criterion, as you did in part (b), stop the iteration when | cosh(xk/4)−xk| < 10−10. Answer: We write a simple function here, too. It is less involved than secant. 3 f u n c t i o n [ x i t e r ] = f i x e d p o i n t ( g , x0 , t o l ) % f i x e d p o i n t = compute a f i x e d p o i n t o f a f u n c t i o n u s i n g f i x e d = p o i n t i t e r a t i o n % Given i n i t i a l g u e s s x0 , compute an % approximate f i x e d p o i n t o f t h e f u n c t i o n g . E x i t once % t h e a b s o l u t e v a l u e o f f f o r t h e c u r r e n t i t e r a t e g o e s below t o l o r i f % we f a i l e d t o c o n v e r g e a f t e r 50 i t e r a t i o n s % Input : % g = a f u n c t i o n % x0 : i n i t i a l g u e s s % t o l : t o l e r a n c e f o r a b s o l u t e f u n c t i o n v a l u e % Output : % x i t e r = a v e c t o r t h a t w i l l h o l d t h e i t e r a t e s % o l d x=x0 ; x i t e r=x0 ; f o r i =1:50 new x=g ( o l d x ) ; i f abs ( new x= o l d x )<1e = 10 r e t u r n end x i t e r =[ x i t e r new x ] ; o l d x=new x ; end % i f we g o t h e r e i t means we have not r e t u r n e d a f t e r 50 i t e r a t i o n s f p r i n t f ( ’ Did not c o n v e r g e a f t e r 50 i t e r a t i o n s ’ ) end We run ﬁrst with an initial guess x0 = 0.5: x_iter=fixed_point(@(x) cosh(x/4),0.5,1e-10); and obtain the iterates: 0.500000000000000 1.007822677825711 1.031909098780871 1.033461097241433 1.033562384077412 1.033568999687761 1.033569431813369 1.033569460039516 1.033569461883229 1.033569462003659 We then run with an initial guess x0 = 2: 2.000000000000000 1.127625965206381 1.039999486460834 1.033990802477277 1.033596989421957 1.033571260108453 1.033569579462626 1.033569469683867 4 1.033569462513192 1.033569462044808 (e) Determine using convergence theory whether the theoretical expectation for the scheme in (d) is to converge, and if so, what is the theoretical expected speed of convergence. Determine whether the results of your program support your theoretical expectations. Make sure to give a full answer here, based on the theory for ﬁxed-point iterations that we discussed in the lecture. Answer: The answer to this boils down mainly to computing ρ = |g′(x∗)|. Before we do so, for completeness we make the argument why there is a unique ﬁxed point and why we expect to converge from the initial guess stated in the question. In order to see this, it is enough to conﬁrm that the values of g(x) in the interval [0.5, 2] (where we take our initial guesses) are bounded by numbers between 0.5 and 2 (note that this is a suﬃcient condition, not a necessary one), and that the derivative g′(x) is bounded by 1 in that interval. Both of those things are easy to conﬁrm and require a couple of simple computations. (We omit presenting these numbers.) We see that we have converged within nine iterations (ten function evaluations including for the initial guess). If we decide that our last iterate is close enough, we can write x∗ = x9 = 1.033569462044808. We do a sanity check here: |x∗ − cosh(x∗/4)| = 3.059 × 10−11. (Alternatively, it is also okay to use the value we obtained for secant using fzero in part (c), for the purpose of our analysis.) We know that g′(x) = 1 4 sinh(x), and |g′(x∗)| = 0.0653..., which is quite a small number. Since it is not zero, convergence is linear. But convergence is pretty fast in relative terms: compare this to the case of α = 2 which we explored in the lecture! We know from the textbook and the slides (see slide 33 of Chapter 2 in pre-lecture slides) that we would expect to converge to within error of 10−10 within approximately 10/ log10 ρ = 8.439... iterations, which is nicely representative of the behaviour we are seeing. Being one iteration short is not a concern because this can be attributed to how far the initial guess is; in general we are interested to know the asymptotic behaviour near the solution. (f) Going back to the equation presented at the beginning of the question, is there an α for which the function f (x) = α cosh(x/4) − x has precisely one zero? If yes, then ﬁnd it, correct to at least six decimal digits, and the corresponding zero; if not, then justify. Some pen-and-paper work is required here, before any computations are carried out. If you then use a numerical method to compute your answer, your method should use as few as possible function evaluations of hyperbolic functions (such as sinh or cosh) Answer: There is exactly one solution when the curves x and g(x) = α cosh(x/4) are tangential at their meeting point, i.e., the derivative of f (x) = α cosh(x/4) − x is zero. We thus have two equations, one for the original equation and one for the derivatives: α cosh(x/4) = x, α 4 sinh(x/4) = 1. Dividing the two eliminates α, and we obtain 4 cosh(x/4) sinh(x/4) ≡ 4 coth(x/4) = x. 5 Once we ﬁnd a zero x∗, we can compute the solution for α, namely α∗. We can now run any of the Matlab functions or scripts we have generated, with the new function. (We are not including the code here.) For example, we can run our new and shiny secant function. We get a positive zero x∗ = 4.79871456103093, and then α∗ = x∗ cosh(x∗/4) = 2.65097367739673. A couple of additional comments: ‹ A more complete analysis will give us also a negative value of α, but we are focusing here on α positive and are looking in the positive interval. ‹ The little pen and paper work done here is greatly beneﬁcial. Just looking experi- mentally for α by some kind of a bisection-of-sorts strategy is extremely computa- tionally costly, and it violates the requirement to ﬁnd α within a minimal number of function evaluations. 2. (a) Write a program for computing the cube root of a number, x = 3√a, with only basic arithmetic operations using Newton’s method, by ﬁnding a root of the polynomial equa- tion f (x) = x3 − a = 0. Run your program for a = 2 and a = 10. For each of these cases, start with an initial guess of your choice, reasonably close to the solution. (A reasonably close initial guess may be, for example, x0 = 1 for a = 2 and x0 = 2 for a = 10.) As a stopping criterion, require the absolute function value to be smaller than 10−10. Print out the values of xk and |f (xk)| in each iteration. Answer: We write f (x) = x3 − a and then we have f ′(x) = 3x2. Therefore, Newton’s method is given by xk+1 = xk − x3 k − a 3x2 k = 2 3 xk + a 3x2 k . We use the simpliﬁed right-hand side on the second line for our scheme. f u n c t i o n [ x i t e r , f i t e r ] = n e w t o n c u b e r o o t ( a , x , f t o l ) % N e w t o n m u l t i p l e = compute t h e cube r o o t o f a number u s i n g Newton ’ s % method . Given a number a and an i n i t i a l g u e s s x0 , compute t h e cube % r o o t o f a by computing a z e r o o f t h e f u n c t i o n xˆ3= a =0. E x i t once % | x k ˆ3= a | f o r t h e c u r r e n t i t e r a t e g o e s below t o l o r i f we f a i l e d % t o c o n v e r g e a f t e r 20 i t e r a t i o n s % Input : % a : g i v e n number whose cube r o o t we w i l l a p p r o x i m a t e l y compute % x0 : i n i t i a l g u e s s % f t o l : t o l e r a n c e f o r a b s o l u t e f u n c t i o n v a l u e % Output : % x i t e r = a v e c t o r t h a t w i l l h o l d t h e i t e r a t e s % f i t e r = a v e c t o r t h a t w i l l h o l d t h e f u n c t i o n v a l u e s x i t e r=x ; f i t e r =xˆ3= a ; f o r k =1:31 6 x=2/3* x+a / ( 3 * x ˆ 2 ) ; f=xˆ3= a ; x i t e r =[ x i t e r ; x ] ; f i t e r =[ f i t e r ; f ] ; i f abs ( f )< f t o l f p r i n t f ( ” k x i t e r | f i t e r | \\ n ” ) f p r i n t f(”=================================\\n ” ) f o r j =1:k f p r i n t f ( ”%2d %13.10 f %13.10 f \\n ” , j = 1 , x i t e r ( j ) , abs ( f i t e r ( j ) ) ) ; end r e t u r n end end % i f we g o t h e r e i t means we have not r e t u r n e d a f t e r 50 i t e r a t i o n s f p r i n t f ( ’ Did not c o n v e r g e a f t e r 20 i t e r a t i o n s \\n ’ ) end We run the program twice. For a = 2 we get: [x_iter,f_iter]=newton_cube_root(2,1,1e-10); k x_iter |f_iter| ================================= 0 1.0000000000 1.0000000000 1 1.3333333333 0.3703703704 2 1.2638888889 0.0189552255 3 1.2599334934 0.0000592593 4 1.2599210500 0.0000000006 For a = 10 we get: [x_iter,f_iter]=newton_cube_root(10,1,1e-10); k x_iter |f_iter| ================================= 0 1.0000000000 9.0000000000 1 4.0000000000 54.0000000000 2 2.8750000000 13.7636718750 3 2.3199432892 2.4862523022 4 2.1659615552 0.1613692075 5 2.1544959252 0.0008527090 6 2.1544346918 0.0000000242 (b) We typically do not know the solution, but in this case we will compute it and check the iterates against it. To that end, add a few more commands to your program in part (a) as follows: use the command nthroot or just simply use a power of 1/3 to compute the exact cube roots (in Matlab ’s opinion, to machine rounding unit) of a = 2 and a = 10. In each case refer to the exact Matlab value you computed as x∗. Run Newton’s method again and this time print out all the absolute values of the error, namely |xk − x∗|, where xk are your Newton iterates for k = 0, 1, . . . until convergence. Answer: The changes in the program are minimal. We incorporate the command exactroot=nthroot(a,3), and slightly rearrange the program to compute the diﬀerence between the iterates and the new variable exactroot, and print them out. The code is not included here. 7 We get >> [x_iter,f_iter]=newton_cube_root_nthroot(2,1,1e-10); k x_iter |x^*-x_k| ================================= 0 1.0000000000 0.2599210499 1 1.3333333333 0.0734122834 2 1.2638888889 0.0039678390 3 1.2599334934 0.0000124436 4 1.2599210500 0.0000000001 [x_iter,f_iter]=newton_cube_root_nthroot(10,1,1e-10); k x_iter |x^*-x_k| ================================= 0 1.0000000000 1.1544346900 1 4.0000000000 1.8455653100 2 2.8750000000 0.7205653100 3 2.3199432892 0.1655085992 4 2.1659615552 0.0115268651 5 2.1544959252 0.0000612351 6 2.1544346918 0.0000000017 We see that qualitatively, the speed of convergence is a bit faster but quite close to what we have seen in terms of the rate of the function absolute value going down to zero. (c) Comment on the convergence speed and show that it matches the convergence theory presented in the lecture. Make sure to refer to Newton’s method convergence theorem (see the slide with this title in Chapter 2 slides) and conﬁrm that its conditions are satisﬁed. Answer: We look at slide 23 and conﬁrm that since f (x) = x3 − a is a polynomial, then trivially the function is continuous and its two ﬁrst two derivatives are continuous. In addition, f ′(x) = 3x2. So for any x ̸= 0 we have f ′(x) ̸= 0 including for x∗. And of course, we have (x∗)3 − a = 0. (d) Repeat your computations for a = 0, starting with x0 = 0.1. In this case the cube root is, trivially, equal to zero, but Newton’s method converges linearly. Explain why. Make sure to provide a full explanation, and determine the constant ρ (see slide entitled Speed of convergence). Answer: We repeat the calculation and get >> [x_iter,f_iter]=newton_cube_root(0,1,1e-10); k x_iter f_iter ================================= 0 1.0000000000 1.0000000000 1 0.6666666667 0.2962962963 2 0.4444444444 0.0877914952 3 0.2962962963 0.0260122949 4 0.1975308642 0.0077073466 5 0.1316872428 0.0022836583 8 6 0.0877914952 0.0006766395 7 0.0585276635 0.0002004858 8 0.0390184423 0.0000594032 9 0.0260122949 0.0000176009 10 0.0173415299 0.0000052151 11 0.0115610199 0.0000015452 12 0.0077073466 0.0000004578 13 0.0051382311 0.0000001357 14 0.0034254874 0.0000000402 15 0.0022836583 0.0000000119 16 0.0015224388 0.0000000035 17 0.0010149592 0.0000000010 18 0.0006766395 0.0000000003 We see that convergence now is much slower. We can modify the program to print the ratio between all adjacent iterates and immediately see that this ratio is ρ = 2 3 . This is obvious and immediate because the scheme with a = 0 is simply xk+1 = 2 3 xk. Not very interesting! 3. (Bonus question, optional, extra up to 5% of assignment grade) This question provides a glimpse into the pros and cons of higher-order methods. This was a challenging question, and we are giving only a partial solution here. You are not expected to know how to solve such a problem in the exams of the course. (a) Derive a third order method for solving f (x) = 0 in a way similar to the derivation of Newton’s method, using evaluations of f (xk), f ′(xk) and f ′′(xk). To do this, use Taylor’s expansion with three terms plus a remainder term. Show that in the course of derivation a quadratic equation arises, and therefore two distinct schemes can be derived. Use cancellation error prevention considerations to determine the preferred choice of the two. (Here is where our discussion of the numerical solution of a quadratic equation may come handy.) Answer: If x∗ is a zero of the function f (x), then we write the Taylor expansion 0 = f (x ∗) = f (xk) + (x∗ − xk)f ′(xk) + (x∗ − xk)2 2 f ′′(xk) + (x∗ − xk)3 6 f ′′(ξ), where ξ is between x∗ and xk. We now seek xk+1 that allows us to ignore the remainder term, just like we did with the linearization for Newton’s method: 0 = f (xk) + (xk+1 − xk)f ′(xk) + (xk+1 − xk)2 2 f ′′(xk). This is a quadratic equation in zk = xk+1 − xk: 0 = f ′′(xk) 2 z2 k + f ′(xk)zk + f (xk). We solve this quadratic equation for zk and obtain z± k = −f ′(xk) ± √ (f ′(xk))2 − 2f (xk)f ′′(xk) f ′′(xk) , 9 where we assume that f ′′(xk) ̸= 0. We pick the root that would prevent cancellation error, the same way we have learned for the solution of quadratic equations, and proceed from there to determine our scheme: That is, if f ′(xk) is positive then we pick z− k , and vice versa. We need to address a potential issue of a negative discrimininant (expression under the root) so as to avoid generating complex numbers. This can be done in various ways; details are omitted. (b) Show that, under the appropriate conditions, the order of convergence is cubic. Answer: The idea is mathematically identical to the proof we show in the book for standard Newton (and see slide 23 of Chapter 2 in pre-lecture slides for a brief summary), but the details are more involved and are omitted. (c) Estimate the number of iterations and the cost (in terms of function evaluations per iteration) needed to reduce an initial error of, say, 0.1, by a factor of 10−9. Compare this to the number of iterations it would take to reduce the error by the same factor for Newton’s method. It is okay here to just refer to the general theory of speed of convergence, discussed in the lecture. Answer: Cubic convergence means that the powers of the error will triple in every iteration. So, if we start with an error of 0.1 then after one iteration the error will be approximately 10−3 and after two iterations (excluding the initial guess) it will be approximately 10−9. In comparison, for quadractically convergent scheme we will have the error going down like 0.1, 0.01, 10−4, 10−8. So, after three iterations in addition to the initial guess we are still not quite with the error we have within two iterations with the cubically convergent scheme, but close. On the other hand, the cost per iteration of this scheme is signiﬁcantly higher than for Newton’s method: we need to do one function, ﬁrst derivative, and second derivative evaluation, as well as compute a square root. (d) Write a program for solving the problem of question 1 with α = 1, with two initial guesses: starting from x0 = 0.5 once, and then starting from x0 = 2. Stop the iteration when |f (xk)| < 10−10. Answer: Once we have the scheme, the Matlab program is straightforward to write, but longer than Newton’s. It is not included. (e) Can you speculate what makes this method less popular than Newton’s method, despite its cubic convergence? Give two reasons. Answer: First, the scheme requires knowing the second derivative, which is not always available. Secondly, the cost of a single iteration is high and it raises the question whether the overall work is better than Newton’s. The answer to this depends among other factors on how expensive it is to compute the second derivative, and may be function- dependent. Generally speaking, quadratic convergence is quite powerful already, and it seems preferable to perform something like four simple and relatively cheap iterations rather than one or two fewer iterations that are each much more computationally costly. 10","libVersion":"0.2.1","langs":""}