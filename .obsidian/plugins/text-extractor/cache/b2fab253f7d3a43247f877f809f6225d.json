{"path":".obsidian/plugins/text-extractor/cache/b2fab253f7d3a43247f877f809f6225d.json","text":"Solutions of Practice Questions, CPSC 302, 2022 1. (a) z=x.*y; (b) a=x’*y; (c) D=diag(x+y); (d) c=norm(x-y); 2. Advantage: faster convergence. Disadvantage: requires a derivative. 3. (a) Solve f (x) = x3 − b = 0 using Newton’s method: xk+1 = xk − x3 k − b 3x2 k , which simpliﬁes to xk+1 = 2 3 xk + b 3x2 k . (b) Quadratic convergence means that the error roughly doubles in each iteration. If the initial error is approximately 10−1, then in the following iterations it will approximately be 10−2, 10−4, 10−8. So, after three iterations we expect to have approximately the number of correct decimal digits that the calculator’s screen has. (c) For b = 0 we have f (x) = x3 and x∗ = 0 is a multiple root. There- fore, the iteration will converge linearly rather than quadratically in this case, with a contraction factor (asymptotic error constant) equal to 2 3 . This is much slower than quadratic convergence, which is obtained for b ̸= 0. 4. (a) True. We have P 2 = P and therefore the eigenvalues may be 1 or 0, as shown in Assignment 5, question 3(b). Whenever P has a zero eigenvalue it is singular. (b) False. An orthogonal matrix Q satisﬁes QQT = I, and therefore it always has an inverse, which is QT . (c) False. A matrix A is nonsingular if and only if the linear system Ax = 0 only has the zero solution. Suppose now that A is singu- lar. Then there is x ̸= 0 such that Ax = 0. But this means that xT Ax = 0 for x ̸= 0, which is a contradiction with the deﬁnition of positive deﬁniteness. We can instead use a simpler eigenvalue argument: all eigenvalues of A are positive and therefore it is nonsingular. (d) True. Take a tridiagonal matrix all of whose row sums are zero, or a tridiagonal matrix with a row of zeros. It is still tridiagonal by deﬁnition, but singular. (e) True. Take a lower triangular matrix with a zero diagonal ele- ment. 5. (a) It interchanges rows i and j of A. (b) It interchanges columns i and j of A. (c) P is an orthogonal matrix. (d) If Ax = b then since P T P = I as per part (c), P A(P T P )x = P b and therefore ˜x = P x. (e) Permutation matrices are used in Gaussian elimination with par- tial pivoting. (f) We only need O(n) integer entries. For example, P =  1 0 0 0 0 1 0 1 0   can be represented as a vector p = [1, 3, 2] and the matrix oper- ations with P can be done by using p. 6. Replace the last equation, because replacing the ﬁrst equation will cause massive ﬁll-in, and U will be a dense tridiagonal matrix. Re- placing the last equation does not cause additional ﬁll-in. See the discussion of arrow matrices in the slides and the textbook. 7. Advantage: signiﬁcantly faster for a large class of matrices. Disadvantage: requires knowing an optimal parameter. 8. (a) Here M = I, and therefore T = I − A. (b) The eigenvalues of T are 1−λi, i = 1, 2, . . . , n. We need ρ(T ) < 1 which translates into max |1 − λi| < 1. We thus require −1 < 1 − λi < 1. Since all λi are positive, we have that the condition on the right of the inequality is always satisﬁed and the condition on the left is satisﬁed if 0 < λi < 2 for all i. From that it follows that if λ1 < 2 then we have convergence. 9. (a) We need to compute the LU decomposition of A−µI once, before we started iterating. (b) We need to perform backward and forward substitutions k times each. (c) O(n) ﬂoating point operations, because both the decomposition and the backward and forward solves are linear in n for a tridiag- onal matrix, and the number of iterations is constant with respect to n. 10. (a) We have (Q1Q2) −1 = (Q2)−1(Q1) −1 = QT 2 Q T 1 = (Q1Q2) T . (b) Yes, they do, because if A = U ΣV T , then QAQT = (QU )Σ(V T QT ), and both QU and V T QT are orthogonal as per (a), therefore this is a valid SVD of the matrix QAQT , and Σ still represents its singular values. (c) Q is orthogonal and therefore κ2(Q) = 1. We have κ2(A) = ∥A∥∥A−1∥ = ∥QR∥∥(QR)−1∥ = ∥QR∥∥R−1Q T ∥ = ∥R∥∥R−1∥ = κ2(R). Therefore κ2(R) = M .","libVersion":"0.2.1","langs":""}