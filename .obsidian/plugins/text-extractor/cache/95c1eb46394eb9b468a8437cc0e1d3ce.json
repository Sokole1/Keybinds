{"path":".obsidian/plugins/text-extractor/cache/95c1eb46394eb9b468a8437cc0e1d3ce.json","text":"Question 2. (77 points) Answer the questions below. Be concise: avoid spending valuable time on lengthy answers. (a) Consider an ensemble clustering method that generates m different boostraps of the data. It then fits a k-means model (with a random initialization) to each of the boostraps. To form the final clustering for example z;, it chooses the y; that is most common across the m clusterings. Would this be an effective or an inneffective ensemble method? (b) Suppose we have a supervised learning problem where we think the examples z; form clusters. To deal with this, we combine our training and test data together and fit a k-means classifier. We then add the cluster number as an extra feature, fit our supervised learning model based on the training data, then evaluate it on the test data. What have we done wrong? (c) What is the advantage of trying k-means with several different initializations? Should we run DBSCAN (density-based clustering) with several different initializations?","libVersion":"0.2.1","langs":"eng"}