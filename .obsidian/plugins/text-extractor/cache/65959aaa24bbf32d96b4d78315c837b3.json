{"path":".obsidian/plugins/text-extractor/cache/65959aaa24bbf32d96b4d78315c837b3.json","text":"2.5 L} regularization [8 points] Proviously we've considered L2- and L1 regularization which use the L2 and L1 norms respectively. Now consider last squares lieas regresion with “L1} regularization” (i quotation marks because the “L} norm” is ot rue norm). L5, 2 S 12 fu) = 3 5w+ Al Let's considr the case of d = 1 and assume thee i no intercept. term being usd, 50 the loss simplifics to 14 2 )= 3w - + W Finally, let’s assume the very special case of n = 2, where our 2 data points are (zy,y1) = (1,2) and (@) = (0,1). 1. Plug in the dataset. values and it the loss in a simpliied form, without a Awswer:[(w) = § T (we — )? + Ayl 2,16 =0, what s the solution, L. argmin, /(u)? 3. 162 - oo, what i the soluion, e, arg min /(u)7 4. Plot f(w) when A = 1. What is argmin,, f(w) when X = 17 Answer to one decimal place if appropriate. (For the plotting questions, you can use natplot1ib or any graphing software, such as doamos. con) 5. Plot f(u) when X = 10. What is argmin, f(w) when ) = 107 Answer to one decimal place if appropriate. 6. Docs L} regularization behave more like L1 regularization or L2 regulasiation when it comes to performing leature selection? Briely justfy your answer. 7. Is st squares with L segulacizaton a conves optimization problem? Bielly Justiy your answer","libVersion":"0.2.1","langs":"eng"}