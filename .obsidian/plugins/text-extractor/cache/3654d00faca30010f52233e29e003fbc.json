{"path":".obsidian/plugins/text-extractor/cache/3654d00faca30010f52233e29e003fbc.json","text":"CPSC 340: Machine Learning and Data Mining Feature Engineering Last Time: Multi-Class Linear Classifiers â€¢ We discussed multi-class classification: yi in {1,2,â€¦,k}. â€¢ One vs. all with +1/-1 binary classifier: â€“ Train weights wc to predict +1 for class â€˜câ€™, -1 otherwise. â€“ Predict by taking â€˜câ€™ maximizing class output oic = wcTxi. â€¢ Multi-class SVMs and multi-class logistic regression: â€“ Train the wc jointly to encourage maximum oic to be oi . Shape of Decision Boundaries â€¢ Recall that a binary linear classifier splits space using a hyper-plane: â€¢ Divides xi space into 2 â€œhalf-spacesâ€. Shape of Decision Boundaries â€¢ Multi-class linear classifier is intersection of these â€œhalf-spacesâ€: â€“ This divides the space into convex regions (like k-means): Shape of Decision Boundaries â€¢ Multi-class linear classifier is intersection of these â€œhalf-spacesâ€: â€“ Though regions could be non-convex with non-linear feature transforms: Next Topic: Feature Engineering Feature Engineering â€¢ â€œâ€¦some machine learning projects succeed and some fail. What makes the difference? Easily the most important factor is the features used.â€ â€“ Pedro Domingos â€¢ â€œComing up with features is difficult, time-consuming, requires expert knowledge. \"Applied machine learning\" is basically feature engineering.â€ â€“ Andrew Ng Feature Engineering â€¢ Better features usually help more than a better model. â€¢ Good features would ideally: â€“ Allow learning with few examples, be hard to overfit with many examples. â€“ Capture most important aspects of problem. â€“ Reflects invariances (generalize to new scenarios). â€¢ There is a trade-off between simple and expressive features: â€“ With simple features overfitting risk is low, but accuracy might be low. â€“ With complicated features accuracy can be high, but so is overfitting risk. Feature Engineering â€¢ The best features may be dependent on the model you use. â€¢ For counting-based methods like naÃ¯ve Bayes and decision trees: â€“ Need to address coupon collecting, but separate relevant â€œgroupsâ€. â€¢ For distance-based methods like KNN: â€“ Want different class labels to be â€œfarâ€. â€¢ For regression-based methods like linear regression: â€“ Want labels to have a linear dependency on features. Discretization for Counting-Based Methods â€¢ For counting-based methods: â€“ Discretization: turn continuous into discrete. â€“ Counting age â€œgroupsâ€ could let us learn more quickly than exact ages. â€¢ But we wouldnâ€™t do this for a distance-based method. Age 23 23 22 25 19 22 < 20 >= 20, < 25 >= 25 0 1 0 0 1 0 0 1 0 0 0 1 1 0 0 0 1 0 Standardization for Distance-Based Methods â€¢ Consider features with different scales: â€¢ Should we convert to some standard â€˜unitâ€™? â€“ It doesnâ€™t matter for counting-based methods. â€¢ It matters for distance-based methods: â€¢ KNN will focus on large values more than small values. â€¢ Often we â€œstandardizeâ€ scales of different variables (e.g., convert everything to grams). â€¢ Also need to worry about correlated features. Egg (#) Milk (mL) Fish (g) Pasta (cups) 0 250 0 1 1 250 200 1 0 0 0 0.5 2 250 150 0 Non-Linear Transformations for Regression-Based â€¢ Non-linear feature/label transforms can make things more linear: â€“ Polynomial, exponential/logarithm, sines/cosines, RBFs. www.google.com/finance Domain-Specific Transformations â€¢ In some domains there are natural transformations to do: â€“ Fourier coefficients and spectrograms (sound data). â€“ Wavelets (image data). â€“ Convolutions (weâ€™ll talk about these later). https://en.wikipedia.org/wiki/Fourier_transform https://en.wikipedia.org/wiki/Spectrogram https://en.wikipedia.org/wiki/Discrete_wavelet_transform Discussion of Feature Engineering â€¢ The best feature transformations are application-dependent. â€“ Itâ€™s hard to give general advice. â€¢ My advice: ask the domain experts. â€“ Often have idea of right discretization/standardization/transformation. â€¢ If no domain expert, cross-validation will help. â€“ Or if you have lots of data, use deep learning methods from Part 5. â€¢ Next: Iâ€™ll give some features used for text applications. Next Topic: Features for Text Data But firstâ€¦ â€¢ How do we use categorical features in regression? â€¢ Standard approach is to convert to a set of binary features: â€“ â€œ1 of kâ€ or â€œone hotâ€ encoding. â€“ What if you get a new city in the test data? â€¢ Common approach: set all three variables to 0. Age City Income 23 Van 22,000.00 23 Bur 21,000.00 22 Van 0.00 25 Sur 57,000.00 19 Bur 13,500.00 22 Van 20,000.00 Age Van Bur Sur Income 23 1 0 0 22,000.00 23 0 1 0 21,000.00 22 1 0 0 0.00 25 0 0 1 57,000.00 19 0 1 0 13,500.00 22 1 0 0 20,000.00 Digression: Linear Models with Binary Features â€¢ What is the effect of a binary features on linear regression? â€¢ Suppose we use a bag of words: â€“ With 3 words {â€œhelloâ€, â€œVicodinâ€, â€œ340â€œ} our model would be: â€“ If e-mail only has â€œhelloâ€ and â€œ340â€ our prediction is: â€¢ So having the binary feature â€˜jâ€™ increases à·œğ‘¦i by the fixed amount wj. â€“ Predictions are a bit like naÃ¯ve Bayes where we combine features independently. â€“ But now weâ€™re learning all wj together so this tends to work better. Text Example 1: Language Identification â€¢ Consider data that doesnâ€™t look like this: â€¢ But instead looks like this: â€¢ How should we represent sentences using features? A (Bad) Universal Representation â€¢ Treat character in position â€˜jâ€™ of the sentence as a categorical feature. â€¢ â€œfais ce que tu veuxâ€ => xi = [f a i s â€˜â€™ c e â€˜â€™ q u e â€˜â€™ t u â€˜â€™ v e u x .] â€¢ â€œPadâ€ end of the sentence up to maximum #characters: â€¢ â€œfais ce que tu veuxâ€ => xi = [f a i s â€˜â€™ c e â€˜â€™ q u e â€˜â€™ t u â€˜â€™ v e u x . Î³ Î³ Î³ Î³ Î³ Î³ Î³ Î³ â€¦] â€¢ Advantage: â€“ No information is lost, KNN can eventually solve the problem. â€¢ Disadvantage: throws out everything we know about language. â€“ Needs to learn that â€œveuxâ€ starting from any position indicates â€œFrenchâ€. â€¢ Doesnâ€™t even use that sentences are made of words (this must be learned). â€“ High overfitting risk, you will need a lot of examples for this easy task. Bag of Words Representation â€¢ Bag of words represents sentences/documents by word counts: â€¢ Bag of words loses a ton of information/meaning: â€“ But it easily solves language identification problem The International Conference on Machine Learning (ICML) is the leading international academic conference in machine learning ICML International Conference Machine Learning Leading Academic 1 2 2 2 2 1 1 Universal Representation vs. Bag of Words â€¢ Why is bag of words better than â€œstring of charactersâ€ here? â€“ It needs less data because it captures invariances for the task: â€¢ Most features give strong indication of one language or the other. â€¢ It doesnâ€™t matter where the French words appear. â€“ It overfits less because it throws away irrelevant information. â€¢ Exact sequence of words isnâ€™t particularly relevant here. Text Example 2: Word Sense Disambiguation â€¢ Consider the following two sentences: â€“ â€œThe cat ran after the mouse.â€ â€“ â€œMove the mouse cursor to the File menu.â€ â€¢ Word sense disambiguation (WSD): classify â€œmeaningâ€ of a word: â€“ A surprisingly difficult task. â€¢ You can do ok with bag of words, but it will have problems: â€“ â€œHer mouse clicked on one cat video after another.â€ â€“ â€œWe saw the mouse run out from behind the computer.â€ â€“ â€œThe mouse was gray.â€ (ambiguous without more context) Bigrams and Trigrams â€¢ A bigram is an ordered set of two words: â€“ Like â€œcomputer mouseâ€ or â€œmouse ranâ€. â€¢ A trigram is an ordered set of three words: â€“ Like â€œcat and mouseâ€ or â€œclicked mouse onâ€. â€¢ These give more context/meaning than bag of words: â€“ Includes neighbouring words as well as order of words. â€“ Trigrams are widely-used for various language tasks. â€¢ General case is called n-gram. â€“ Unfortunately, coupon collecting becomes a problem with larger â€˜nâ€™. Text Example 3: Part of Speech (POS) Tagging â€¢ Consider problem of finding the verb in a sentence: â€“ â€œThe 340 students jumped at the chance to hear about POS features.â€ â€¢ Part of speech (POS) tagging is the problem of labeling all words. â€“ >40 common syntactic POS tags. â€“ Current systems have ~97% accuracy on standard (â€œcleanâ€) test sets. â€“ You can achieve this by applying a â€œword-levelâ€ classifier to each word. â€¢ That independently classifies each word with one of the 40 tags. â€¢ What features of a word should we use for POS tagging? POS Features â€¢ Regularized multi-class logistic regression with these features gives ~97% accuracy: â€“ Categorical features whose domain is all words (â€œlexicalâ€ features): â€¢ The word (e.g., â€œjumpedâ€ is usually a verb). â€¢ The previous word (e.g., â€œheâ€ hit vs. â€œaâ€ hit). â€¢ The previous previous word. â€¢ The next word. â€¢ The next next word. â€“ Categorical features whose domain is combinations of letters (â€œstemâ€ features): â€¢ Prefix of length 1 (â€œwhat letter does the word start with?â€) â€¢ Prefix of length 2. â€¢ Prefix of length 3. â€¢ Prefix of length 4 (â€œdoes it start with JUMP?â€) â€¢ Suffix of length 1. â€¢ Suffix of length 2. â€¢ Suffix of length 3 (â€œdoes it end in ING?â€) â€¢ Suffix of length 4. â€“ Binary features (â€œshapeâ€ features): â€¢ Does word contain a number? â€¢ Does word contain a capital? â€¢ Does word contain a hyphen? â€¢ Total number of features: ~2 million (same accuracy with ~10 thousand using L1-regularization). Ordinal Features â€¢ Categorical features with an ordering are called ordinal features. â€¢ If using decision trees, makes sense to replace with numbers. â€“ Captures ordering between the ratings. â€“ A rule like (rating â‰¥ 3) means (rating â‰¥ Good), which make sense. Rating Bad Very Good Good Good Very Bad Good Medium Rating 2 5 4 4 1 4 3 Ordinal Features â€¢ With linear models, â€œconvert to numberâ€ assumes ratings are equally spaced. â€“ â€œBadâ€ and â€œMediumâ€ distance is similar to â€œGoodâ€ and â€œVery Goodâ€ distance. â€¢ One alternative that preserves ordering with binary features: â€¢ Regression weight wmedium represents: â€“ â€œHow much medium changes prediction over badâ€. â€¢ Bonus slides discuss â€œcyclicâ€ features like â€œtime of dayâ€. Rating Bad Very Good Good Good Very Bad Good Medium â‰¥ Bad â‰¥ Medium â‰¥ Good Very Good 1 0 0 0 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 0 1 1 1 0 1 1 0 0 Next Topic: Personalized Features Motivation: â€œPersonalizedâ€ Important E-mails â€¢ Features: bag of words, trigrams, regular expressions, and so on. â€¢ There might be some â€œgloballyâ€ important messages: â€“ â€œThis is your mother, something terrible happened, give me a call ASAP.â€ â€¢ But your â€œimportantâ€ message may be unimportant to others. â€“ Similar for spam: â€œspamâ€ for one user could be â€œnot spamâ€ for another. â€œGlobalâ€ and â€œLocalâ€ Features â€¢ Consider the following weird feature transformation: â€¢ First feature: did â€œ340â€ appear in this e-mail? â€¢ Second feature: if â€œ340â€ appeared in this e-mail, who was it addressed to? â€¢ First feature will increase/decrease importance of â€œ340â€ for every user (including new users). â€¢ Second (categorical feature) increases/decreases importance of â€œ340â€ for a specific user. â€“ Lets us learn more about specific users where we have a lot of data â€œ340â€ (any user) â€œ340â€ (user?) 1 User 1 1 User 1 1 User 2 0 <no â€œ340â€> 1 User 3 â€œ340â€ 1 1 1 0 1 â€œGlobalâ€ and â€œLocalâ€ Features â€¢ Recall we usually represent categorical features using â€œ1 of kâ€ binaries: â€¢ First feature â€œmoves the line upâ€ for all users. â€¢ Second feature â€œmoves the line upâ€ when the e-mail is to user 1. â€¢ Third feature â€œmoves the line upâ€ when the e-mail is to user 2. â€œ340â€ (any user) â€œ340â€ (user = 1) â€œ340â€ (user = 2) 1 1 0 1 1 0 1 0 1 0 0 0 1 0 0 â€œ340â€ 1 1 1 0 1 The Big Global/Local Feature Table for E-mails â€¢ Each row is one e-mail (there are lots of rows): Predicting Importance of E-mail For New User â€¢ Consider a new user: â€“ We start out with no information about them. â€“ So we use global features to predict what is important to a generic user. â€“ Weights on local/user features are initialized to zero. â€¢ With more data, update global features and userâ€™s local features: â€“ Local features make prediction personalized. â€“ What is important to this user? â€¢ G-mail system: classification with logistic regression. â€“ Trained with a variant of stochastic gradient descent (later). Summary â€¢ Feature engineering can be a key factor affecting performance. â€“ Good features depend on the task and the model. â€¢ Bag of words: not a good representation in general. â€“ But good features if word order isnâ€™t needed to solve problem. â€¢ Trigram features: try to capture local context. â€¢ Text features (beyond bag of words): trigrams, lexical, stem, shape. â€“ Try to capture important invariances in text data. â€¢ Global vs. local features allow â€œpersonalizedâ€ predictions. â€¢ Next time: â€“ A trick that lets you find gold and use the polynomial basis with d > 1. â€œAll-Pairsâ€ and ECOC Classification â€¢ Alternative to â€œone vs. allâ€ to convert binary classifier to multi-class is â€œall pairsâ€. â€“ For each pair of labels â€˜câ€™ and â€˜dâ€™, fit a classifier that predicts +1 for examples of class â€˜câ€™ and -1 for examples of class â€˜dâ€™ (so each classifier only trains on examples from two classes). â€“ To make prediction, take a vote of how many of the (â„“-1) classifiers for class â€˜câ€™ predict +1. â€“ Often works better than â€œone vs. allâ€, but not so fun for large â€˜â„“â€™. â€¢ Need O(â„“2) classifiers. â€¢ A variation on this is using â€œerror correcting output codesâ€ from information theory (see Math 342). â€“ Each classifier trains to predict +1 for some of the classes and -1 for others. â€“ You setup the +1/-1 code so that it has an â€œerror correctingâ€ property. â€¢ It will make the right decision even if some of the classifiers are wrong. Motivation: Dog Image Classification â€¢ Suppose weâ€™re classifying images of dogs into breeds: â€¢ What if we have images where class label isnâ€™t obvious? â€“ Syberian husky vs. Inuit dog? https://www.slideshare.net/angjoo/dog-breed-classification-using-part-localization https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements Learning with Preferences â€¢ Do we need to throw out images where label is ambiguous? â€“ We donâ€™t have the yi. â€“ We want classifier to prefer Syberian husky over bulldog, Chihuahua, etc. â€¢ Even though we donâ€™t know if these are Syberian huskies or Inuit dogs. â€“ Can we design a loss that enforces preferences rather than â€œtrueâ€ labels? https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements Learning with Pairwise Preferences (Ranking) â€¢ Instead of yi, weâ€™re given list of (c1,c2) preferences for each â€˜iâ€™: â€¢ Multi-class classification is special case of choosing (yi,c) for all â€˜câ€™. â€¢ By following the earlier steps, we can get objectives for this setting: https://ischlag.github.io/2016/04/05/important-ILSVRC-achievements Learning with Pairwise Preferences (Ranking) â€¢ Pairwise preferences for computer graphics: â€“ We have a smoke simulator, with several parameters: â€“ Donâ€™t know what the optimal parameters are, but we can ask the artist: â€¢ â€œWhich one looks more like smokeâ€? https://circle.ubc.ca/bitstream/handle/2429/30519/ubc_2011_spring_brochu_eric.pdf?sequence=3 Learning with Pairwise Preferences (Ranking) â€¢ Pairwise preferences for humour: â€“ New Yorker caption contest: â€“ â€œWhich one is funnierâ€? https://homes.cs.washington.edu/~jamieson/resources/next.pdf Risk Scores â€¢ In medicine/law/finance, risk scores are sometimes used to give probabilities: â€“ Get integer-valued â€œpointsâ€ for each â€œrisk factorâ€, and probability is computed from data based on people with same number of points. â€“ Less accurate than fancy models, but interpretable and can be done by hand. â€¢ Some work on trying to â€œlearnâ€ the whole thing (like doing feature selection then rounding). https://arxiv.org/pdf/1610.00168.pdf","libVersion":"0.2.1","langs":""}