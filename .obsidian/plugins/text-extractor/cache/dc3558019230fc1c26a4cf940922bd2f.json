{"path":".obsidian/plugins/text-extractor/cache/dc3558019230fc1c26a4cf940922bd2f.json","text":"UBC CPSC 340 / CPSC 532M 2018W1 FINAL EXAM December 13th, 2018 Instructors: Mark Schmidt and Mike Gelbart TIME: 150 minutes Name: Student number: CS ugrad id: Signature: By signing above, I hereby acknowledge that I did not / will not cheat on this exam. • Do not open the exam until you are directed to do so. • Once you open the exam, make sure that it contains this cover page plus 15 additional pages. • Two letter-size sheets (both sides) of notes are allowed. No other material or accessories may be used. • Please be prepared to present, upon request, a student card for identiﬁcation. • If you need more space, use the blank page at the end of the exam, and clearly indicate that your work continues there. • Most questions require a short answer. Work eﬃciently and avoid writing lengthy answers. • If anything is unclear or seems ambiguous, state your assumptions. Question: 1 2 3 4 5 6 7 8 9 10 Total Points: 20 20 6 8 6 12 6 6 10 6 100 Score: G O O D L U C K !! CPSC 340 2018W1 Final Exam REFERENCE INFORMATION — PLEASE READ! Potentially relevant acronyms • CNN: convolutional neural network • DBSCAN: density-based spatial clustering of applications with noise • KNN: k-nearest neighbours • MAP: maximum a posteriori • MDS: multi-dimensional scaling • MLE: maximum likelihood estimation • NMF: non-negative matrix factorization • OLS: ordinary least squares linear regression • PCA: principal component analysis • RBF: radial basis function • ReLU: rectiﬁed linear unit • SGD: stochastic gradient descent • SVD: singular value decomposition • SVM: support-vector machine Potentially relevant conventions • Unless otherwise stated, n refers to the number of training examples and d is the number of features. We will use k in a context-dependent way: it may mean the number of classes, or the number of latent factors, or the number of clusters, etc. Hopefully the meaning is clear from the context. • Unless otherwise speciﬁed, the norm of a vector, written as ||x||, refers to the L2-norm. • The notation ||X||F refers to the Frobenius norm of the matrix X, which is the square root of the sum of the squares of all the elements of X. • The phrase “fully-connected neural network” refers to a regular neural network as originally introduced in the course. This phrasing is using to distinguish the model from a convolutional neural network. Page 1 of 15 CPSC 340 2018W1 Final Exam Question 1. (20 points) Circle all that apply. There may be zero correct choices or multiple correct choices, not necessarily just one!! No need to justify your answers. (a)7 pts Which of the following methods are examples of supervised learning? i. KNN classiﬁcation ii. KNN regression iii. k-means clustering iv. ISOMAP v. kernel logistic regression vi. robust PCA vii. convolutional neural networks (b)6 pts Which of the following methods are non-parametric? i. OLS with degree-10 polynomial features ii. RBF SVM iii. robust PCA iv. MDS v. convolutional neural networks vi. decision trees with no depth limit (c)7 pts Which of the following loss functions are convex? i. f (w) = − ∑d i=1 wi log wi ii. f (w) = ∥Xw − y∥1 + λ∥w∥1 iii. f (w) = ∥Xw − y∥2 + λ1∥w∥1 + λ2∥w∥2 iv. f (w) = ∑n i=1 max{0, yiwT xi} v. f (w) = ∑n i=1 max{0, 1 − yiwT xi} + λ||w||0 vi. the MDS loss vii. a neural network with one hidden layer, ReLU activations, and the squared loss Page 2 of 15 CPSC 340 2018W1 Final Exam Question 2. (20 points) Circle all that apply. There may be zero correct choices or multiple correct choices, not necessarily just one!! No need to justify your answers. (a)20 pts Which of the following changes would typically reduce training error? Note: For unsupervised learning algorithms, “reducing training error” means reducing the loss. Also, for the neural network questions, assume you always reach a global minimum of the loss. i. using the faster decision tree algorithm based on sorting, instead of the naive approach ii. increasing the amount of Laplace smoothing (higher β parameter) for naive Bayes iii. using a smaller k with KNN iv. increasing the maximum tree depth in a random forest v. increasing k in k-means clustering vi. increasing λ in regularized linear regression vii. ﬁtting OLS with gradient descent instead of the normal equations viii. running more iterations of gradient descent (assuming α is small enough) ix. using a higher degree polynomial basis for linear regression x. removing some irrelevant features from your model xi. removing some relevant features from your model xii. ﬁtting logistic regression with SGD instead of gradient descent xiii. increasing the width, σ, of the RBF kernel xiv. increasing the variance of the prior in the MLE/MAP view of a model xv. ﬁtting PCA with gradient descent instead of SVD xvi. removing the orthogonality constraint from PCA xvii. increasing k in NMF xviii. adding another layer to a neural network xix. increasing the hidden layer size in a 1-hidden-layer neural network xx. using larger ﬁlters in a CNN Page 3 of 15 CPSC 340 2018W1 Final Exam Question 3. (6 points) Brieﬂy describe the two major problems with the following pseudocode for k-fold cross-validation: function cross_val_score(model, X, y, k): shuffle the rows of X divide the data into k equal-size folds, store in X_folds, y_folds for i from 1 to k: train model using X and y predict on X_folds[i] compare predictions to y_folds[i], compute scores[i] return average of scores Page 4 of 15 CPSC 340 2018W1 Final Exam Question 4. (8 points) For each of the following methods, how many trainable parameters does the model have? In other words, how many values need to be stored after training in order to make predictions? Your answers may involve n, d, and perhaps additional quantities deﬁned in the question. Ex- press your answer in big-O notation. As an example, (linear) least squares has O(d) parameters. (a)2 pts naive Bayes with binary features, k classes, and Laplace smoothing strength β. (b)2 pts linear SVM with regularization strength λ. (c)2 pts linear regression with d = 1 using a polynomial basis of degree p. (d)2 pts a fully-connected neural network for k-class classiﬁcation, with one hidden layer of size m. Page 5 of 15 CPSC 340 2018W1 Final Exam Question 5. (6 points) Short answer: 2 sentences maximum for each answer. Be careful not to use up too much of your valuable time writing lengthy answers. (a)2 pts Do you expect the resulting loss from running NMF to be lower, higher, or the same as the loss from running PCA on the same data? Brieﬂy justify your answer. (b)2 pts Is L2-regularized logistic regression aﬀected by standardizing the features? Brieﬂy justify your answer. (c)2 pts What does minimizing f (w) = ||Xw −y||1 +λ||w||2 correspond to in the MLE/MAP view? (Your answer can be a description or an actual mathematical formula; either is OK.) Page 6 of 15 CPSC 340 2018W1 Final Exam Question 6. (12 points) You run PCA with k = 2 on some (already centred) data set X (not shown) and arrive at Z =     10 10 2 −2 4 3 4 3     , W = [0.5 0.5 −0.5 −0.5 0.7 0.1 0.7 0.1 ] . Assume these results were generated with the standard implementation of PCA using SVD, where the principal components are orthonormal and ordered. (a)2 pts What is the ﬁrst element of the reconstructed input matrix, namely ˆx11? (b)2 pts If you had instead used k = 1, what would Z and W be? (Note: use the original k = 2 matrices for the remaining parts.) (c)2 pts The third and fourth rows of Z (above) are identical. Does this mean that the third and fourth rows of the training data matrix, X, are also identical (to each other, not to Z)? Brieﬂy justify your answer. Page 7 of 15 CPSC 340 2018W1 Final Exam (d)2 pts Why can we be conﬁdent that the variance explained is at least 50%? (e)2 pts Imagine you swapped the ﬁrst and second columns of the original data matrix X. How would this change Z and W ? Express your answer by writing down a possible Z and W that could have resulted from running PCA on this new data set. (I say “a possible” solution because the solution to PCA is not unique.) (f)2 pts If I told you the above Z and W actually came from the L1-regularized variant of PCA, would you guess the λ used was large or small? Brieﬂy explain your reasoning. Page 8 of 15 CPSC 340 2018W1 Final Exam Question 7. (6 points) Consider performing regression on a data set with d = 1. Match the models below to the plots showing the learned regression curves by writing the corresponding Roman numeral (i,ii,iii,iv,v,vi) next to each model name. Each numeral should be used exactly once. (a)1 pt OLS (b)1 pt L2-regularized linear regression (a.k.a. Ridge regression) (c)1 pt L2-regularized RBF kernel regression with larger σ (larger “width”) (d)1 pt L2-regularized RBF kernel regression with smaller σ (smaller “width”) (e)1 pt L2-regularized polynomial basis regression with p = 20 (f)1 pt KNN regression with k = 3 (predicts the average y-value of the 3 nearest points) 2 0 2 0 2 4 i 2 0 2 0 2 4 ii 2 0 2 0 5 10 iii 2 0 2 0 2 4 iv 2 0 2 0 2 4 v 2 0 2 0 2 4 vi Page 9 of 15 CPSC 340 2018W1 Final Exam Question 8. (6 points) Consider performing 3-class classiﬁcation on a data set with d = 2. Match the models below to the plots showing the learned decision to by writing the corresponding Roman numeral (i,ii,iii,iv,v,vi) next to each model name. Each numeral should be used exactly once. (a)1 pt L2-regularized logistic regression (b)1 pt RBF SVM with a small λ (c)1 pt KNN with k = 1 (d)1 pt neural network with ReLU activations and one hidden layer with k = 2 (e)1 pt decision tree, no depth restrictions (f)1 pt random forest but with only one tree, no depth restrictions i ii iii iv v vi Page 10 of 15 CPSC 340 2018W1 Final Exam Question 9. (10 points) Consider the ﬁgures below. Figure i (left) shows a data set with d = 1. Figure ii (right) shows the contours of the least squares loss for a linear regression model on this data set, given by f (w, w0) = 1 2 n∑ i=1(wxi + w0 − yi)2 . The point (2, 2) in parameter space is indicated with a star. Note: you may be used to seeing the loss written as f (w) = 1 2 ∑n i=1(wT xi − yi)2, but for d = 1 we can write the loss as above where w, w0, xi and yi are all scalars. The parameter w0 is the intercept or bias term. 2 0 2 x 1 0 1 2y i The data set 1 0 1 2 3 w 1 0 1 2 3w05.0 10.0 20.0 30.0 50.0 100.0100.0 150.0 150.0 ii Contours of the squared error loss (a)2 pts On Figure i (left) draw the line corresponding to the star’s location in parameter space. (b)2 pts Is the star at the global minimum of the least squares loss? Brieﬂy justify your answer. (c)2 pts Is the plausible that the star is at the global minimum of the L2-regularized least squares problem, f (w, w0) = 1 2 ∑n i=1(wxi + w0 − yi)2 + λ 2 w2? Brieﬂy justify your answer. Page 11 of 15 CPSC 340 2018W1 Final Exam (d)2 pts Consider a variation on L2-regularized least squares where we have weights on the training examples and within the regularizer, with an objective of the form f (w) = 1 2 (Xw − y) T Z(Xw − y) + λ 2 ∥Aw∥2. Here, Z is an n × n diagonal matrix with non-negative values zi along the diagonals and A is a k × d matrix. Write a linear system whose solution gives the minimum of this (convex quadratic) objective function. (e)2 pts Consider a function P (r) that takes an n × 1 vector r and returns an n × 1 vector u, where ui = ri if ri is positive and ui = 0 otherwise. Use this function to write the hinge loss with weighted L1-regularization, f (w) = n∑ i=1 max{0, 1 − yiwT xi} + λ d∑ j=1 vj|wj|, in matrix and norm notation. You may ﬁnd it helpful to use Y as a diagonal matrix with the yi along the diagonal, V as a diagonal matrix with the non-negative values vj along the diagonal, and 1 as a vector containing all ones. Page 12 of 15 CPSC 340 2018W1 Final Exam Question 10. (6 points) The loss function for L2-regularized logistic regression is f (w) = n∑ i=1[log(1 + exp(−yiwT xi))] + λ 2 ∥w∥2 . The gradient of the objective function can be written in the form ∇f (w) = n∑ i=1[g(xi, w)xi + λ n w] for some scalar-valued function g(xi, w). In general, computing g costs O(d). (a)2 pts What is the cost of performing an iteration of stochastic gradient in terms of n and d? Justify your answer. Note: you can assume that generating a random number between 1 and n costs O(1). (b)2 pts It turns out that if xi has only m non-zero values, the cost of computing g is O(m). What is the cost of performing an iteration of stochastic gradient if each xi has at most m non-zero values? Page 13 of 15 CPSC 340 2018W1 Final Exam (c)2 pts It turns out we can reduce the cost in part (b) by representing w as the product of a scalar β and a vector v, so that w = βv. At each iteration we can update β and then update v, in a way that corresponds to the desired update to w. What are these update rules, and the reduced cost? Note that: • Multiplying a vector with m non-zeroes by a scalar costs O(m). • Adding a vector with m non-zeroes to a dense vector also costs O(m). Page 14 of 15 CPSC 340 2018W1 Final Exam This page is intentionally blank. You can use it for scratch work or to continue an answer if you run out of space somewhere. End of exam Page 15 of 15 End of exam","libVersion":"0.2.1","langs":""}