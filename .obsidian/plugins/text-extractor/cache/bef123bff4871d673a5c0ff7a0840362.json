{"path":".obsidian/plugins/text-extractor/cache/bef123bff4871d673a5c0ff7a0840362.json","text":"CPSC 340: Machine Learning and Data Mining Multi-Dimensional Scaling Last Time: Recommende Systems â€¢ We discussed recommender systems: â€“ Predicting what ratings users have for different products. â€¢ We mentioned content-based filtering: â€“ Formulates recommendation as supervised learning. â€¢ Extract features of users and products, and use these to predict rating. â€¢ We introduced collaborative filtering: â€“ Methods that only looks at ratings, not features of products. â€¢ Looks more like unsupervised learning. â€¢ Most common approach to solve this problem is â€œmatrix factorizationâ€ (today). â€“ Performed surprisingly well in Netflix competition and has been adopted by companies. Matrix Factorization for Collaborative Filtering â€¢ The standard latent-factor model for entries in matrix â€˜Yâ€™: â€¢ User â€˜iâ€™ has latent features zi. â€¢ Movie â€˜jâ€™ has latent features wj. â€¢ Our loss functions sums over available ratings â€˜Râ€™: â€¢ And we add L2-regularization to both types of features. â€“ Basically, this is regularized PCA on the available entries of Y. â€“ Typically fit with SGD. â€¢ This simple method gives you a 7% improvement on the Netflix problem. Adding Global/User/Movie Biases â€¢ Our standard latent-factor model for entries in matrix â€˜Yâ€™: â€¢ Sometimes we donâ€™t assume the yij have a mean of zero: â€“ We could add bias Î² reflecting average overall rating: â€“ We could also add a user-specific bias Î²i and item-specific bias Î²j. â€¢ Some users rate things higher on average, and movies are rated better on average. â€¢ These might also be regularized. Beyond Accuracy in Recommender Systems â€¢ Winning system of Netflix Challenge was never adopted. â€¢ Other issues important in recommender systems: â€“ Diversity: how different are the recommendations? â€¢ If you like â€˜Battle of Five Armies Extended Editionâ€™, recommend Battle of Five Armies? â€¢ Even if you really really like Star Wars, you might want non-Star-Wars suggestions. â€“ Persistence: how long should recommendations last? â€¢ If you keep not clicking on â€˜Hunger Gamesâ€™, should it remain a recommendation? â€“ Trust: tell user why you made a recommendation. â€¢ Quora gives explanations for recommendations. â€“ Social recommendation: what did your friends watch? â€“ Freshness: people tend to get more excited about new/surprising things. â€¢ Collaborative filtering does not predict well for new users/movies. â€“ New movies donâ€™t yet have ratings, and new users havenâ€™t rated anything. Content-Based vs. Collaborative Filtering â€¢ Consider content-based filtering, our usual supervised learning (Part 3): â€“ Here xij is a fixed vector of features for the movie/user. â€¢ Usual supervised learning setup: â€˜yâ€™ would contain all the yij, X would have xij as rows. â€“ Can predict on new users/movies, but canâ€™t learn about each user/movie. â€¢ If two users have the same features, then they get the exact same recommendations. â€¢ Our latent-factor approach to collaborative filtering (Part 4): â€“ Learns vector of features zi for each user â€˜iâ€™. â€“ But canâ€™t predict on new users (with no ratings). Hybrid Content/Collaborative: SVDfeature â€¢ SVDfeature combines content-based/collaborative filtering: â€¢ Learns weights â€˜wâ€™ on fixed features xij. â€“ Allows predictions for generic users/movies (including new ones). â€¢ And learns movie-specific weights wj on learned user-specific features zi. â€“ Allows more-accurate predictions for users/movies with lots of data. â€¢ Typically you also have a global bias ğ›½, user-specific bias ğ›½ğ‘–, and movie-specific ğ›½ğ‘—. â€“ And train with SGD (see bonus slides). â€¢ Won â€œKDD Cupâ€ competition in 2011 and 2012. Social Regularization â€¢ Many recommenders are now connected to social networks. â€“ â€œLogin using your Facebook accountâ€. â€¢ Often, people like similar movies to their friends. â€¢ Recent recommender systems use social regularization. â€“ Add a â€œregularizerâ€ encouraging friendsâ€™ weights to be similar: â€“ If we get a new user, recommendations are based on friendâ€™s preferences. Next Topic: Multi-Dimensional Scaling Visualization High-Dimensional Data â€¢ PCA for visualizing high-dimensional data: â€“ Use PCA â€˜Wâ€™ matrix to linearly transform data to get the zi values. â€“ And then we plot the zi values as locations in a scatterplot. http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/ http://scienceblogs.com/gnxp/2008/08/14/the-genetic-map-of-europe/ Visualization High-Dimensional Data â€¢ PCA for visualizing high-dimensional data: â€“ Use PCA â€˜Wâ€™ matrix to linearly transform data to get the zi values. â€“ And then we plot the zi values as locations in a scatterplot. â€¢ An common alternative is multi-dimensional scaling (MDS): â€“ Directly optimize the pixel locations of the zi values. â€¢ â€œGradient descent on the points in a scatterplotâ€. â€“ Needs a â€œcostâ€ function saying how â€œgoodâ€ the zi locations are. â€¢ Traditional MDS cost function: MDS Method (â€œSammon Mappingâ€) VideoMulti-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. Multi-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. â€“ Non-parametric dimensionality reduction and visualization: â€¢ No â€˜Wâ€™: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. â€“ Non-parametric dimensionality reduction and visualization: â€¢ No â€˜Wâ€™: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. â€“ Non-parametric dimensionality reduction and visualization: â€¢ No â€˜Wâ€™: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. â€“ Non-parametric dimensionality reduction and visualization: â€¢ No â€˜Wâ€™: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling â€¢ Multi-dimensional scaling (MDS): â€“ Directly optimize the final locations of the zi values. â€¢ Cannot use SVD to compute solution: â€“ Instead, do gradient descent on the zi values. â€“ You â€œlearnâ€ a scatterplot that tries to visualize high-dimensional data. â€“ Not convex and sensitive to initialization. â€¢ And solution is not unique due to various factors like translation and rotation. Different MDS Cost Functions â€¢ Unfortunately, MDS often does not work well in practice. â€¢ Problem with traditional MDS methods: focus on large distances. â€“ MDS tends to â€œcrowd/squashâ€ all the data points together like PCA. â€¢ But we could consider different distances/similarities: â€“ Where the functions are not necessarily the same: â€¢ d1 is the high-dimensional distance we want to match. â€¢ d2 is the low-dimensional distance we can control. â€¢ d3 controls how we compare high-/low-dimensional distances. â€¢ Early example was Sammonâ€™s Mapping (details in bonus). â€“ We next discuss t-SNE, a more recent method that tends to work better. MDS with Squared Distances vs. Sammonâ€™s Map â€¢ MDS based on Eucliean distances (left) vs. Sammonâ€™s Map (right): http://www.mdpi.com/1422-0067/15/7/12364/htm Next Topic: t-SNE Data on Manifolds â€¢ Consider data that lives on a low-dimensional â€œmanifoldâ€. â€“ Where Euclidean distances make sense â€œlocallyâ€. â€¢ But Euclidean distances may not make sense â€œgloballyâ€. â€“ Wikipedia example: Surface of the Earth is â€œlocallyâ€ flat. â€¢ Euclidean distance accurately measures distance â€œalong the surfaceâ€ locally. â€¢ For far points Euclidean distance is a poor measure of distance â€œalong the surfaceâ€. http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf Data on Manifolds â€¢ Consider data that lives on a low-dimensional â€œmanifoldâ€. â€“ Where Euclidean distances make sense â€œlocallyâ€. â€¢ But Euclidean distances may not make sense â€œgloballyâ€. â€¢ Example is the â€˜Swiss rollâ€™: http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf Example: Manifolds in Image Space â€¢ Slowly-varying image transformations exist on a manifold: â€¢ â€œNeighbouringâ€ images are close in Euclidean distance. â€“ But distances between very-different images are not reliable. https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf Learning Manifolds â€¢ With usual distances, PCA/MDS do not discover non-linear manifolds. http://www.peh-med.com/content/9/1/12/figure/F1 Learning Manifolds â€¢ With usual distances, PCA/MDS do not discover non-linear manifolds. â€¢ We could use change of basis or kernels: but still need to pick basis. http://www.peh-med.com/content/9/1/12/figure/F1 Sammonâ€™s Map vs. ISOMAP vs. PCA (MNIST) â€¢ A classic way to visualize manifolds is ISOMAP. â€“ Uses approximation of geodesic distance within MDS (see bonus slides). http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammonâ€™s Map vs. ISOMAP vs. t-SNE (MNIST) â€¢ A modern way to visualize manifolds and clusters is t-SNE. http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammonâ€™s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammonâ€™s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammonâ€™s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammonâ€™s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf t-Distributed Stochastic Neighbour Embedding â€¢ One key idea in t-SNE: â€“ Focus on distance to â€œneighboursâ€ (allow large variance in other distances) t-Distributed Stochastic Neighbour Embedding â€¢ t-SNE is a special case of MDS (specific d1, d2, and d3 choices): â€“ d1: for each xi, compute probability that each xj is a â€˜neighbourâ€™. â€¢ Computation is similar to k-means++, but most weight to close points (Gaussian). â€¢ Does not require explicit geodesic distance approximation. â€“ d2: for each zi, compute probability that each zj is a â€˜neighbourâ€™. â€¢ Similar to above, but uses studentâ€™s t (grows really slowly with distance). â€¢ Avoids â€˜crowdingâ€™, because you have a huge range that large distances can fill. â€“ d3: Compares xi and zi using an entropy-like measure: â€¢ How much â€˜randomnessâ€™ is in probabilities of xi if you know the zi (and vice versa)? â€¢ Interactive demo: https://distill.pub/2016/misread-tsne t-SNE on Wikipedia Articles http://jasneetsabharwal.com/assets/files/wiki_tsne_report.pdf t-SNE on Product Features http://blog.kaggle.com/2015/06/09/otto-product-classification-winners-interview-2nd-place-alexander-guschin/ t-SNE on Leukemia Heterogeneity http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076922/ Next Topic: Word2Vec Latent-Factor Representation of Words â€¢ For natural language, we often represent words by an index. â€“ E.g., â€œcatâ€ is word 124056 among a â€œbag of wordsâ€. â€¢ But this may be inefficient: â€“ Should â€œcatâ€ and â€œkittenâ€ features be related is some way? â€¢ We want a latent-factor representation of individual words: â€“ Closeness in latent space should indicate similarity. â€“ Distances could represent meaning? â€¢ Recent alternative to PCA is word2vecâ€¦ Using Context â€¢ Consider these phrases: â€“ â€œthe cat purredâ€ â€“ â€œthe kitten purredâ€ â€“ â€œblack cat ranâ€ â€“ â€œblack kitten ranâ€ â€¢ Words that occur in the same context likely have similar meanings. â€¢ Word2vec uses this insight to design an MDS distance function. Word2Vec (Continuous Bag of Words) â€¢ A common word2vec approaches (called continuous bag of words): â€“ Each word â€˜iâ€™ is represented by a vector of real numbers zi. â€“ Training data: sentence fragments with â€œhiddenâ€ middle word: â€¢ â€œWe introduce basic principles and techniques inâ€ â€¢ â€œthe fields of data mining and machineâ€ â€¢ â€œtools behind the emerging field of dataâ€ â€¢ â€œtechniques are now running behind the scenesâ€ â€¢ â€œdiscover patterns and make predictions in variousâ€ â€¢ â€œthe core data mining and machine learningâ€ â€¢ â€œwith motivating applications from a variety ofâ€ â€“ Train so that zi of â€œhiddenâ€ words is are similar to zi of surrounding words. Word2Vec (Continuous Bag of Words) â€¢ Continuous bag of words model probability of middle word â€˜iâ€™ as: â€¢ We use gradient descent on negative logarithm of these probabilities: â€“ Makes ziTzj big for words appearing in same context (making zi close to zj). â€“ Makes ziTzj small for words not appearing together (makes zi and zj far). â€¢ Once trained, you use these zi as features for language tasks. â€“ Tends to work much better than bag of words. â€“ Allows you to get useful features of words from unlabeled text data. Word2Vec (Skip-Gram) â€¢ A common word2vec approaches (skip gram): â€“ Each word â€˜iâ€™ is represented by a vector of real numbers zi. â€“ Training data: sentence fragments with â€œhiddenâ€ surrounding word: â€¢ â€œWe introduce basic principles and techniques inâ€ â€¢ â€œthe fields of data mining and machineâ€ â€¢ â€œtools behind the emerging field of dataâ€ â€¢ â€œtechniques are now running behind the scenesâ€ â€¢ â€œdiscover patterns and make predictions in variousâ€ â€¢ â€œthe core data mining and machine learningâ€ â€¢ â€œwith motivating applications from a variety ofâ€ â€“ Train so that zi of â€œhiddenâ€ words is are similar to zi of surrounding words. â€¢ Uses same probability as continuous bag of words. â€“ But denominator sums over all possible surrounding words (often just sample terms for speed). Word2Vec Example â€¢ MDS visualization of a set of related words: â€¢ Distances between vectors might represent semantics. http://sebastianruder.com/secret-word2vec Word2Vec â€¢ Subtracting word vectors to find related vectors. â€¢ Word vectors for 157 languages here. https://arxiv.org/pdf/1301.3781.pdf Summary â€¢ Collaborative filtering tries to fill in missing values in a matrix. â€“ Matrix factorization is a common approach (â€œPCA with missing entriesâ€). â€¢ Multi-dimensional scaling is a non-parametric latent-factor model. â€¢ Different MDS distances/losses/weights usually gives better results. â€¢ Manifold: space where local Euclidean distance is accurate. â€“ Structured data like images often form manifolds in space. â€¢ t-SNE is an MDS method focusing on matching small distances. â€¢ Word2vec: â€“ Latent-factor (continuous) representation of words. â€“ Based on predicting word from its context (or context from word). â€¢ Next time: deep learning. Stochastic Gradient for SVDfeature â€¢ Common approach to fitting SVDfeature is stochastic gradient. â€¢ Previously you saw stochastic gradient for supervised learning: â€¢ Stochastic gradient for SVDfeature (formulas as bonus): SVDfeature with SGD: the gory detailsTensor Factorization â€¢ Tensors are higher-order generalizations of matrices: â€¢ Generalization of matrix factorization is tensor factorization: â€¢ Useful if there are other relevant variables: â€¢ Instead of ratings based on {user,movie}, ratings based {user,movie,group}. â€¢ Useful if you have groups of users, or if ratings change over time. Field-Aware Matrix Factorization â€¢ Field-aware factorization machines (FFMs): â€“ Matrix factorization with multiple zi or wc for each example or part. â€“ You choose which zi or wc to use based on the value of feature. â€¢ Example from â€œclick through rateâ€ prediction: â€“ E.g., predict whether â€œmaleâ€ clicks on â€œnikeâ€ advertising on â€œespnâ€ page. â€“ A previous matrix factorization method for the 3 factors used: â€“ FFMs could use: â€¢ wespnA is the factor we use when multiplying by a an advertiserâ€™s latent factor. â€¢ wespnG is the factor we use when multiplying by a groupâ€™s latent factor. â€¢ This approach has won some Kaggle competitions (link), and has shown to work well in production systems too (link). Warm-Starting â€¢ Weâ€™ve used data {X,y} to fit a model. â€¢ We now have new training data and want to fit new and old data. â€¢ Do we need to re-fit from scratch? â€¢ This is the warm starting problem. â€“ Itâ€™s easier to warm start some models than others. Easy Case: K-Nearest Neighbours and Counting â€¢ K-nearest neighbours: â€“ KNN just stores the training data, so just store the new data. â€¢ Counting-based models: â€“ Models that base predictions on frequencies of events. â€“ E.g., naÃ¯ve Bayes. â€“ Just update the counts: â€“ Decision trees with fixed rules: just update counts at the leaves. Medium Case: L2-Regularized Least Squares â€¢ L2-regularized least squares is obtained from linear algebra: â€“ Cost is O(nd2 + d3) for â€˜nâ€™ training examples and â€˜dâ€™ features. â€¢ Given one new point, we need to compute: â€“ XTy with one row added, which costs O(d). â€“ Old XTX plus xixiT, which costs O(d2). â€“ Solution of linear system, which costs O(d3). â€“ So cost of adding â€˜tâ€™ new data point is O(td3). â€¢ With â€œmatrix factorization updatesâ€, can reduce this to O(td2). â€“ Cheaper than computing from scratch, particularly for large d. Medium Case: Logistic Regression â€¢ We fit logistic regression by gradient descent on a convex function. â€¢ With new data, convex function f(w) changes to new function g(w). â€¢ If we donâ€™t have much more data, â€˜fâ€™ and â€˜gâ€™ will be â€œcloseâ€. â€“ Start gradient descent on â€˜gâ€™ with minimizer of â€˜fâ€™. â€“ You can show that it requires fewer iterations. Hard Cases: Non-Convex/Greedy Models â€¢ For decision trees: â€“ â€œWarm startâ€: continue splitting nodes that havenâ€™t already been split. â€“ â€œCold startâ€: re-fit everything. â€¢ Unlike previous cases, this wonâ€™t in general give same result as re-fitting: â€“ New data points might lead to different splits higher up in the tree. â€¢ Intermediate: usually do warm start but occasionally do a cold start. â€¢ Similar heuristics/conclusions for other non-convex/greedy models: â€“ K-means clustering. â€“ Matrix factorization (though you can continue PCA algorithms). Different MDS Cost Functions â€¢ MDS default objective function with general distances/similarities: â€¢ A possibility is â€œclassicâ€ MDS with d1(xi,xj) = xi Txj and d2(zi,zj) = zi Tzj. â€“ We obtain PCA in this special case (centered xi, d3 as the squared L2-norm). â€“ Not a great choice because itâ€™s a linear model. Different MDS Cost Functions â€¢ MDS default objective function with general distances/similarities: â€¢ Another possibility: d1(xi,xj) = ||xi â€“ xj||1 and d2(zi,zj) = ||zi â€“ zj||. â€“ The zi approximate the high-dimensional L1-norm distances. http://www.mdpi.com/1422-0067/15/7/12364/htm Sammonâ€™s Mapping â€¢ Challenge for most MDS models: they focus on large distances. â€“ Leads to â€œcrowdingâ€ effect like with PCA. â€¢ Early attempt to address this is Sammonâ€™s mapping: â€“ Weighted MDS so large/small distances are more comparable. â€“ Denominator reduces focus on large distances. http://www.mdpi.com/1422-0067/15/7/12364/htm Sammonâ€™s Mapping â€¢ Challenge for most MDS models: they focus on large distances. â€“ Leads to â€œcrowdingâ€ effect like with PCA. â€¢ Early attempt to address this is Sammonâ€™s mapping: â€“ Weighted MDS so large/small distances are more comparable. http://www.mdpi.com/1422-0067/15/7/12364/htm Geodesic Distance on Manifolds â€¢ Consider data that lives on a low-dimensional â€œmanifoldâ€. â€“ With usual distances, PCA/MDS will not discover non-linear manifolds. â€¢ We need geodesic distance: the distance through the manifold. http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf ISOMAP â€¢ ISOMAP is latent-factor model for visualizing data on manifolds: ISOMAP â€¢ ISOMAP can â€œunwrapâ€ the roll: â€“ Shortest paths are approximations to geodesic distances. â€¢ Sensitive to having the right graph: â€“ Points off of manifold and gaps in manifold cause problems. http://www.peh-med.com/content/9/1/12/figure/F1 Constructing Neighbour Graphs â€¢ Sometimes you can define the graph/distance without features: â€“ Facebook friend graph. â€“ Connect YouTube videos if one video tends to follow another. â€¢ But we can also convert from features xi to a â€œneighbourâ€ graph: â€“ Approach 1 (â€œepsilon graphâ€): connect xi to all xj within some threshold Îµ. â€¢ Like we did with density-based clustering. â€“ Approach 2 (â€œKNN graphâ€): connect xi to xj if: â€¢ xj is a KNN of xi OR xi is a KNN of xj. â€“ Approach 2 (â€œmutual KNN graphâ€): connect xi to xj if: â€¢ xj is a KNN of xi AND xi is a KNN of xj. http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf Converting from Features to Graph http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5B0%5D.pdf ISOMAP â€¢ ISOMAP is latent-factor model for visualizing data on manifolds: 1. Find the neighbours of each point. â€¢ Usually â€œk-nearest neighbours graphâ€, or â€œepsilon graphâ€. 2. Compute edge weights: â€¢ Usually distance between neighbours. 3. Compute weighted shortest path between all points. â€¢ Dijkstra or other shortest path algorithm. 4. Run MDS using these distances. http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf Does t-SNE always outperform PCA? â€¢ Consider 3D data living on a 2D hyper-plane: â€¢ PCA can perfectly capture the low-dimensional structure. â€¢ T-SNE can capture the local structure, but can â€œtwistâ€ the plane. â€“ It doesnâ€™t try to get long distances correct. Graph Drawing â€¢ A closely-related topic to MDS is graph drawing: â€“ Given a graph, how should we display it? â€“ Lots of interesting methods: https://en.wikipedia.org/wiki/Graph_drawing Bonus Slide: Multivariate Chain Rule â€¢ Recall the univariate chain rule: â€¢ The multivariate chain rule: â€¢ Example: Bonus Slide: Multivariate Chain Rule for MDS â€¢ General MDS formulation: â€¢ Using multivariate chain rule we have: â€¢ Example: Multiple Word Prototypes â€¢ What about homonyms and polysemy? â€“ The word vectors would need to account for all meanings. â€¢ More recent approaches: â€“ Try to cluster the different contexts where words appear. â€“ Use different vectors for different contexts. Multiple Word Prototypes http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes","libVersion":"0.2.1","langs":""}