{"path":".obsidian/plugins/text-extractor/cache/bef123bff4871d673a5c0ff7a0840362.json","text":"CPSC 340: Machine Learning and Data Mining Multi-Dimensional Scaling Last Time: Recommende Systems • We discussed recommender systems: – Predicting what ratings users have for different products. • We mentioned content-based filtering: – Formulates recommendation as supervised learning. • Extract features of users and products, and use these to predict rating. • We introduced collaborative filtering: – Methods that only looks at ratings, not features of products. • Looks more like unsupervised learning. • Most common approach to solve this problem is “matrix factorization” (today). – Performed surprisingly well in Netflix competition and has been adopted by companies. Matrix Factorization for Collaborative Filtering • The standard latent-factor model for entries in matrix ‘Y’: • User ‘i’ has latent features zi. • Movie ‘j’ has latent features wj. • Our loss functions sums over available ratings ‘R’: • And we add L2-regularization to both types of features. – Basically, this is regularized PCA on the available entries of Y. – Typically fit with SGD. • This simple method gives you a 7% improvement on the Netflix problem. Adding Global/User/Movie Biases • Our standard latent-factor model for entries in matrix ‘Y’: • Sometimes we don’t assume the yij have a mean of zero: – We could add bias β reflecting average overall rating: – We could also add a user-specific bias βi and item-specific bias βj. • Some users rate things higher on average, and movies are rated better on average. • These might also be regularized. Beyond Accuracy in Recommender Systems • Winning system of Netflix Challenge was never adopted. • Other issues important in recommender systems: – Diversity: how different are the recommendations? • If you like ‘Battle of Five Armies Extended Edition’, recommend Battle of Five Armies? • Even if you really really like Star Wars, you might want non-Star-Wars suggestions. – Persistence: how long should recommendations last? • If you keep not clicking on ‘Hunger Games’, should it remain a recommendation? – Trust: tell user why you made a recommendation. • Quora gives explanations for recommendations. – Social recommendation: what did your friends watch? – Freshness: people tend to get more excited about new/surprising things. • Collaborative filtering does not predict well for new users/movies. – New movies don’t yet have ratings, and new users haven’t rated anything. Content-Based vs. Collaborative Filtering • Consider content-based filtering, our usual supervised learning (Part 3): – Here xij is a fixed vector of features for the movie/user. • Usual supervised learning setup: ‘y’ would contain all the yij, X would have xij as rows. – Can predict on new users/movies, but can’t learn about each user/movie. • If two users have the same features, then they get the exact same recommendations. • Our latent-factor approach to collaborative filtering (Part 4): – Learns vector of features zi for each user ‘i’. – But can’t predict on new users (with no ratings). Hybrid Content/Collaborative: SVDfeature • SVDfeature combines content-based/collaborative filtering: • Learns weights ‘w’ on fixed features xij. – Allows predictions for generic users/movies (including new ones). • And learns movie-specific weights wj on learned user-specific features zi. – Allows more-accurate predictions for users/movies with lots of data. • Typically you also have a global bias 𝛽, user-specific bias 𝛽𝑖, and movie-specific 𝛽𝑗. – And train with SGD (see bonus slides). • Won “KDD Cup” competition in 2011 and 2012. Social Regularization • Many recommenders are now connected to social networks. – “Login using your Facebook account”. • Often, people like similar movies to their friends. • Recent recommender systems use social regularization. – Add a “regularizer” encouraging friends’ weights to be similar: – If we get a new user, recommendations are based on friend’s preferences. Next Topic: Multi-Dimensional Scaling Visualization High-Dimensional Data • PCA for visualizing high-dimensional data: – Use PCA ‘W’ matrix to linearly transform data to get the zi values. – And then we plot the zi values as locations in a scatterplot. http://www.turingfinance.com/artificial-intelligence-and-statistics-principal-component-analysis-and-self-organizing-maps/ http://scienceblogs.com/gnxp/2008/08/14/the-genetic-map-of-europe/ Visualization High-Dimensional Data • PCA for visualizing high-dimensional data: – Use PCA ‘W’ matrix to linearly transform data to get the zi values. – And then we plot the zi values as locations in a scatterplot. • An common alternative is multi-dimensional scaling (MDS): – Directly optimize the pixel locations of the zi values. • “Gradient descent on the points in a scatterplot”. – Needs a “cost” function saying how “good” the zi locations are. • Traditional MDS cost function: MDS Method (“Sammon Mapping”) VideoMulti-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. Multi-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. – Non-parametric dimensionality reduction and visualization: • No ‘W’: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. – Non-parametric dimensionality reduction and visualization: • No ‘W’: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. – Non-parametric dimensionality reduction and visualization: • No ‘W’: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. – Non-parametric dimensionality reduction and visualization: • No ‘W’: just trying to make zi preserve high-dimensional distances between xi. Multi-Dimensional Scaling • Multi-dimensional scaling (MDS): – Directly optimize the final locations of the zi values. • Cannot use SVD to compute solution: – Instead, do gradient descent on the zi values. – You “learn” a scatterplot that tries to visualize high-dimensional data. – Not convex and sensitive to initialization. • And solution is not unique due to various factors like translation and rotation. Different MDS Cost Functions • Unfortunately, MDS often does not work well in practice. • Problem with traditional MDS methods: focus on large distances. – MDS tends to “crowd/squash” all the data points together like PCA. • But we could consider different distances/similarities: – Where the functions are not necessarily the same: • d1 is the high-dimensional distance we want to match. • d2 is the low-dimensional distance we can control. • d3 controls how we compare high-/low-dimensional distances. • Early example was Sammon’s Mapping (details in bonus). – We next discuss t-SNE, a more recent method that tends to work better. MDS with Squared Distances vs. Sammon’s Map • MDS based on Eucliean distances (left) vs. Sammon’s Map (right): http://www.mdpi.com/1422-0067/15/7/12364/htm Next Topic: t-SNE Data on Manifolds • Consider data that lives on a low-dimensional “manifold”. – Where Euclidean distances make sense “locally”. • But Euclidean distances may not make sense “globally”. – Wikipedia example: Surface of the Earth is “locally” flat. • Euclidean distance accurately measures distance “along the surface” locally. • For far points Euclidean distance is a poor measure of distance “along the surface”. http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf Data on Manifolds • Consider data that lives on a low-dimensional “manifold”. – Where Euclidean distances make sense “locally”. • But Euclidean distances may not make sense “globally”. • Example is the ‘Swiss roll’: http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf Example: Manifolds in Image Space • Slowly-varying image transformations exist on a manifold: • “Neighbouring” images are close in Euclidean distance. – But distances between very-different images are not reliable. https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf Learning Manifolds • With usual distances, PCA/MDS do not discover non-linear manifolds. http://www.peh-med.com/content/9/1/12/figure/F1 Learning Manifolds • With usual distances, PCA/MDS do not discover non-linear manifolds. • We could use change of basis or kernels: but still need to pick basis. http://www.peh-med.com/content/9/1/12/figure/F1 Sammon’s Map vs. ISOMAP vs. PCA (MNIST) • A classic way to visualize manifolds is ISOMAP. – Uses approximation of geodesic distance within MDS (see bonus slides). http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammon’s Map vs. ISOMAP vs. t-SNE (MNIST) • A modern way to visualize manifolds and clusters is t-SNE. http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammon’s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammon’s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammon’s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf Sammon’s Map vs. ISOMAP vs. t-SNE (MNIST) http://lvdmaaten.github.io/publications/papers/JMLR_2008.pdf t-Distributed Stochastic Neighbour Embedding • One key idea in t-SNE: – Focus on distance to “neighbours” (allow large variance in other distances) t-Distributed Stochastic Neighbour Embedding • t-SNE is a special case of MDS (specific d1, d2, and d3 choices): – d1: for each xi, compute probability that each xj is a ‘neighbour’. • Computation is similar to k-means++, but most weight to close points (Gaussian). • Does not require explicit geodesic distance approximation. – d2: for each zi, compute probability that each zj is a ‘neighbour’. • Similar to above, but uses student’s t (grows really slowly with distance). • Avoids ‘crowding’, because you have a huge range that large distances can fill. – d3: Compares xi and zi using an entropy-like measure: • How much ‘randomness’ is in probabilities of xi if you know the zi (and vice versa)? • Interactive demo: https://distill.pub/2016/misread-tsne t-SNE on Wikipedia Articles http://jasneetsabharwal.com/assets/files/wiki_tsne_report.pdf t-SNE on Product Features http://blog.kaggle.com/2015/06/09/otto-product-classification-winners-interview-2nd-place-alexander-guschin/ t-SNE on Leukemia Heterogeneity http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4076922/ Next Topic: Word2Vec Latent-Factor Representation of Words • For natural language, we often represent words by an index. – E.g., “cat” is word 124056 among a “bag of words”. • But this may be inefficient: – Should “cat” and “kitten” features be related is some way? • We want a latent-factor representation of individual words: – Closeness in latent space should indicate similarity. – Distances could represent meaning? • Recent alternative to PCA is word2vec… Using Context • Consider these phrases: – “the cat purred” – “the kitten purred” – “black cat ran” – “black kitten ran” • Words that occur in the same context likely have similar meanings. • Word2vec uses this insight to design an MDS distance function. Word2Vec (Continuous Bag of Words) • A common word2vec approaches (called continuous bag of words): – Each word ‘i’ is represented by a vector of real numbers zi. – Training data: sentence fragments with “hidden” middle word: • “We introduce basic principles and techniques in” • “the fields of data mining and machine” • “tools behind the emerging field of data” • “techniques are now running behind the scenes” • “discover patterns and make predictions in various” • “the core data mining and machine learning” • “with motivating applications from a variety of” – Train so that zi of “hidden” words is are similar to zi of surrounding words. Word2Vec (Continuous Bag of Words) • Continuous bag of words model probability of middle word ‘i’ as: • We use gradient descent on negative logarithm of these probabilities: – Makes ziTzj big for words appearing in same context (making zi close to zj). – Makes ziTzj small for words not appearing together (makes zi and zj far). • Once trained, you use these zi as features for language tasks. – Tends to work much better than bag of words. – Allows you to get useful features of words from unlabeled text data. Word2Vec (Skip-Gram) • A common word2vec approaches (skip gram): – Each word ‘i’ is represented by a vector of real numbers zi. – Training data: sentence fragments with “hidden” surrounding word: • “We introduce basic principles and techniques in” • “the fields of data mining and machine” • “tools behind the emerging field of data” • “techniques are now running behind the scenes” • “discover patterns and make predictions in various” • “the core data mining and machine learning” • “with motivating applications from a variety of” – Train so that zi of “hidden” words is are similar to zi of surrounding words. • Uses same probability as continuous bag of words. – But denominator sums over all possible surrounding words (often just sample terms for speed). Word2Vec Example • MDS visualization of a set of related words: • Distances between vectors might represent semantics. http://sebastianruder.com/secret-word2vec Word2Vec • Subtracting word vectors to find related vectors. • Word vectors for 157 languages here. https://arxiv.org/pdf/1301.3781.pdf Summary • Collaborative filtering tries to fill in missing values in a matrix. – Matrix factorization is a common approach (“PCA with missing entries”). • Multi-dimensional scaling is a non-parametric latent-factor model. • Different MDS distances/losses/weights usually gives better results. • Manifold: space where local Euclidean distance is accurate. – Structured data like images often form manifolds in space. • t-SNE is an MDS method focusing on matching small distances. • Word2vec: – Latent-factor (continuous) representation of words. – Based on predicting word from its context (or context from word). • Next time: deep learning. Stochastic Gradient for SVDfeature • Common approach to fitting SVDfeature is stochastic gradient. • Previously you saw stochastic gradient for supervised learning: • Stochastic gradient for SVDfeature (formulas as bonus): SVDfeature with SGD: the gory detailsTensor Factorization • Tensors are higher-order generalizations of matrices: • Generalization of matrix factorization is tensor factorization: • Useful if there are other relevant variables: • Instead of ratings based on {user,movie}, ratings based {user,movie,group}. • Useful if you have groups of users, or if ratings change over time. Field-Aware Matrix Factorization • Field-aware factorization machines (FFMs): – Matrix factorization with multiple zi or wc for each example or part. – You choose which zi or wc to use based on the value of feature. • Example from “click through rate” prediction: – E.g., predict whether “male” clicks on “nike” advertising on “espn” page. – A previous matrix factorization method for the 3 factors used: – FFMs could use: • wespnA is the factor we use when multiplying by a an advertiser’s latent factor. • wespnG is the factor we use when multiplying by a group’s latent factor. • This approach has won some Kaggle competitions (link), and has shown to work well in production systems too (link). Warm-Starting • We’ve used data {X,y} to fit a model. • We now have new training data and want to fit new and old data. • Do we need to re-fit from scratch? • This is the warm starting problem. – It’s easier to warm start some models than others. Easy Case: K-Nearest Neighbours and Counting • K-nearest neighbours: – KNN just stores the training data, so just store the new data. • Counting-based models: – Models that base predictions on frequencies of events. – E.g., naïve Bayes. – Just update the counts: – Decision trees with fixed rules: just update counts at the leaves. Medium Case: L2-Regularized Least Squares • L2-regularized least squares is obtained from linear algebra: – Cost is O(nd2 + d3) for ‘n’ training examples and ‘d’ features. • Given one new point, we need to compute: – XTy with one row added, which costs O(d). – Old XTX plus xixiT, which costs O(d2). – Solution of linear system, which costs O(d3). – So cost of adding ‘t’ new data point is O(td3). • With “matrix factorization updates”, can reduce this to O(td2). – Cheaper than computing from scratch, particularly for large d. Medium Case: Logistic Regression • We fit logistic regression by gradient descent on a convex function. • With new data, convex function f(w) changes to new function g(w). • If we don’t have much more data, ‘f’ and ‘g’ will be “close”. – Start gradient descent on ‘g’ with minimizer of ‘f’. – You can show that it requires fewer iterations. Hard Cases: Non-Convex/Greedy Models • For decision trees: – “Warm start”: continue splitting nodes that haven’t already been split. – “Cold start”: re-fit everything. • Unlike previous cases, this won’t in general give same result as re-fitting: – New data points might lead to different splits higher up in the tree. • Intermediate: usually do warm start but occasionally do a cold start. • Similar heuristics/conclusions for other non-convex/greedy models: – K-means clustering. – Matrix factorization (though you can continue PCA algorithms). Different MDS Cost Functions • MDS default objective function with general distances/similarities: • A possibility is “classic” MDS with d1(xi,xj) = xi Txj and d2(zi,zj) = zi Tzj. – We obtain PCA in this special case (centered xi, d3 as the squared L2-norm). – Not a great choice because it’s a linear model. Different MDS Cost Functions • MDS default objective function with general distances/similarities: • Another possibility: d1(xi,xj) = ||xi – xj||1 and d2(zi,zj) = ||zi – zj||. – The zi approximate the high-dimensional L1-norm distances. http://www.mdpi.com/1422-0067/15/7/12364/htm Sammon’s Mapping • Challenge for most MDS models: they focus on large distances. – Leads to “crowding” effect like with PCA. • Early attempt to address this is Sammon’s mapping: – Weighted MDS so large/small distances are more comparable. – Denominator reduces focus on large distances. http://www.mdpi.com/1422-0067/15/7/12364/htm Sammon’s Mapping • Challenge for most MDS models: they focus on large distances. – Leads to “crowding” effect like with PCA. • Early attempt to address this is Sammon’s mapping: – Weighted MDS so large/small distances are more comparable. http://www.mdpi.com/1422-0067/15/7/12364/htm Geodesic Distance on Manifolds • Consider data that lives on a low-dimensional “manifold”. – With usual distances, PCA/MDS will not discover non-linear manifolds. • We need geodesic distance: the distance through the manifold. http://www.biomedcentral.com/content/pdf/1471-2105-13-S7-S3.pdf http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf ISOMAP • ISOMAP is latent-factor model for visualizing data on manifolds: ISOMAP • ISOMAP can “unwrap” the roll: – Shortest paths are approximations to geodesic distances. • Sensitive to having the right graph: – Points off of manifold and gaps in manifold cause problems. http://www.peh-med.com/content/9/1/12/figure/F1 Constructing Neighbour Graphs • Sometimes you can define the graph/distance without features: – Facebook friend graph. – Connect YouTube videos if one video tends to follow another. • But we can also convert from features xi to a “neighbour” graph: – Approach 1 (“epsilon graph”): connect xi to all xj within some threshold ε. • Like we did with density-based clustering. – Approach 2 (“KNN graph”): connect xi to xj if: • xj is a KNN of xi OR xi is a KNN of xj. – Approach 2 (“mutual KNN graph”): connect xi to xj if: • xj is a KNN of xi AND xi is a KNN of xj. http://ai.stanford.edu/~ang/papers/nips01-spectral.pdf Converting from Features to Graph http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5B0%5D.pdf ISOMAP • ISOMAP is latent-factor model for visualizing data on manifolds: 1. Find the neighbours of each point. • Usually “k-nearest neighbours graph”, or “epsilon graph”. 2. Compute edge weights: • Usually distance between neighbours. 3. Compute weighted shortest path between all points. • Dijkstra or other shortest path algorithm. 4. Run MDS using these distances. http://wearables.cc.gatech.edu/paper_of_week/isomap.pdf Does t-SNE always outperform PCA? • Consider 3D data living on a 2D hyper-plane: • PCA can perfectly capture the low-dimensional structure. • T-SNE can capture the local structure, but can “twist” the plane. – It doesn’t try to get long distances correct. Graph Drawing • A closely-related topic to MDS is graph drawing: – Given a graph, how should we display it? – Lots of interesting methods: https://en.wikipedia.org/wiki/Graph_drawing Bonus Slide: Multivariate Chain Rule • Recall the univariate chain rule: • The multivariate chain rule: • Example: Bonus Slide: Multivariate Chain Rule for MDS • General MDS formulation: • Using multivariate chain rule we have: • Example: Multiple Word Prototypes • What about homonyms and polysemy? – The word vectors would need to account for all meanings. • More recent approaches: – Try to cluster the different contexts where words appear. – Use different vectors for different contexts. Multiple Word Prototypes http://www.socher.org/index.php/Main/ImprovingWordRepresentationsViaGlobalContextAndMultipleWordPrototypes","libVersion":"0.2.1","langs":""}