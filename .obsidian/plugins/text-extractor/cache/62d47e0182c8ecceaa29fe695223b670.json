{"path":".obsidian/plugins/text-extractor/cache/62d47e0182c8ecceaa29fe695223b670.json","text":"Entropy as Measure of Randomness * Another common summary statistic is . — Entropy measures “randomness” of a set of variables. * Roughly, another measure of the “spread” of values. * Formally, “how many bits of information are encoded in the average example”. — For a categorical variable that can take ‘k’ values, entropy is defined by: where p, is the proportion of times you have value ‘c’. — Minimum value is 0 (no randomness). * We use the convention that 0 log 0 = 0. — Maximum value is log(k).","libVersion":"0.2.1","langs":"eng"}