{"path":".obsidian/plugins/text-extractor/cache/b2d73cfd65a27d36b269d34f6bc75e93.json","text":"C·oni.puter Systems A Programmer's Perspective THIRD EDITION \"Randal E. Bryant Carnegie Mellon University David R. O'Hallaron Carnegie Mellon University Pearson Boston Columbus Hoboken Indianapolis New York San Francisco Amsterdam Cape Town Dubai London Madrid Milan Munich Paris Montre'h.1 .Jbro:rito Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore Taipei Tokyo \\ I Vice President and Editorial Director: Marcia J. Horton Executive Editor: Matt Goldstein Editorial Assistant: Kelsey Loanes VP of Marketing: Christy Lesko Director of Field Marketing: Tim Galligan Product Marketing Manager: Bram van Kempen Field Marketing Manager: Demetrius Hall Marketing Assistant: Jon Bryant Director of Product Management: Erin Gregg Team Lead Product Management: Scott Disanno Program Manager: Joanne Manning Procurement Manager: Mary Fischer Senior Specialist, Program Planning and Support: Maura Zaldivar-Garcia Cover Designer: Joyce Wells Manager, Rights Management: Rachel Youdelman Associate Project Manager, Rights Management: William J. Opaluch Full-Service Project Management: Paul Anagnostopoulos, Windfall Software Composition: Windfall Software Printer/Binder: Courier Westford Cover Printer: Courier Westford Typeface: 10/12 Times 10, ITC Stone Sans Tue graph on the front cover is a \"memory mountain\" that shows the measured read throughput of an Intel Core i7 processor as a function of spatial and temporal locality. Copyright© 2016, 2011, and 2003 by Pearson Education, Inc. or its affiliates. All Rights Reserved. Printed in the United States of America. This publication is protected by copyright, and permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording. or otherwise. For information regarding permissions, request forms and the appropriate contacts within the Pearson Education Global Rights & Permissions department, please visit www.pearsoned.com/permissions/. Many of the designations by manufacturers and seller to distinguish their products are claimed as trademarks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps. Tue author and publisher of this book have used their best efforts in preparing this book. These efforts include the development, research, and testing of theories and programs to determine their effectiveness. The author and publisher make no warranty of any kind, expressed or implied, with regard to these programs or the documentation contained in this book. The author and publisher shall not be liable in any event for incidental or consequential damages with, or arising out of, the furnishing, performance, or use of these programs. Pearson Education Ltd., London Pearson Education Singapore, Pte. Ltd Pearson Education Canada, Inc. Pearson Education-Japan Pearson Education Australia PTY, Limited Pearson Education North Asia, Ltd., Hong Kong Pearson Educacill de Mexico, S.A. de C.V. Pearson Education Malaysia, Pte. Ltd. Pearson Education, Inc., Upper Saddle River, New Jersey Library of Congress Cataloging-in-Publication Data Bryant, Randal E. Computer systems: a programmer's perspective I Randal E. Bryant, Carnegie Mellon University, David R. O'Hallaron, Carnegie Mellon. University.-Third edition. pages cm Includes bibliographical references and index. ISBN 978-0-13-409266-9-ISBN 0-13-409266-X 1. Computer systems. 2. Computers. 3. Telecommunication. 4. User interfaces (Computer systems) I. O'Hallaron, David R. (David Richard) II. Title. QA76.5.B795 2016 OOS.3--dc23 2015000930 10 9 8 7 6 5 4 3 2 PEARSON wmv.pearsonhighered.com ISBN 10: 0-13-409266-X ISBN 13: 978-0-13-409266-9 - Mastering Engineering® For Computer Systems: A Programmer's Perspective, Third Edition Mastering is Pearson's proven online Tutorial Homework program, newly available with the third edition of Computer Systems: A Programmer's Perspective. The Mastering platform allows you to integrate dynamic homework-with many problems taken directly from the Bryant/O'Hallaron textbook-with automatic grading. Mastering allows you to easily track the performance of your entire class on an assignment-by-assignment basis, or view the detaileq work of an individual student. t -. I ~ , \" For more information or a demonstration of the course, visit www.MasteringEngineering.com I ' or contact your local Pearson representative. .._._. Contents t \" ,. Preface xix About the Autho~s xxxv 1 A Tour of Cqmputer ~ystell!;s 1 Information Is,Bits + Context ''.l 1.1 L2 L3 1.4 Programs Are Translated .by Other P,rograms into Different Forms 4 It Pays to Understand How_ Compilation Systems Work 6 1.5 Processors Read and-Interpret Instructions Stored in Memory 7 1.4.1 Hardware Organization of a System 8 1.4.2 Running the hello Program 10 Caches Matter 11 1.6 Storage Devices Form a Hierarchy. 14 1. 7 The Operating System Manages the Hard~are 14 1.7.1 Processes 15 1.7.2 Threads 17 1.7.3 Virtual Memory 18 1.7.4 Files 19 LS Systems Communicate with Other Systems Using Networks 19. 1.9 Important Themes 22 1.9.1 Amdahl's Law 22· '•· 1.9.2 Concurrency and Parallelism 24 1.9.3 The Importance of Abstractions in Computer Systems 26 1.10 Summary 27 Bibliographic Notes 28 Solutions to Practice Problems 28 Part I Program Structui:e and.Execution 2 Represepting and Manipulating Information Jl 2.1 Information Storage 34 2.1.1 Hexadecimal Notation 36 2.1.2 Data Sizes 39 vii 3.2 Program Encodings 169 3.3 3.4 3.5 3.6 3.2.l Machine-Level Code 170 3.2.2 Code Examples 172 3.2.3 Notes on Formatting 175 Data Formats 177· (I ' 11 Accessing Infonrlation 17~ 3.4.l , (!peranq,Speciflers. 180 \" 3.4.2 'Data Movell).ent Instwcti<;ms , 1~2 3.4.3 Data Moyement Exllmple 0 186 3.4.4 Pushing and' Poppi~g Stack Data Arithmetic and Logical Operations 191 3.5.1 Load Effective Address 191 i89 3.5.2 Unary and Bi~ary 9peqtip,J!~r 194 3.5.3 Shift Operations 194 3.5.4 ,. Disc_u~sion, 196 , 3.5.5 Special Arithmetic Operatio11s 197 Control 200' 3.6.1 Condition Codes 201 l J I ' \" f;t I ·'\\ ,, • ' l 3.6.2 Accessing the Condition Codes 202 · • ~· , 3.6.3 Jump Instructions 205 • 3.6.4 Jump Instruction Encodings 207 3.6.5 Impleme9ting Conditional Branches with Conditional ·Control 209 3.6.6 Implementing.Conditional Branches.with Conditional Moves 214 > 3.6.7 Loops 220 3.6.8 Switch Statements 232 3.7 Procedures 238 3.7'.l The Run-Turie, Stack 23~ 3.7.2 Control Transfer 241 3.7.3 Data Transfer 245 3.7.4 3.7.5 3.7.6 Local Storage on the Stack 248 • • J Local Storage m Registers 251 Recursive Procedure~ 253 \" \"· 3.8 :Array Allocati91)-~nd Access 255 3.8.1 Basic Principles 255 3.8.2 Pointer Arithmetic 257 3.8.3 Nested Arrays 258 3.8.4 Fixed-Size Arrays 260, 3.8.5 Variable-Size Arrays 262 , , \" ; l \" Co,ntents- ix { ,, ') ' ,,, 4.3.2 SEQ Hardware Structure 396G0 . ,. '· ' • 1., • ·' 4.3.3 SEQ Timing 400 ·¥. \" ,. '! n•J '\"\" 4.3.4 SEQ Stage Jmplementations 404 11\"' ., ' 4.4 General Principles of Pipelining 412 , l\" 4.4.1 Coi;nputat~onal Pipelines 412· '(It .t/ 4.4.2 A Detailed Look at Pipeline Operation,. il14l 4.4.3 Limit,atiqns of Pipelining 416 4.4.4 <•,l?ipelihingc.a'System with Feedback· 419 1 , :. 4.5 Pipelined Y86'64 Impleinentations• '421·tJ .·11< 4.5.l SEQ+: Rearranging the Computation Stagesu-.421 4.5.2 Insefting.Pipeline Registers 422 , .fi\" 4.5.3 Rearranging and Relabeling Signals 426 4.5.4 Next PC Prediction 427 • > 4.5.5 Pipeline Hazards 429 4.5.6 Exception Handling 444 _, 4.5.7 PIPE Stage Implementations 447 4.5.8 Pipeline Control Logic 455 4.5.9 - Performance Analysis -464. 4,5.10 Unfinisped Business 468 4.6 Summary 470 5 4.6.1 Y86-64 Simulators 472 Bibliographic Notes 473 Homework Problems 473 Solutions to Practice Problems 480 . . ,, Optimizing Program'P'erforniqnc~; 495 5.1 5.2 5.3 5.4 5.5 5.6 5.7 Capabilities and Limitations· of Optimizing Compilers 1198 Expressing Program Performance. ·502 Program Example 504 Eliminating Loop•I:gefficiencies 508 .J• Reducing Procedure Calls 51'.Z.< '\" \" Eliminating Unneeded Memory Referencesu514 ... Understanding Modern Processors ·517,. 5.7.l Overall Operation 518 5.7.2 Functional Unit Performance 523 5.7.3 An Abstract Model of Processor Operation 525 5.8 Loop Unrolling 53.hr\" \"' 5.9 Enhanqing Parallelism 536 5.9.l Multiple.Accumulators 536 5.9.2 Reassociation Transformation 541 ~Contents xi 6.6.1 The Memory Mountain 639 6.6.2 Rearranging Loops to Increase Spatial Locality 643 6.6.3 Exploiting Locality in Your Programs '647 6.7 Summary 648 Bibliographic Notes 648 ' Homework' Problems 649 Solutions to Practice Problems 660 ' Part II Running Programs on a System 7 Linking 669 7.1 Compiler Drivers 671 7.2 Static Linking 672 7.3 Object Files 673 7.4 Relocatable Object Files 674 7.5 SY1Pbols and Symbol.Tables 675 7.6 Symbol Resolution 679 7.6.1 How Linkers Resolve Duplicate Symbol Names 680 7.6.2 Linking with Static Libraries 684 7.6.3 How Linkers Use Static Libraries to Resolve References 688 7.7 Relocation 689 7.7.1 Relocation Entries 690 7.7.2 Relocating Symbol References 691 7.8 d Executable Object Files 695 7.9 Loading Executable Object Files 697 7.10 Dynamic Linking with Shared Libraries 698 ' . 7.11 Loading and Linking Shared Libraries from Applications 701 7.12 Position-Independent Code (PIC) 704 7.13 Library Interpositioning 707 7.13.1 Compile-Time Interpositioning 708 7.13.2 Link-Time Interpositioning 708 7.13.3 Run-Time Interpositioning 710 7.14 Tools for Manipulating Object Files 713 7.15 Summary 713 Bibliographic Notes 714 Homework Problems 714 Solutions to Practice Problems 717 Contents xiii 9.3 VM as a Tool f6r Caching 805 9.3.1 DRAM Cache Organization 806 9.3.2 Page Tables 806 9.3.3 Page Hits, 808 9.3.4 Page Faults 808- 1 9.3.5 Allocating Pages 810 9.3.6 Locality to the Rescue Again 810 9.4 VM as \\I Tool fdr Memory Management 811 9.5 VM as a Tool for Mem'ofy Protection .812 9.6 Address Translation 813 9.6.l Integrati~g Caches andYM 817 9.6.2 Speeding Up Address 'Ifanslation with a TLB 817 9.6.3 Multi-Level Page Tables 819 ( ' 9.6.4 Putting It Together: End-to-End Addrl',s~ Trapslation ~21 9.7 Case Study: The Intel Core i7/Linux Memory System 825 9.7.1 Core i7 Address Translation 826 9.7.2 Linux Virtual Memory System 828 9.8 Memory Mapping 833 9.8.1 Shared Objects Revisited 833 9.8.2 The fork Function Revisited 836 9.8.3 The execve Function Revisited 836 9.8.4 User-Level Memory Mapping with the mmap Function 837 9.9 Dynamic Memory Alloqtion 839 9.9.l 9.9.2 9.9.3 9.9.4 9.9.5 9.9.6 9.9.7 9.9.8 9.9.9 9.9.10 9.9.11 9.9.12 9.9.13 9.9.14 The malloc and free Functions ~40 Why Dynamic Memory Allocation? 843 Allocator Requirements and Goals 844 Fragmentation 846 Implementation Issues 846 Implicit Free Lists 847 Pl~cing Allocated Blocks 849 Splitting· Free Blocks -849' 1 L ' Getting Additional Heap Memory '850 Coalescing Free Blocks 850 Coalescing with Boundary Tags 851 Putting It Together: Implementing a Simple Allocator Explicit Free Lists 862 S_egregated Free Lists 863 9.10 Garbage Collection 865' 9.10.l Garbage Collector Basics 866 9.10.2 Mark&Sweep Garbage Collectors 867 · • 9.10.3 Conservative Mark&Sweep for C Programs 869 ., \" 854 h ' I .. ~ .. Gontents xv xvi Conterits 9.11 Common Memory-Related Bugs in-CPrqgfams 870. 9.11.1 Dereferencing Bad Pointers 870 9.11.2 Reading Uninitialized Memory 871 9.11.3 Allowing Stack Buffer Overflows 871 9.11.4 Assuming That Pointers and the Objects They Point to Are the Same Size 872 9.11.5 Making Off-by-One Errors 872 9.11.6 Referencing a Pointer Instead of the Object It Points f'o 873 9.11.7 'Misunderstanding Pointer Arithmetic &73 9.11.8 Referencing Nonexistent Variables 874 9.11.9 Referencing Data in Free Heap Blocks 874 9.11.10 Introducing Memory Leaks 875. 9.12 Summary 875 Bibliographic Notes 876 Homework Problems 876 Solutions to Practice Pr6blems 880' Part Ill Interaction and Communicatiqn between Programs 10 System-Level I/O 889 10.1 Unix I/O S90 10.2 Files 891 10.3 Opening and Closing Files 893 10.4 Reading and Writing Files 895 10.5 Robust Reading and Writing with the Rm Pack~ge 897_ 10.5.1 Rm Unbuffered Input and Output Functions 897 10.5.2 Rio Buffered Input Function§ 898 10.6 Reading File Metadata 903 10.7 Reading Directory Contents 905 10.8 Sharing Files 206 10.9 I/O Redirection 909 10.10 Standard I/O 911 10.11 Putting It Together: Which I/O Functions Shoul,d I Use? 911 10.12 Summary 913 Bibliographic Notes 914 Homework Problems 914 Solutions to Practice Problems 915 11 Network Programming 917 11.l The Client-Server Programming M9del 918 11.2 Networks 919 11.3 The Global IP Inten;iet 924 11.3.1 IP .:\\dc!resses 925 11.3.2 Internet Domain N'ames 927 11.3.3 Internet Connections 929 11.4 The Sockets Interface 932 11.4.1 Socket Addre,ss Structures 933 11.4.2 The socket Function 934 11.4.3 The connect Function 934 11.4.4 The bind Function 935 r ·~ f 11.4.5 The listen Function 93~ r . 11.4.6 The acc-:pt Function 936 11.4.7 Host and Service Conversion 937 £ I 11.4.8 Helper Functions for the Socket~ Ip.terface 942 11.4.9 Example Echo Client and Server ·944 11.S Web Servers 948 11.5.1 Web Basics 948 11.5.7 Web Content 949 11.5.3 HTIP Transactions 950 11.5.4 Serving Dynamic Content 953 11.6 Putting It Together: The TINY Web Server 956 11.7 Summary 964 Bibliographic Notes 965 Homework Problems 965 Solutions to Practice Problems 966 12 Concurrent Programming 971 12.l Concurrent Programming with Processes 973 12.1.1 A Concurrent Server Based on Processes 974 12.1.2 Pros and Cons of Processes 975 12.2 Concurrent Programming with I/O Multiplexing 977 12.2.l A Concurrent Event-Driven Server Based on I/O Multiplexing 980 12.2.2 Pros and Cons of I/O Multiplexing 985 12.3 Concurrent Programming with Threads 985 12.3.l Thread Execution Model 986 Contents xvii Preface This book (known as CS:APP) is for computer scientists, computer engineers, and others who want to be able to write better programs by learning what is going on \"under the hood'\" of.a computer systefn. Our aim'is to explain the enduring concepts underlying all computer systenls, and to show you the cohcrete ways that these ideas affect the correctn'ess;perfSr- mance,'lmd utility of your application programs.'Mlmy systems bob ks are1\\vritten from a builder's perspective, describing how to implement the hardware or th'e sys- tems software, inC!uding the operating system, compiler, and network·iiltefface. Thisbook is wrihen from\"a progr'dmme\"\"' pefspective,'describing how application programmers can use their knowledge of a system to write better programs. 'Of course, learning what a system i§ supposed to do provide&.a good first step in learn- ing how to build one,'so this book also serves as a· valuable introduction to those who go on to iinplemeht systems hardware and soffware. Most systems books also tend to focus on just one aspect of the system, for example, the•hardware archi- tecture: the operating system, the compiler, or the \"network. Tiiis book\" spans all of·these aspects,' with !lie unifying theme of a progranfmer's perspective. If you·study ancl-learh>.the-concepts-in-this-!:mok~you-.wiU1:le\"on·yoni-wzj>'tcr--- becoming the 'rare power progranimer'who knows how things work'and how io fix them when tbey'bteak. You will· be able to write programs that·make•better use of the'caP,abiij(ies provided by theloperating systeni'and systellis software, that operate 'correctly across'1i wicte' range of operating conditibhs and run'.-t:llne parameters,' that run faster, and that avoid the flaws that make·ptogramS vu!Iler- able t'6 cybefatt'ack-. You will be prepared to delve deeper into 'advanced topics sucn as conipilers;'c6mputer architecture, 'operating sy~tems, embedded systems, networking, and cybersecurity. Assumptions' abou't the Reader's Backgrourid \" This book focuses on systems that execute x86-64 machine code. x86-64 is the latest in an evolutionary path followed by Intel and its competitors .that started with the 8086 microprocessor in 1978. Due to the naming conventions used by Intel for its microprocessor line, this class of microprocessors is referr~d tq coll9ql'ially as \"x86.\" As semiconductor technology has evolved to allow more transistors to be integrated onto a single. chip, these processors have progressed greatly in their computing.ppwer. and theii:.mel)lory capacity. 'As.part of, this ptogression, (hey have gone from pp,erating on· 16-bit 'XQW, to.32-bit.words with the; introduction of IA32 processors, and most recently to 64-bit words with x86-64. We consider how these machines execute C programs on Linux. Linux is.one ~ of a number·of operating systems·having their heritage in the Unix operating system developed originally by Bell Laboratories. Other members-of this class xix --~· ~· ------'----------~-'---'code!intro/hello.c 1' #include <st!dio.h> 2 3 int main() 4 { 5 printf(\"hello, world\\n\"); 6 return O; 7 } -------------~---------- code/introlhel/o.c Figure 1 A typical code exqmple. immeC!iately to test your understanding. Solutions t6 the practice problems are at the end of each chapter. As you read, try to solve each problem on your own and then check the solution to make sure you are on the rigll.t track. Each chapter is fallowed by a s6t of homework problems of varying difficulty. Your instructor has the solutiohs to \\he h6mework p1ob!ems in an instructor's manual. For each homew9rk problem, we show a rating 6f the amount of effort we feel it will require: ' ' + Should require just a few min~t~s. Little or no programming required. ~ • ~ • .1 • ~,t MighVygui,re up, to 20 mip.utes. Often il)volves Wfili.i;ig.and ,t,esting some code. (Many of these,arq'flerived from pn1blems.we ha,ve given on exams.) +++ Requires a sjgnificant;,effort 1 perhaps1-2 hours. Generally in\\lolves writ- ing and testing a significant amount of code .. ++++ A lab assignment, requiring up to 10 hours of effort. Each eode example in the te;t was formatted directly, Without any manual intervention, from a C program compile<i\":ith clC<; and' tested on a Lin ID:· system. of\"course, your system may have a different version of ace, or a different compiler • ' ' '<} altogether, so your compiler might generate different \\fiachme code; but the overall behavior should be the same. All bf the.source code!i§' available from the CS:APP Web page (\"CS:APP\" being our shorthahd fot 146 book's title) at csapp .cs.cmu.edu. In the text, the filenarlies 6f the so'urce progrAhts are docuinented in horizont111 bars that sutround the formatted code. For example; the program in Figure' I can be found in the file hello. c in directory code\"f intro/. We encourage yoli1o try rurming the example programs on your system as you encounter them. To avoid.having a.book that is overwhelming, both irrbulk and in content, we have:cre'ated•a'number of.w.rb asides containing matetial that.supplements the main presentatioh of1ha boolo.,These asides are referenced!Within the book with a notation'of the form CHAE:TOP, where CHAP is ashoi;~ encoding of the chapter sub- ·iect, and TOPds.a shott-cocje·fonthe topic-that is covered.·For example, Web Aside DATK.BOOL contains supplementary material on·Booleanalgebra for the presenta- tion on data representations in Chapter 2, while Web Aside ARCH:VLOG contains Preface ,xxi xxii Preface I,, I material describing processor designs using the Verilog hardware description lan- guage, supplementing the presentation of processor design in Chapter 4. All of these Web asides are available from the CS:APP Web page. Book Overview The CS:APP book consists of 12 chapters designed to capture the core ideas in computer systems. Here is an overview. Chapter 1: A Tour of Computer Systems. This chapter introduces the major ideas and themes in computer systems by tracing the life cycle of a simple \"hello, world\" program. Chapter 2: Representing and Manipulating Information. We cover computer arith- metic, emphasizing the properties of unsigned and two's-complement num- ber representations that affect programmers. We consider how numbers are represented and therefore what range of values can be encoded for a given word size. We consider the effect of casting between signed and unsigned numbers. We cover the mathematical properties of arithmetic op- erations. Novice programmers are often surprised to learn that the (two's- complement) sum or product of two positive numbers can be negative. On the other hand, two's-complement arithmetic satisfies many of the algebraic properties of integer arithmetic, and hence a compiler can safely transform multiplication by a constant into a sequence of shifts and adds. We use the bit-level operations of C to demonstrate the principles and applications of Boolean algebra. We cover the IEEE floating-point format in terms of how it represents values and the mathematical properties of floating-point oper- ations. Having a solid understanding of computer arithmetic is critical to writ- ing reliable programs. For ex~ple, programmers and compilers cannot re- place the expression (x<y) with (x-y < O), due to the possibility of overflow. They cannot even replace it with the expression (-y < -x), due to the asym- metric range of negative and positive numbers in the two's-complement representation. Arithmetic overflow is a common source of programming errors and security vulnerabilities, yet few other books cover the properties of computer arithmetic from a programmer's perspective. Chapter 3: Machine-Level Representation of Programs. We teach you how to read the x86-64 machine code generated by a C compiler. We cover the ba- sic instruction patterns generated for different control constructs, such as conditionals, loops, and switch statements. We cover the implementation of procedures, including stack allocation, register usage convention~, and parameter passing. We cover the way different data structures such as struc- tures, unions, and arrays are allocated and accessed. We cover the instruc- tions that implement both integer and floating-point arithmetic. We also use the machine-level view of programs as a way to understand common code se~urity vulnerabilities, such as buffer overflow, and steps that the pro- ering this material serves several purposes. It reinforces the concept that the virtual memory spac~ is just an array of bytes that the program can subdivide info different storage units. It helps you understand thy effects of programs containing memory referencing errors such as storage leaks an'a invalid pointer references. Ffnaliy;many a~plication programmers write 'their own' storage allocators optimized toward the needs and characteris- tics of the application. This chapter, more than any other, dem8nstrates the benefit of covering lioth the'hardw'are and the foftware aspec:ts'bf computer systems in a unified way. Traditional computer architecture and operating systems texts present only part of the virtual memory story. Chapter 10: System-Level 110. We cover the basic concepts of Unix I/O such as files and descriptors. We describe how files are shared, how I/O redirection works, and how to access file metadata. We also develop a robust buffered I/O package that deals coqect)y with a curious behavior known as. s/iort counts, where the library function reads only parb of the iµput data. We cover the C standard I/O lib~ary and its relationship to Linux I/O, focusing on limitations of standard I/O that make it unsuitable fbr'network program- ming. In .general, the topics covered in this'chapter are-building blocks for 'the next two c)lapters on network and concurrent programming . .... t ~ • ' Chapter JJ; Network Programming. Networks are interesting I/O clevices to pro- gram, tying together many of the ideas that we study earlier in'the text, such as processes, signals, byte ordering, memory mapping, and dynamic storage allocation. Net)V6fk programs also provide a -compelling context for con- currerlc~, which is the topic of the next chapfe'r. 'This chapter' is· a thin slice through network programmin'_g that' gets you to the' pbilli wbere you can w'rite'a simple Web server. We cover tlie client-server model that underlies all network applications. We preselit a programmer's view of the Internet and show how to write Internet clients 'ahll servers using the sockets inter- face. Finally, we introduce HTTP and develop a simple iterative Web server. Chapter 12: Concurrent Programming. This cha'pt'er iniroduces concurrent pro- gramming using Internet'sei:ver design as the tunifing motivational·example. We compare and contrast the three basic mechanisms for writing concur- rent programs-;;P,rocesses'II/O multiplexing, al\\d thread,s-al).d sh9w how to use them to build concurrent Intyrn~t servers. We cover basic principles of synchr0nization,usipg P and y semaphore opera,tions,,thrp,ad safety and , ,. reyntrancy, race condition~, and deadloc,ks. Writing concur,rent code is es- sential for mo~t server appl1cations. We also nescribe the use of thread-level IJ I ror\"l'Jit I { '} programming to express parallelism in.an applicatiop.,.pro!f~m, enabling faster execution on multi-core processors. Getting all of the cores working on a single computational P.roblem requires a. careful coordination of the concurrent threads, both fo; correctness and to achieve high performance. Preface xxv Chapter 7: Linking. We have rewritten this chapter. for x86-64, expanded the discussion of using the GOT and PLT to create position-independent code, ansl.added a new section on a powerful linking techniqiie known as library interpositioning. Chppt~r 8.!'Exr~ptio'!'al Control Flow .. We have addpd a more rigorous treatment ,,1 of sig~a.l',hand~~rs, including as~c-sig9al-sa\\f functions'. specific guidelines {or wntm&,sll!J!~l ):landlers, and usmg sigsuspend to wait for handlers. Chapter 9: Virtual Memory. This chapter has changed only slightly. <;f'apter,10: Sy-;iem-Level 110. We hav,e added 'I new section on files and the file (~ ~ } ' U I hierarchy, but 9therwise, this chapter has changed only slightly. Chapter JI: Network Programming. We have introduced techniques for protocol- independent and thread-safe network programming using the modern getaddrinfo and getnameinfo functions, which replace the obsolete and non-reentrant gethostbyname and gethostbyaddr functions. Chapter 12: Concurrent Programming. We have increas,ed our coverage of using thread-level parallelism to make programs run faster on multi-core ma- chines. In addition, we have added and revised a number of practice and homework problems throughout the text. Origins of the ~ook This book stems from an introductory course that we developed at Carnegie Mel- lon University in the fall of1998, called 15-213: Introduction to Computer Systems (rCS) [14]. The res cburse has' been taught every semester since then. Over 400 students.take the; course each semester. Jbe students range from sophomores to gra<;luate ~tudents ip a wide variety of majors. It is a required core course for all undergradu~t<;s in the C~ and ECE departments at Carnegie Mellon, and it has become a prerequisite for most upper-level systems courses in CS and ECE. The idea with res was to introduce students to computers in a different way. Few of our students would have the opportunity to build a computer system. On the other frand, most students, inchtding all computer scientists and computer engineers, w'ould be required to use arid program computers on a daily basis. So we decided to teach about systems from the point of view of the programmer, using the following filter: we would cover a iOpic only if it affected the performance, correctness, or. utility of user-level C programs. For example, topics such as hardwate adder and bus designs were out. Top- ics such as machine language were in; but instead of focusing on how to write assembly language by hand, we would look at how a C compiler translates C con- structs into.machine code, includipg pointers, loops, procedure calls, and switch statements. Further, we wo'uld take a broader and more holistic view of the system as both hardware and systems software, covering such topics as linking, loading, Preface xxvii mands. This is the student'sdirst introduction to concurrency, and it gives them a clear idea of Unix process control, signals, and signal ·handling. Ma/Joe Lab. Students implement their myn versions• of ~ailoc, free, and'( op- tionally) realfoc. This lab gives students a clear understanding of data layout anCI organization, and requires them to evaluate different trade-offs between space anCI time 1 efficiency. Proxy Lab. •Students implement a concl\\rr.ent1 Web ,proxy tha~ sits between their , , browsers and the rest of the World Wide Web. '!his lab exposes the students to such topics as Web clients and servers, and ties together many of the con- cepts from the course, such as byte ordering, file 1/0, process control, signals, signal handling, memory mapping, sockets, and concurrency. Students like being able to see their programs in action with real Web browsers and Web servers. , t. ·, The CS:APP instructor's manual has a detailed discussion of the labs, as well as directions for downloading the support software . • Acknowledgments for the Third Edition It is a pleasure to acknowledge and thank those who have helped us produce this third edition of.the CS:APP text. We would like to thank our Carnegie Mellon colleagues who have taught the JCS course over the years and wlio have provided so much in'sightful feedback and encouragement: Guy Blelloch, Roger Dannenberg, David Eckhardt, Franz Franchetti, Greg Ganger, Seth Goldsteirt, Khale!f Harras,,Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, Markus Pueschel, and Anthony Rowe. David Wirtters was very helpful in installing and configuring the reference Linux box. ., Jason Fritts (St. Louis University) and• i=;indyi Norris (Appalachian State) provided us with detailed and thoughtful reviews of the second edition. Yili Gong (Wuhan University) wrote the Chinese'translation, maintained the errata page for the Chinese.version, and contributed many bug reports. Godmar Back (Virginia Tech) helped us improve the text significantly by introducing us to the.notions of async-signal safety and protocoi'independeb.M1etwork programming. Many. thanks to our eagle-eyed readers who reported bugs in the second edi- tion: Rami Ammari, Paul Anagn6stopoulos, Lucas Biirenfiinger, Gotlmar Back, Ji Bin, Sharbel Bousemaan, Ric'hard..callahan, Seth Chaiken, Cheng Chen, Libo Chen; Tao Du, Pase~! Garcia, Yili .Gong, Ronald Greenberg, Dorukhan Gill6z, Dong Han,· Dominik Helm, Ronald Jones, Mustafa Kazdag!i, Gordon Kindlmann, Sankar Krishnan, Kianak Kshetri, Jun!in Lu, Qiangqiang Luo, Sebastian Luy, Lei Ma, Ashwin Nanjappa, Gr~goir~ Paradis, Jonas Pfenninger, Karl Pichotta, David Ramsey, Kaustabh Roy, David.Selvaraj, Sankar Shanmugam, Dbminique Smulkowska, Dag S111rb111, Michael Spear, Yu Tanaka, Steven Tricanowicz, Scott Wright, Waiki Wright, Han Xu, Zhengshan Yan, Firo Yang, Shuang·Yang, John Ye, Taketa Yoshida, Yan Zhu, and Michael Zink. Preface xxxi xxxii Preface Thanks also to our readers who have contributed to the labs, including God- mar Back (VJrginia Tech), Taymo'n Beal (Worcester Polytechnic Institute), Aran Clauson (Western Washington University), Cary Gray (Wheaton College), Paul ' Haiduk (West Texas A&M University), Len Hamey (Macquarie University), Ed- die Kohler (Harvard), Hugh Lauer (Worcester Polytechnic Institute), Robert Marmorstein (Longwood University), and James Riely (DePaul University). Once again, Paul Anagnostopoulos of Windfall Software did a masterful job of typesetting the book and leading the production process. Many thanks to Paul and his stellar team: Richard Camp (copy editing), Jennifer McClain (proofread- ing), Laurel Muller (art production), and Ted Laux (indexing). Paul even spotted a bug in our description of the origins of the acronym BSS that had persisted undetected since the first edition! Finally, we would like to thank our friends at Prentice Hall. Marcia Horton and our editor, Matt Goldstein, have been unflagging in their support and encour- agement, and we are deeply grateful to them. Acknowledgments from the Second Edition We are deeply grateful to the many people who have helped us produce this second edition of the CS:APP text. First and foremost, we would like to recognize our colleagues who have taught the ICS course at Carnegie Mellon for their insightful feedback and encourage- ment: Guy Blelloch, Roger Dannenberg, David Eckhardt, Greg Ganger, Seth Goldstein, Greg Kesden, Bruce Maggs, Todd Mowry, Andreas Nowatzyk, Frank Pfenning, and Markus Pueschel. Thanks also to our sharp-eyed readers who contributed reports to the errata page for the first edition: Daniel Amelang, Rui Baptista, Quarup Barreirinhas, Michael Bombyk, forg Brauer, Jordan Brough, Yixin Cao, James Caroll, Rui Car- valho, Hyoung-Kee Choi, Al Davis, Grant Davis, Christian Dufour, Mao Fan, Tim Freeman, Inge Frick, Max Gebhardt, Jeff Goldblat, Thomas Gross, Anita Gupta, John Hampton, Hiep Hong, Greg Israelsen, Ronald Jones, Haudy Kazemi, Brian Kell, Constantine Kousoulis, Sacha Krakowiak, Arun Krishnaswamy, Mar- tin Kulas, Michael Li, Zeyang Li, Ricky Liu, Mario Lo Conte, Dirk Maas, Devon Macey, Carl Marcinik, Will Marrero, Simone Martins, Tao Men, Mark Morris- sey, Venkata Naidu, Bhas Nalabothula, Thonias Niemann, Eric Peskin, David Po, Anne Rogers, John Ross, Michael Scott, Seiki, Ray Shih, Darren Shultz, Erik Silkensen, Suryanto, Emil Tarazi, Nawanan Theera-Ampornpunt, Joe Trdinich, Michael Trigoboff, James Troup, Martin Vopatek, Alan West, Betsy Wolff, Tim Wong, James Woodruff, Scott Wright, Jackie Xiao, Guanpeng Xu, Qing Xu, Caren Yang, Yin Yongsheng, Wang Yuanxuan, Steven Zhang, and D~y Zhong. Special thanks to Inge Frick, who identified a subtle deep copy bug in our lock-and-copy example, and to Ricky Liu for his amazing proofreading skills. Our Intel Labs colleagues Andrew Chien and Limor Fix were exceptionally supportive throughout the writing of the text. Steve Schlosser graciously provided some disk dri~e characterizations. Casey Helfrich and Michael Ryan installed g and maintained our· new Core i7 box .. Michael Kozuch, Babu Pillai, and Jason Campbell provided valuable insight !On memory system performance, multi-core systems, and the power wall. Phil Gibbons and Shimin Chen shared their consid; erable expertise on solid state disk. designs. We have·been able to call on.the talents' of many, including Wen-Mei Hwu; Markus Pueschel, and Jiri Sinisa, to provide both detailed comments and high- level advice. James Hoe helped us create a Verilog version of the Y86 processor and did all of the'work needed to sy~thesize working hardware. Many thanks to our colleagues who..provided reviews,0£ the draft manu- script: James Archibald (Brigham Yonng University), Richard Carver (George Mason University), Mirela Damian (Villanova University), Peter Dinda (North- western University), John Fiore (Temple University), Jason Fritts (St: Louis Uni- versity), John Greiner (Rice University), Brian Harvey (University of California, Berkeley), Don Heller (Penn State Unive~sity), Wei.Chung Hsu (University of Minnesota), Michelle Hugue (University of Maryland),.Jeremy Johnson (Drexel University), Geoff Kuenning (Harvey Mudd College), Ricky Liu, Sam.Mad- d.en (MIT), Fred Martin (University otMassachusetts, Lowell), Abraham Matta (Boston University), Mqrkus Pueschel (Carnegie Mellon University), Norman Ramsey (Tufts University), Glenn Reinmann (UCLA): Michela Taufer (Univer- sity of Delaware), and Craig Zilles (UIUC). Paul Anagnostopoulos of Windfall Software did an outstanding job of type- setting the book and leading the production team. Many thanks to Paul and his superb team: Rick Camp ( copyeditor), Joe Snowden (compositor), Mary Ellen N. Oliver (proofreader); Laurel Muller (artist), andTed Laux (indexer). Finally, we would like to thank our friends at Prentice Hall: Marcia Horton has always been there for us. Our editor, Matt Goldstein, provided stellar leadership from·beginning to end. We are profoundly grateful for their help, encouragement, and insights. Acknowledgments from the First Edition We are deeply indebted to many friends and colleagues for their thoughtful crit- icisms and encouragement. A special thanks to our 15-213 students, whose infec- tious energy and enthusiasm spurred us on. Nick Carter and Vinny Furia gener- ously provided their malloc package. Guy Blelloch, Greg Kesden, Bruce Maggs, and Todd Mowry taught the course over multiple semesters, gave us encouragement, and helped improve the course material. Herb Derby provided early spiritual guidance and encouragement. Al- lan Fisher, Garth Gibson, Thomas Gross, Satya, Peter Steenkiste, and Hui Zhang encouraged us to develop the course from the start. A suggestion from Garth early on got the whole ball rolling, and this was picked up and refined with the help of a group led by Allan Fisher. Mark Stehlik and Peter Lee have been very supportive about building this material into the undergraduate curriculum. Greg Kesden provided helpful feedback on the impact of ICS on.the OS course. Greg Ganger and Jiri Schindler. graciously provided some disk drive characterizations Preface xxxiii ~' - ~ - - -- - I ,, !I~ xx xiv Preface and answered our questions on modern disks. Tom Stricker showed us the mem- ory mountain. James Hoe provided useful ideas and feedback on how to present processor architecture. A special group of students-Khalil Amiri, Angela Demke Brown, Chris Calahan, Jason Crawford, Peter Dinda, Julio Lopez, Bruce Lowekamp, Jeff Pierce, Sanjay Rao, Balaji Sarpeshkar, Blake Scholl, Sanjit Seshia, Greg Stef- fan, Tiankai Tu, Kip Walker, and Yinglian Xie-were instrumental in helping us develop the content of the course. In particular, Chris Calahan established a fun (and funny) tone that persists to this day, and invented the legendary \"binary bomb\" that has proven to be a great tool for teaching machine code and debugging concepts. Chris Bauer, Alan Cox, Peter Dinda, Sandhya Dwarkadas, John Greiner, Don Heller, Bruce Jacob, Barry Johnson, Bruce Lowekamp, Greg Morrisett, Brian Noble, Bobbie Othmer, Bill Pugh, Michael Scott, Mark Smotherman, Greg Steffan, and Bob Wier took time that they did not•have to read· and advise us on early drafts of the book. A very special thanks to Al Davis (University of Utah), Peter Dinda \\Northwestern University), John Greiner (Rice University), Wei Hsu (University of Minnesota), Bruce Lowekamp'(College of William & Mary), Bobbie Othmer (University of Minnesota), Michael Scott (University of Rochester), and Bob Wier (Rocky Mountain College) for class testing the beta version. A special thanks to their students as well! We would also like to thank our colleagues at Prentice Hall. Marcia Horton, Eric Frank, and Harold Stone have been unflagging in their support and vision. Harold also helped us present an accurate historical perspective on RISC and CISC processor architectures. Jerry Ralya provided sharp insights and taught us a lot about good writing. Finally, we would like to acknowledge the great technical writers Brian Kernighan and the late W. Richard Stevens, for showing us that technical books can be beautiful. Thank you all. Randy Bryant Dave O'Hallaron Pittsburgh, Pennsylvania Chapter 3 Machine-Level Representation of Programs 165 machine code has shifted over the years from one of being able to write programs directly in assembly code lo one of being able to read and understand the code generated by compilers. In this chapter, we will learn the details of one particular assembly language and see how C programs get compiled into this form of machine code. Reading the assembly code generated by a compiler involves a different set of skills than writing assembly code by hand. We must understand the transformations typical compilers make in converting the constructs of C into machine code. Relative to the computations expressed in the C code. optimizing compilers can rearrange execution order. eliminate unneeded con1putations, replace slow operations with faster ones, and even change recursive computations into iterative ones. Under- standing the relation between source code and the generated assembly can often be a challenge-it's much like putting together a puzzle having a slightly differ- ent design than the picture on the box. It is a form of reverse engineering-trying to understand the process by which a system was created by studying the system and working backward. l n this case. the system is a machine-generated assembly- language program, rather than something designed by a human. This simplifies the task of reverse engineering because the generated code follows fairly regu- lar patterns and we can run experiments, having the compiler generate code for many different programs. In our presentation, we give many examples and pro- vide a number of exercises illustrating different aspects of assembly language and compilers. This is a subject where mastering the details is a prerequisite to under- standing the deeper and more fundamental concepts. Those who say'\"! understand the general principles, I don't want to bother learning the details\" arc deluding themselves. It is critical for you to spend time studying the examples, working through the exercises, and checking your solutions with those provided. Our presentation is based on x86-64, the machine language for most of the processors found in today's laptop and desktop machines. as well as those that power very large data centers and supercomputers. This language has evolved over a long history, starting witl1 Intel Corporation's first 16-bit processor in 1978. through to the expansion to 32 bits. and most recently to 64 bits. Along the way, features have been added to make better use of the available semiconductor tech- nology, and to satisfy the demands of the marketplace. Much of the development has been driven by Intel. but its rival Advanced Micro Devices (AMD) has also made important contributions. The result is a rather peculiar design with features that make sense only when viewed from a historical perspective. It is also laden with features providing backward compatibility that are not used by modern com- pilers and operating systems. We will focus on the subset of the features used by ace and Linux. This allows us to avoid much of the complexity and many of the arcane features of x86-64. Our technical presentation starts with a quick tour to show the relation be- tween C, assembly code, and machine code. We then proceed to the details of x86-64, starting with the representation and manipulation of data and the imple- mentation of control. We see how control constructs in C, such as if, while, and switch statements, arc implemented. We then cover the implementation of pro- cedures. including how the program maintains a run-time stack to support the l I I I I I • Section 4.2 Logic Design and the Hardware Control Language HCL 375 Figure 4.11 Single-bit multiplexor circuit. The output will equal input a if the control signal s is 1 and will equal input b when s is 0. \"' b --i'--!----1 / out selects a value from among a set of different data signals, depending on the value of a control input signal. In this single-bit multiplexor, the two data signals are the input bits a and b, while the control signal is the input bit s. The output will equal a whens is 1, and it will equal b whens is 0. In this circuit, we can see that the two AND gates determine whether to pass their respective data inputs to the OR gate. The upper AND gate passes signal b whens is 0 (since the other input to the gate is ! s), while the lower AND gate passes signal a whens isl. Again, we can write an HCL expression for the output signal, using the same operations as are present in the combinational circuit: bool out= (s && a) II (!s && b); Our HCL expressions demonstrate a clear parallel between combinational logic circuits and logical expressions in C. They both use Boolean operations to compute functions over their inputs. Several differences between these two ways of expressing computation are worth noting: • Since a combinational circuit consists of a series of logic gates, it has the property that the outputs continually respond to changes in the inputs. If some input to the circuit changes, then after some delay, the outputs will change accordingly. By contrast, a C expression is only evaluated when it is encountered during the execution of a program. • Logical expressions in Callow arguments to be arbitrary integers, interpreting 0 as FALSE and anything else as TRUE. In contrast, our logic gates only operate over the bit values 0 and l. • Logical expressions in C have the property that they might only be partially evaluated. If the outcome of an AND or OR operation can be determined by just evaluating the first argument, then the second argument will not be evaluated. For example, with the C expression (a && !a) && func(b,c) the function func will not be called, because the expression (a && ! a) evalu- ates to 0. In contrast, combinational logic does not have any partial evaluation rules. The gates simply respond to changing inputs. Section 4.3 Sequential Y86-64 Implementations 385 Decode. The decode stage reads up to two operands from the register file, giving values valA and/or valB. 'I}'pically, it reads the registers designated by instruction fields rA and rB, but for some instructions it reads register %rsp. Execute. In the execute stage, the arithmetic/logic unit (ALU) either performs the operation specified by the instruction (according to the value of ifun), computes the effective address of a memory reference, or increments or decrements the stack pointer. We refer to the resulting value as valE. The condition codes are possibly set. For a conditional move instruction, the stage will evaluate the condition codes and move condition (given by ifun) and enable the updating of the destination register only if the condition holds. Similarly, for a jump instruction, it determines whether or not the branch should be taken. Memory. The memory stage may write data to memory, or it may read data from memory. We refer to the value read as valM. Write back. The write-back stage writes up to two results to the register file. PC update. The PC is set to the address of the next instruction. The processor loops indefinitely, performing these stages. In our simplified im- plementation, the processor will stop when any exception occurs-that is, when it executes a halt or invalid instruction, or it attempts to read or write an invalid ad- dress. In a more complete design, the processor would enter an exception-handling mode and begin executing special code determined by the type of exception. As can be seen by the preceding description, there is a surprising amount of processing required to execute a single instruction. Not only must we perform the stated operation of the instruction, we must also compute addresses, update stack pointers, and determine the next instruction address. Fortunately, the overall flow can be similar for every instruction. Using a very simple and uniform struc- ture is important when designing hardware, since we want to minimize the total amount of hardware and we must ultimately map it onto the two-dimensional surface of an integrated-circuit chip. One way to minimize the complexity is to have the different instructions share as much of the hardware as possible. For example, each of our processor designs contains a single arithmetic/logic unit that is used in different ways depending on the. type of instruction being exe- cuted. The cost of duplicating blocks of logic in hardware is much higher than the cost of having multiple copies of code in software. It is also more difficult to deal with many special cases and idiosyncrasies in a hardware system than with software. Our challenge is to arrange the computing required for each of the different instructions to fit within this general framework. We will use the code shown in Figure 4.17 to illustrate the processing of different Y86-64 instructions. Figures 4.18 through 4.21 contain tables describing bow the different Y86-64 instructions proceed through the stages. It is worth the effort to study these tables carefully. They are in a form that enables a straightforward mapping into the hardware. Each line in these tables describes an assignment to some signal or stored state 950 Chapter 11 Network Programming identifies an HTML file called /index.html on Internet host www.google.com that is managed by a Web server listening on port 80. The port number is op- tional and defaults to the well-known HTTP port 80. URLs for executable files can include program arguments after the filename. A '?' character separates the filename from the arguments, and each argument is separated by an '&' character. For example, the URL http://bluefish.ics.cs.cmu.edu:8000/cgi-bin/adder?15000&213 identifies an executable called I cgi -bin/ adder that will be called with two argu- ment strihgs: 15000 and 213. Clients and servers use different parts of the URL during a transaction. For instance, a client uses the prefix http://www.google.com:SO to determine what kind of server to contact, where the server is, and what port it is listening on. The server uses the suffix I index . html to find the file on its filesystem and to determine whether the request is for static or dynamic content. There are several points to understand about how servers interpret the suffix ofa URL: • There are no standard rules for determining whether a URL refers to static or dynamic content. Each server has its own.rules for the files it manages. A classic (old-fashioned) approach is to identify a set of directories, such as cgi-bin, where all executables must reside. • The initial'/' in the suffix does not denote the Linux ro'of directory. Rather, it denotes the home directory for whatever kind of content is being requested. For exampl~, a. server might be cqnfigured so that all stadc c01{tent is stor~d in directory /usr /httpd/html and all dynamic content is stored in directory /usr/httpd/cgi-bin. • The minimal URL suffix is the'/' character, which all servers expand to some default home page such as I index·. html. This explains why it is possible to fetch the'home page of a site liy simply typing a domain name'to the browser. The browser appends the missing '/' to the URL and passes it to the server, which expands the '/' to sm:ne default filename. 11.5.3 HTIP Transactions Since HTTP is based on text Jines transmitted over Internet connections, we can use the Linux TELNET program to conduct transactions with any Web server on the Internet. The TELNET program·has been largely supplanted by SSH as a remote login tool, but it is very handy for debugging servers that talk to clients with text Jines over connections. For example, Figure 11.24 uses TELNET to request the home page from the AOL Web server. 1070 Index mispredicted branches handling, 443-444 performance penalties, 467, 520i 549-553 miss rates, 631 misses, caches, 470, 612 kinds, 612--013 penalties, 632, 806 rates, 631 mkdir command, 892, mm_coalesce [CS:APP] allocator: boundary tag coalescing, 860 mm_free [ CS:APP] allocator: free heap block, 860 mm-ijk [CS:APP] matrix multiply ijk, 645 mm-ikj [CS:APP] matrix multiply ikj, 645 mm_ini t [CS:APP] allocator: initialize heap,858 mm-jik [CS:APP] matrix multiply jik, 645 mm-jki [CS:APP] matrix multiply jki, 645 mm-kij [CS:APP] matrix multiply kij, 645 mm-kj i [CS:APP] matrix multiply kji, 645 mm_malloc [CS:APP] allocator: allocate heap,block, 860, 861 r:tmap [Unix] map disk obje~t into memory, 837, 837-839 MMUs (memory management units), 804,807 MMX media instructions, 167, 294 Mockapetris, Paul, 931 mode bits, 735 modern processor performance, 518-531 modes kernel, 726, 728 processes, 734-736, 735 user, 72fJ, 728 modified sequential processor implementation, 421--422 modular arithmetic, 85-86,..89 modules DRAM, 584, 585 object, 673 monitors, Java, 1010 monotonicity assumption, 846 monotonicity property, 124 :tvloore,G-ordon,169 Moore's Law, 169, 169 MOSAIC browser, 949 motherboards, 9 Motorola RISC processors, 363 MOV [instruction class] move data, 182, 182-183 i< movabsq [x86-64}move absolute quad word, 183, 183 movb [x86-64] move byte, 183, move absolute quad word instruction, 183,183 move aligned, packed double precision instruction, 296 move aligned, packed single precision instruction, 296 l move and sign-extend instruction, 184, 185 move byte instruction, 183 move data instructions, 182-189 move double precision instruction, 296 move double word instruction, 183 move if even parity instruction, 324 move if greater instruction,.217, 357 move if greater or equal'inStruction, 217,357· move if less instruction, 217, 357 move if less or equal instruction, 217, 357, move if negative instruction, 217 move if nonnegative instruction, 217 move if not equal instruction, 21 \"i/, 357 move if not greater instruction, 217 move if not greater or equal instruction, 217 move if not less instruction, 217 move if not less or equal instruction, 217''' move if not unsigned greater instruction, 217 move if not unsigned less instruction, 217 move if not unsigned less or equal instructiofi, 217 move if not zero inStructidn~..-217 move if unsigned greater instruction, 217 move if unsigned greater or.equal instruction~ 217 move if unsigned less instruction, 217 move if unsigned less •of' equal instruction, 217 move if zero instruction, 217 move instructions, conditional; 214- 220, 55Cl-553' move quad word instruction, 183 move sign-extended byte to double word instruction, 185 move sign-extended byte to quad word instruction, 1.85 move sign-extended byte to word instruction, 185 move sign-extended double word 1o· quad word instruction, 185 move sign-extended word to double word instruction, 185 move sign-ex1erlded word to quad word ihstruction, 185 move single precision instruction~'296 move when equal instruction, 357 move with zero extension instruction, 184,184 move word instruction, 183~ move zero-extended byte to~double word instruction, 184 move zero-extended byte to quad word instruction, 184 move zero-extended byte to word instruction, 184 move zero-extended word to double word instruction, 184 ;' move zero-extended word to quad word instruction, 184 movement operations, floating-point code, 296-301 ' movl [x86-64] move double word, 183 movq [x86-64] move quad word, 183 Mo vs [instruction class] move and sign-extend, 184, 185 movsbl [x86-64] move sign-extended byte to double word, 185 movsbq [x86-64] move sign-extended byte to quad word, 185 movsbw [x86-64] move sign-extended byte to word, 185 movslq [x86-64] move sign-extended double word to quad word, 185 movswl [x86-64] move sign-extended word to double word, 185 movswq [x86-64] ruove sign-extended , word tO quad word, 185 movw [x86-64] move word, 183 MOVZ [instruction class] move with, zero extension, 184, 184 movzbl [x86-64] move zero-extended byte to double word, 184 movzbq [x86-64] move zero-extended byte to quad word, 184 movzbw [x86-64] move zero-extended· byte to word, 184 ,,, movzwl'[x86-64] move zero-ext6nded word to double word, 184 movzwq•[x86-64] move zero-extended word to quad word, 184 \"","libVersion":"0.2.1","langs":""}