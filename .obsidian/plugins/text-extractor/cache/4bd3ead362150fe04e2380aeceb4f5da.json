{"path":".obsidian/plugins/text-extractor/cache/4bd3ead362150fe04e2380aeceb4f5da.json","text":"CPSC 340: Machine Learning and Data Mining More Regularization Last Time: L2-Regularization â€¢ We discussed regularization: â€“ Adding a continuous penalty on the model complexity: â€“ Best parameter Î» almost always leads to improved test error. â€¢ L2-regularized least squares is also known as â€œridge regressionâ€. â€¢ Can be solved as a linear system like least squares. â€“ Numerous other benefits: â€¢ Solution is unique, less sensitive to data, gradient descent converges faster. Regularizing the y-Intercept? â€¢ Should we regularize the y-intercept? â€¢ No! Why encourage it to be closer to zero? (It could be anywhere.) â€“ You should be allowed to shift function up/down globally. â€¢ Yes! It makes the solution unique and it easier to compute â€˜wâ€™. â€¢ Compromise: regularize by a smaller amount than other variables. Parametric vs. Non-Parametric Transforms â€¢ Weâ€™ve been using linear models with polynomial bases: â€¢ But polynomials are not the only possible bases: â€“ Exponentials, logarithms, trigonometric functions, etc. â€“ The right basis will vastly improve performance. â€“ If we use the wrong basis, our accuracy is limited even with lots of data. â€“ But the right basis may not be obvious. Parametric vs. Non-Parametric Transforms â€¢ Weâ€™ve been using linear models with polynomial bases: â€¢ Alternative is non-parametric bases: â€“ Size of basis (number of features) grows with â€˜nâ€™. â€“ Model gets more complicated as you get more data. â€“ Can model complicated functions where you donâ€™t know the right basis. â€¢ With enough data. â€“ Classic example is â€œGaussian RBFsâ€ (â€œGaussianâ€ == â€œnormal distributionâ€). â€¢ Gaussian RBFs are universal approximators (compact subets of â„d) â€“ Enough bumps can approximate any continuous function to arbitrary precision. â€“ Achieve optimal test error as â€˜nâ€™ goes to infinity. Gaussian RBFs: A Sum of â€œBumpsâ€Gaussian RBFs: A Sum of â€œBumpsâ€ â€¢ Polynomial fit: â€¢ Constructing a function from bumps (â€œsmooth histogramâ€): Gaussian RBFs: A Sum of â€œBumpsâ€ â€¢ Red is weight*feature, green is prediction (sum of red lines): Gaussian RBF Parameters â€¢ Some obvious questions: 1. How many bumps should we use? 2. Where should the bumps be centered? 3. How high should the bumps go? 4. How wide should the bumps be? â€¢ The usual answers: 1. We use â€˜nâ€™ bumps (non-parametric basis). 2. Each bump is centered on one training example xi. 3. Fitting regression weights â€˜wâ€™ gives us the heights (and signs). 4. The width is a hyper-parameter (narrow bumps == complicated model). Gaussian RBFs: Formal Details â€¢ What is a radial basis functions (RBFs)? â€“ A set of non-parametric bases that depend on distances to training points. â€“ Have â€˜nâ€™ features, with feature â€˜jâ€™ depending on distance to example â€˜iâ€™. â€¢ Typically the feature will decrease as the distance increases: â€¢ What is a radial basis functions (RBFs)? â€“ Most common choice of â€˜gâ€™ is Gaussian RBF: â€¢ Variance Ïƒ2 is a hyper-parameter controlling â€œwidthâ€. â€“ This affects fundamental trade-off (set it using a validation set). â€“ Why donâ€™t we have 2ğœ‹ğœ in the above formula? â€¢ If you do not regularize it does not matter: â€“ If â€˜vâ€™ is least squares solution with features zi, then ( 2ğœ‹ğœ)v is solution with features (1/ 2ğœ‹ğœ)zi. â€“ So you get the same predictions (least squares is invariant to scaling of features). â€¢ If you regularize it â€œsort ofâ€ matters: â€“ It changes the effect of a fixed Î». â€“ But the regularization path is the same, so if you search for the best Î» you get same predictions. Gaussian RBFs: Formal DetailsGaussian RBFs: Formal Details â€¢ What is a radial basis functions (RBFs)? â€“ The training and testing matrices when using RBFs: Gaussian RBFs: Pseudo-CodeNon-Parametric Basis: RBFs â€¢ Least squares with Gaussian RBFs for different Ïƒ values: RBFs and Regularization â€¢ Gaussian Radial basis functions (RBFs) predictions: â€“ Flexible bases that can model any continuous function. â€“ But with â€˜nâ€™ data points RBFs have â€˜nâ€™ basis functions. â€¢ How do we avoid overfitting with this huge number of features? â€“ We regularize â€˜wâ€™ and use validation error to choose ğœ and Î». RBFs, Regularization, and Validation â€¢ A model that is hard to beat: â€“ RBF basis with L2-regularization and cross-validation to choose ğœ and Î». â€“ Flexible non-parametric basis, magic of regularization, and tuning for test error. â€“ Can add bias or linear/poly basis to do better away from data. â€“ Expensive at test time: need distance to all training examples. 16 RBFs, Regularization, and Validation â€¢ A model that is hard to beat: â€“ RBF basis with L2-regularization and cross-validation to choose ğœ and Î». â€“ Flexible non-parametric basis, magic of regularization, and tuning for test error! â€“ Expensive at test time: needs distance to all training examples. Hyper-Parameter Optimization â€¢ In this setting we have 2 hyper-parameters (ğœ and Î»). â€¢ More complicated models have even more hyper-parameters. â€“ This makes searching all values expensive (increases over-fitting risk). â€¢ Leads to the problem of hyper-parameter optimization. â€“ Try to efficiently find â€œbestâ€ hyper-parameters. â€¢ Simplest approaches: â€“ Exhaustive search: try all combinations among a fixed set of Ïƒ and Î» values. â€“ Random search: try random values. Hyper-Parameter Optimization â€¢ Other common hyper-parameter optimization methods: â€“ Exhaustive search with pruning: â€¢ If it â€œlooksâ€ like test error is getting worse as you decrease Î», stop decreasing it. â€“ Coordinate search: â€¢ Optimize one hyper-parameter at a time, keeping the others fixed. â€¢ Repeatedly go through the hyper-parameters â€“ Stochastic local search: â€¢ Generic global optimization methods (simulated annealing, genetic algorithms, and so on). â€“ Bayesian optimization: â€¢ Use RBF regression to build model of how hyper-parameters affect validation error. â€¢ Try the best guess based on the model, then repeat. Next Topic: Interpolation vs. Extrapolation Predicting the Future â€¢ In principle, we can use any features xi that we think are relevant. â€¢ This makes it tempting to use time as a feature, and predict future. https://gravityandlevity.wordpress.com/2009/04/22/the-fastest-possible-mile/ Predicting the Future â€¢ In principle, we can use any features xi that we think are relevant. â€¢ This makes it tempting to use time as a feature, and predict future. https://gravityandlevity.wordpress.com/2009/04/22/the-fastest-possible-mi https://overthehillsports.wordpress.com/tag/hicham-el-guerrouj/le/ Predicting 100m times 400 years in the future? https://plus.maths.org/content/sites/plus.maths.org/files/articles/2011/usain/graph2.gif Predicting 100m times 400 years in the future? https://plus.maths.org/content/sites/plus.maths.org/files/articles/2011/usain/graph2.gif http://www.washingtonpost.com/blogs/london-2012-olympics/wp/2012/08/08/report-usain-bolt-invited-to-tryout-for-manchester-united/ Interpolation vs Extrapolation â€¢ Interpolation is task of predicting â€œbetween the data pointsâ€. â€“ Regression models are good at this if you have enough data and function is continuous. â€¢ Extrapolation is task of prediction outside the range of the data points. â€“ Without assumptions, regression models can be embarrassingly-bad at this. â€¢ If you run the 100m regression models backwards in time: â€“ They predict that humans used to be really really slow! â€¢ If you run the 100m regression models forwards in time: â€“ They might eventually predict arbitrarily-small 100m times. â€“ The linear model actually predicts negative times in the future. â€¢ These time traveling races in 2060 should be pretty exciting! â€¢ Some discussion here: â€“ http://callingbullshit.org/case_studies/case_study_gender_gap_running.html https://www.smbc-comics.com/comic/rise-of-the-machines No Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the Future â€¢ We can resolve â€œblue vs. greenâ€ by collecting more data: No Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureNo Free Lunch, Consistency, and the FutureDiscussion: Climate Models â€¢ Has Earth warmed up over last 100 years? (Consistency zone) â€“ Data clearly says â€œyesâ€. â€¢ Will Earth continue to warm over next 100 years? (generalization error) â€“ We should be more skeptical about models that predict future events. https://en.wikipedia.org/wiki/Global_warming Discussion: Climate Models â€¢ So should we all become global warming skeptics? â€¢ If we average over models that overfit in *independent* ways, we expect the test error to be lower, so this gives more confidence: â€“ We should be skeptical of individual models, but agreeing predictions made by models with different data/assumptions are more likely be true. â€¢ All the near-future predictions agree, so they are likely to be accurate. â€“ And itâ€™s probably reasonable to assume fairly continuous change (no big â€œjumpsâ€). â€¢ Variance is higher further into future, so predictions are less reliable. â€“ Relying more on assumptions and less on data. https://en.wikipedia.org/wiki/Global_warming Index Funds: Ensemble Extrapolation for Investing â€¢ Want to do extrapolation when investing money. â€“ What will this be worth in the future? â€¢ Index funds can be viewed as an ensemble method for investing. â€“ For example, buy stock in top 500 companies proportional to value. â€“ Tries to follow average price increase/decrease. â€“ This simple investing strategy outperforms most managed funds. http://fibydesign.com/005-introduction-to-index-investing-stocks-index-funds-vtsax/ Next Topic: L1-Regularization Previously: Search and Score â€¢ We talked about search and score for feature selection: â€“ Define a â€œscoreâ€ and â€œsearchâ€ for features with the best score. â€¢ Usual scores count the number of non-zeroes (â€œL0-normâ€): â€¢ But itâ€™s hard to find the â€˜wâ€™ minimizing this objective. â€¢ We discussed forward selection, but requires fitting O(d2) models. Previously: Search and Score â€¢ What if we want to pick among millions or billions of variables? â€¢ If â€˜dâ€™ is large, forward selection is too slow: â€“ For least squares, need to fit O(d2) models at cost of O(nd2 + d3). â€“ Total cost O(nd4 + d5), and even if you are clever still costs O(nd2 + d4). â€¢ The situation is worse if we are not using basic least squares: â€“ For robust regression, need to run gradient descent O(d2) times. â€“ With regularization, need to search for lambda O(d2) times. L1-Regularization â€¢ Instead of L0- or L2-norm, consider regularizing by the L1-norm: â€¢ Like L2-norm, itâ€™s convex and improves our test error. â€¢ Like L0-norm, it encourages elements of â€˜wâ€™ to be exactly zero. â€¢ L1-regularization simultaneously regularizes and selects features. â€“ Very fast alternative to search and score. â€“ Sometimes called â€œLASSOâ€ regularization. L2-Regularization vs. L1-Regularization â€¢ Regularization path of wj values as â€˜Î»â€™ varies: â€¢ L1-Regularization sets values to exactly 0 (next slides explore why). Regularizers and Sparsity â€¢ L1-regularization gives sparsity but L2-regularization does not. â€“ But donâ€™t they both shrink variables towards zero? â€¢ What is the penalty for setting wj = 0.00001? â€¢ L0-regularization: penalty of Î». â€“ A constant penalty for any non-zero value. â€“ Encourages you to set wj exactly to zero, but otherwise doesnâ€™t care if wj is small or not. â€¢ L2-regularization: penalty of (Î»/2)(0.00001)2 = 0.0000000005Î». â€“ The penalty gets smaller as you get closer to zero. â€“ The penalty asymptotically vanishes as wj approaches 0 (no incentive for â€œexactâ€ zeroes). â€¢ L1-regularization: penalty of Î»|0.00001| = 0.00001Î». â€“ The penalty stays is proportional to how far away wj is from zero. â€“ There is still something to be gained from making a tiny value exactly equal to 0. L2-Regularization vs. L1-Regularization â€¢ L2-Regularization: â€“ Insensitive to changes in data. â€“ Decreased variance: â€¢ Lower test error. â€“ Closed-form solution. â€“ Solution is unique. â€“ All â€˜wjâ€™ tend to be non-zero. â€“ Can learn with linear number of irrelevant features. â€¢ E.g., only O(d) relevant features. â€¢ L1-Regularization: â€“ Insensitive to changes in data. â€“ Decreased variance: â€¢ Lower test error. â€“ Requires iterative solver. â€“ Solution is not unique. â€“ Many â€˜wjâ€™ tend to be zero. â€“ Can learn with exponential number of irrelevant features. â€¢ E.g., only O(log(d)) relevant features. Paper on this result by Andrew Ng L1-Regularization Applications â€¢ Used to give super-resolution in imaging black holes. â€“ Sparsity arises in a particular basis. â€¢ Another application: â€“ Use L1-regularization with Gaussian RBFs to reduce prediction time. https://iopscience.iop.org/article/10.1088/1742-6596/699/1/012006/pdf Summary â€¢ Radial basis functions: â€“ Non-parametric bases that can model any function. â€“ But prediction is slow since with â€˜nâ€™ training examples you have â€˜nâ€™ features. â€¢ Interpolation vs. Extrapolation: â€“ Machine learning with large â€˜nâ€™ is good at predicting â€œbetween the dataâ€. â€“ Without assumptions, can be arbitrarily bad â€œaway from the dataâ€. â€¢ L1-regularization: â€“ Simultaneous regularization and feature selection. â€“ Robust to having lots of irrelevant features. â€¢ Next time: are we really going to use regression for classification? Ockhamâ€™s Razor vs. No Free Lunch â€¢ Ockhamâ€™s razor is a problem-solving principle: â€“ â€œAmong competing hypotheses, the one with the fewest assumptions should be selected.â€ â€“ Suggests we should select linear model. â€¢ Fundamental trade-off: â€“ If same training error, pick model less likely to overfit. â€“ Formal version of Occamâ€™s problem-solving principle. â€“ Also suggests we should select linear model. â€¢ No free lunch theorem: â€“ There exists possible datasets where you should select the green model. Regularizers and Sparsity â€¢ L1-regularization gives sparsity but L2-regularization doesnâ€™t. â€“ But donâ€™t they both shrink variables to zero? â€¢ Consider problem where 3 vectors can get minimum training error: â€¢ Without regularization, we could choose any of these 3. â€“ They all have same error, so regularization will â€œbreak tieâ€. â€¢ With L0-regularization, we would choose w2: Regularizers and Sparsity â€¢ L1-regularization gives sparsity but L2-regularization doesnâ€™t. â€“ But donâ€™t they both shrink variables to zero? â€¢ Consider problem where 3 vectors can get minimum training error: â€¢ With L2-regularization, we would choose w3: â€¢ L2-regularization focuses on decreasing largest (makes wj similar). Regularizers and Sparsity â€¢ L1-regularization gives sparsity but L2-regularization doesnâ€™t. â€“ But donâ€™t they both shrink variables to zero? â€¢ Consider problem where 3 vectors can get minimum training error: â€¢ With L1-regularization, we would choose w2: â€¢ L1-regularization focuses on decreasing all wj until they are 0. Sparsity and Least Squares â€¢ Consider 1D least squares objective: â€¢ This is a convex 1D quadratic function of â€˜wâ€™ (i.e., a parabola): â€¢ This variable does not look relevant (minimum is close to 0). â€“ But for finite â€˜nâ€™ the minimum is unlikely to be exactly zero. Sparsity and L0-Regularization â€¢ Consider 1D L0-regularized least squares objective: â€¢ This is a convex 1D quadratic function but with a discontinuity at 0: â€¢ L0-regularized minimum is often exactly at the â€˜discontinuityâ€™ at 0: â€“ Sets the feature to exactly 0 (does feature selection), but is non-convex. Sparsity and L2-Regularization â€¢ Consider 1D L2-regularized least squares objective: â€¢ This is a convex 1D quadratic function of â€˜wâ€™ (i.e., a parabola): â€¢ L2-regularization moves it closer to zero, but not all the way to zero. â€“ It doesnâ€™t do feature selection (â€œpenalty goes to 0 as slope goes to 0â€). Sparsity and L1-Regularization â€¢ Consider 1D L1-regularized least squares objective: â€¢ This is a convex piecwise-quadratic function of â€˜wâ€™ with â€˜kinkâ€™ at 0: â€¢ L1-regularization tends to set variables to exactly 0 (feature selection). â€“ Penalty on slope is ğœ† even if you are close to zero. â€“ Big ğœ† selects few features, small ğœ† allows many features. Sparsity and Regularization (with d=1)Why doesnâ€™t L2-Regularization set variables to 0? â€¢ Consider an L2-regularized least squares problem with 1 feature: â€¢ Letâ€™s solve for the optimal â€˜wâ€™: â€¢ So as Î» gets bigger, â€˜wâ€™ converges to 0. â€¢ However, for all finite Î» â€˜wâ€™ will be non-zero unless yTx = 0 exactly. â€“ But itâ€™s very unlikely that yTx will be exactly zero. Why doesnâ€™t L2-Regularization set variables to 0? 58 â€¢ Small ğœ† Big ğœ† â€¢ Solution further from zero Solution closer to zero (but not exactly 0) Why does L1-Regularization set things to 0? â€¢ Consider an L1-regularized least squares problem with 1 feature: â€¢ If (w = 0), then â€œleftâ€ limit and â€œrightâ€œ limit are given by: â€¢ So which direction should â€œgradient descentâ€ go in? Why does L1-Regularization set things to 0? 60 â€¢ Small Î» Big Î» â€¢ Solution nonzero Solution exactly zero (minimum of left parabola is past origin, but right parabola is not) (minimum of both parabola are past the origin) L2-regularization vs. L1-regularization â€¢ So with 1 feature: â€“ L2-regularization only sets â€˜wâ€™ to 0 if yTx = 0. â€¢ There is a only a single possible yTx value where the variable gets set to zero. â€¢ And Î» has nothing to do with the sparsity. â€“ L1-regularization sets â€˜wâ€™ to 0 if |yTx| â‰¤ Î». â€¢ There is a range of possible yTx values where the variable gets set to zero. â€¢ And increasing Î» increases the sparsity since the range of yTx grows. â€¢ Note that itâ€™s important that the function is non-differentiable: â€“ Differentiable regularizers penalizing size would need yTx = 0 for sparsity. L1-Loss vs. Huber Loss â€¢ The same reasoning tells us the difference between the L1 *loss* and the Huber loss. They are very similar in that they both grow linearly far away from 0. So both are both robust butâ€¦ â€“ With the L1 loss the model often passes exactly through some points. â€“ With Huber the model doesnâ€™t necessarily pass through any points. â€¢ Why? With L1-regularization we were causing the elements of â€™wâ€™ to be exactly 0. Analogously, with the L1-loss we cause the elements of â€˜râ€™ (the residual) to be exactly zero. But zero residual for an example means you pass through that example exactly. 62 Non-Uniqueness of L1-Regularized Solution â€¢ How can L1-regularized least squares solution not be unique? â€“ Isnâ€™t it convex? â€¢ Convexity implies that minimum value of f(w) is unique (if exists), but there may be multiple â€˜wâ€™ values that achieve the minimum. â€¢ Consider L1-regularized least squares with d=2, where feature 2 is a copy of a feature 1. For a solution (w1,w2) we have: â€¢ So we can get the same squared error with different w1 and w2 values that have the same sum. Further, if neither w1 or w2 changes sign, then |w1| + |w2| will be the same so the new w1 and w2 will be a solution. Splines in 1D â€¢ For 1D interpolation, alternative to polynomials/RBFs are splines: â€“ Use a polynomial in the region between each data point. â€“ Constrain some derivatives of the polynomials to yield a unique solution. â€¢ Most common example is cubic spline: â€“ Use a degree-3 polynomial between each pair of points. â€“ Enforce that fâ€™(x) and fâ€™â€™(x) of polynomials agree at all point. â€“ â€œNaturalâ€ spline also enforces fâ€™â€™(x) = 0 for smallest and largest x. â€¢ Non-trivial fact: natural cubic splines are sum of: â€“ Y-intercept. â€“ Linear basis. â€“ RBFs with g(Îµ) = Îµ3. â€¢ Different than Gaussian RBF because it increases with distance. http://www.physics.arizona.edu/~restrepo/475A/Notes/sourcea-/node35.html Splines in Higher Dimensions â€¢ Splines generalize to higher dimensions if data lies on a grid. â€“ Many methods exist for grid-structured data (linear, cubic, splines, etc.). â€“ For more general (â€œscatteredâ€) data, there isnâ€™t a natural generalization. â€¢ Common 2D â€œscatteredâ€ data interpolation is thin-plate splines: â€“ Based on curve made when bending sheets of metal. â€“ Corresponds to RBFs with g(Îµ) = Îµ2 log(Îµ). â€¢ Natural splines and thin-plate splines: special cases of â€œpolyharmonicâ€ splines: â€“ Less sensitive to parameters than Gaussian RBF. http://step.polymtl.ca/~rv101/thinplates/ L2-Regularization vs. L1-Regularization â€¢ L2-regularization conceptually restricts â€˜wâ€™ to a ball. L2-Regularization vs. L1-Regularization â€¢ L2-regularization conceptually restricts â€˜wâ€™ to a ball. â€¢ L1-regularization restricts to the L1 â€œballâ€: â€“ Solutions tend to be at corners where wj are zero. Related Infinite Series video","libVersion":"0.2.1","langs":""}