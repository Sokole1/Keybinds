{"path":".obsidian/plugins/text-extractor/cache/6eb38b5ecd564d37f6c572ffd451105e.json","text":"68 CoMM unICATIonS o F ThE ACM | july 2011 | vol. 54 | no. 7 contributed articles L ArG e-s CAL e s OFtWAre DeVeLOpment is a notoriously difficult problem. Software is built in layers, and APIs are exposed by each layer to its clients. APIs come with usage rules, and clients must satisfy them while using the APIs. Violations of API rules can cause runtime errors. Thus, it is useful to consider whether API rules can be formally documented so programs using the APIs can be checked at compile time for compliance against the rules. Some API rules (such as agreement on the number of parameters and data types of each parameter) can be checked by compilers. However, certain rules involve hidden state; for example, consider the rule that the acquire method and release method of a spinlock must be done in strict alter- nation and the rule that a file can be read only after it is opened. We built the SLAM engine (SLAM from now on) to allow programmers to specify state- ful usage rules and statically check if clients follow such rules. We wanted SLAM to be scalable and at the same time have a very low false-error rate. To scale the SLAM engine, we constructed abstractions that retain only informa- tion about certain predicates related to the property being checked. To reduce false errors, we refined abstractions automatically using counterexamples from the model checker. Constructing and refining abstractions for scaling model checking has been known for more than 15 years; Kurshan 35 is the earliest reference we know. SLAM automated the process of abstraction and refinement with counterexamples for programs writ- ten in common programming lan- guages (such as C) by introducing new techniques to handle program- ming-language constructs (such as pointers, procedure calls, and scop- ing constructs for variables).2,4–8 In- dependently and simultaneously with our work, Clarke et al.17 auto- mated abstraction and refinement with counterexamples in the con- text of hardware, coining the term “counterexample-driven abstraction refinement,” or CEGAR, which we use to refer to this technique throughout A Decade of Software Model Checking with SLAM DoI:10.1145/1965724.1965743 SLAM is a program-analysis engine used to check if clients of an API follow the API’s stateful usage rules. BY ThoMAS BALL, VLADIMIR LEVIn, AnD SRIRAM K. RAJAMAnI key insights Even though programs have many states, it is possible to construct an abstraction of a program fine enough to represent parts of a program relevant to an API usage rule and coarse enough for a model checker to explore all the states. SLAM synthesizes and extends diverse ideas from model checking, theorem proving, and data-flow analysis to automate construction, checking, and refinement of abstractions. SLAM showed that such abstractions can be constructed automatically for real-world programs, becoming the basis of Microsoft’s Static Driver Verifier tool. july 2011 | vol. 54 | no. 7 | CoMMunICATIonS oF ThE ACM 69IllustratIon By ryan aleXanDer this article. The automation of CE- GAR for software is technically more intricate, since software, unlike hard- ware, is infinite state, and program- ming languages have more expres- sive and complex features compared to hardware-description languages. Programming languages allow pro- cedures with unbounded call stacks (handled by SLAM using pushdown model-checking techniques), scoping of variables (exploited by SLAM for ef- ficiency), and pointers allowing the same memory to be aliased by differ- ent variables (handled by SLAM using pointer-alias-analysis techniques). We also identified a “killer-app” for SLAM—checking if Windows de- vice drivers satisfy driver API usage rules. We wrapped SLAM with a set of rules specific to the Windows driver API and a tool chain to enable push- button validation of Windows drivers, resulting in a tool called “static driver verifier,” or SDV. Such tools are stra- tegically important for the Windows device ecosystem, which encourages and relies on hardware vendors mak- ing devices and writing Windows de- vice drivers while requiring vendors to provide evidence that the devices and drivers perform acceptably. Be- cause many drivers use the same Win- dows-driver API, the cost of manually specifying the API rules and writing them down is amortized over the value obtained by checking the same rules over many device drivers. Here, we offer a 10-year retrospec- tive of SLAM and SDV, including a self- contained overview of SLAM, our ex- perience taking SLAM to a full-fledged SDV product, a description of how we built and deployed SDV, and results obtained from the use of SDV. SLAM Initially, we coined the label SLAM as an acronym for “software (speci- fications), programming languages, abstraction, and model checking.” Over time, we used SLAM more as a forceful verb; to “SLAM” a program is to exhaustively explore its paths and eliminate its errors. We also de- signed the “Specification Language for Interface Checking,” or SLIC,9 to specify stateful API rules and created the SLAM tool as a flexible verifier to check if code that uses the API follows the SLIC rules. We wanted to build a verifier covering all possible behav- iors of the program while checking the rule, as opposed to a testing tool that checks the rule on a subset of be- haviors covered by the test. In order for the solution to scale while covering all possible behaviors, we introduced Boolean programs. Boolean programs are like C programs in the sense that they have all the con- trol constructs of C programs—se- quencing, conditionals, loops, and pro- cedure calls—but allow only Boolean variables (with local, as well as global, 70 CoMM unICATIonS o F ThE ACM | july 2011 | vol. 54 | no. 7 contributed articles A SLIC rule includes three compo- nents: a static set of state variables, described as a C structure; a set of events and event handlers that specify state transitions on the events; and a set of annotations that bind the rule to various object instances in the pro- gram (not shown in this example). As an example of a rule, consider the locking rule in Figure 1a. Line 1 de- clares a C structure containing one field state, an enumeration that can be either Unlocked or Locked, to capture the state of the lock. Lines 3–5 describe an event handler for calls to KeInitializeSpinLock. Lines 7–13 describe an event han- dler for calls to the function KeAc- quireSpinLock. The code for the handler expects the state to be in Unlocked and moves it to Locked (specified in line 9). If the state is already Locked, then the program has called KeAcquireSpinLock twice without an intervening call to KeReleaseSpinLock and is an er- ror (line 9). Lines 15–21 similarly de- scribe an event handler for calls to the function KeReleaseSpinLocka. Figure 1b is a piece of code that uses the functions KeAcquireSpinLock and KeReleaseSpinLock. Figure 1c a A more detailed example of this rule would han- dle different instances of locks, but we cover the simple version here for ease of exposition. scope). Boolean programs made sense as an abstraction for device drivers because we found that most of the API rules drivers must follow tend to be control-dominated, and so can be checked by modeling control flow in the program accurately and modeling only a few predicates about data rel- evant to each rule being checked. The predicates that need to be “pulled into” the model are dependent on how the client code manages state relevant to the rule. CEGAR is used to discover the relevant state automatical- ly so as to balance the dual objectives of scaling to large programs and reducing false errors. SLIC specification language. We de- signed SLAM to check temporal safety properties of programs using a well- defined interface or API. Safety proper- ties are properties whose violation is witnessed by a finite execution path. A simple example of a safety property is that a lock should be alternatively acquired and released. SLIC allows us to encode temporal safety proper- ties in a C-like language that defines a safety automaton44 that monitors a program’s execution behavior at the level of function calls and returns. The automaton can read (but not modify) the state of the C program that is vis- ible at the function call/return inter- face, maintain a history, and signal the occurrence of a bad state. is the same code after it has been in- strumented with calls to the appropri- ate event handlers. We return to this example later. CEGAR via predicate abstraction. Figure 2 presents ML-style pseudo- code of the CEGAR process. The goal of SLAM is to check if all executions of the given C program P (type cprog) satisfy a SLIC rule S (type spec). The instrument function takes the program P and SLIC rule S as inputs and produces an instrumented pro- gram P´ as output, based on the prod- uct-construction technique for safety properties described in Vardi and Wol- per.44 It hooks up relevant events via calls to event handlers specified in the rule S, maps the error statements in the SLIC rule to a unique error state in P´, and guarantees that P satisfies S if and only if the instrumented program P´ never reaches the error state. Thus, this function reduces the problem of checking if P satisfies S to checking if P´ can reach the error state. The function slam takes a C pro- gram P and SLIC rule specification S as input and passes the instrumented C program to the tail-recursive func- tion cegar, along with the predicates extracted from the specification S (specifically, the guards that appear in S as predicates). The first step of the cegar function is to abstract program P´ with respect to Figure 1. (a) Simplified SLIC locking rule; (b) code fragment using spinlocks; (c) fragment after instrumentation. 1 state { enum {Unlocked, Locked} state; } 2 3 KeInitializeSpinLock.call { 4 state = Unlocked; 5 } 6 7 KeAcquireSpinLock.call { 8 if ( state == Locked ) { 9 error; 10 } else { 11 state = Locked; 12 } 13 } 14 15 KeReleaseSpinLock.call { 16 if ( !(state == Locked) ) { 17 error; 18 } else { 19 state = Unlocked; 20 } 21 } 22 1 .. 2 KeInitializeSpinLock(); 3 .. 4 .. 5 if(x > 0) 6 KeAcquireSpinlock(); 7 count = count+1; 8 devicebuffer[count] = localbuffer[count]; 9 if(x > 0) 10 KeReleaseSpinLock(); 11 ... 12 ... 1 .. 2 { state = Unlocked; 3 KeInitializeSpinLock();} 4 .. 5 .. 6 if(x > 0) 7 { SLIC_KeAcquireSpinLock_call(); 8 KeAcquireSpinlock(); } 9 count = count+1; 10 devicebuffer[count] = localbuffer[count]; 11 if(x > 0) 12 { SLIC_KeReleaseSpinLock_call(); 13 KeReleaseSpinLock(); } 14 ... 15 ... (a) (b) (c) contributed articles july 2011 | vol. 54 | no. 7 | CoMMunICATIonS oF ThE ACM 71 the predicate set preds to create a Bool- ean program abstraction B. The auto- mated transformation of a C program into a Boolean program uses a tech- nique called predicate abstraction, first introduced in Graf and Saïdi29 and later extended to work with program- ming-language features in Ball et al.2 and Ball et al. 3 The program B has exactly the same control-flow skeleton as program P´. By construction, for any set of predi- cates preds, every execution trace of the C program P´ also is an execution trace of B = abstract(P´, preds); that is, the execution traces of P´ are a subset of those of B. The Boolean program B models only the portions of the state of P´ relevant to the current SLIC rule, us- ing nondeterminism to abstract away irrelevant state in P´. Once the Boolean program B is con- structed, the check function exhaus- tively explores the state space of B to determine if the (unique) error state is reachable. Even though all variables in B are Boolean, it can have procedure calls and a potentially unbounded call stack. Our model checker performs symbolic reachability analysis of the Boolean program (a pushdown system) using binary decision diagrams.11 It uses ideas from interprocedural data flow analysis 42,43 and builds summaries for each procedure to handle recursion and variable scoping. If the check function returns Ab- stractPass, then the error state is not reachable in B and therefore is also not reachable in P´. In this case, SLAM has proved that the C program P satis- fies the specification S. However, if the check function returns AbstractFail with witness trace trc, the error state is reachable in the Boolean program B but not necessarily in the C program P´. Therefore, the trace trc must be validated in the context of P´ to prove it really is an execution trace of P´. The function symexec symbolically executes the trace trc in the context of the C program P´. Specifically, it con- structs a formula φ(P´, trc) that is satis- fiable if and only if there exists an input that would cause program P´ to execute trace trc. If symexec returns Satisfiable, then SLAM has proved program P does not satisfy specification S and returns the counterexample trace trc. If the function symexec returns Unsatisfiable(prf), then it has found a proof prf that there is no input that would cause P´ to execute trace trc. The function refine takes this proof of unsatisfiability, reduces it to a smaller proof of unsatisfiability, and returns the set of constituent predicates from this smaller proof. The function refine guarantees that the trace trc is not an execution trace of the Boolean program abstract (P´, preds ∪ refine(pr f)) The ability to refine the (Boolean pro- gram) abstraction to rule out a spurious counterexample is known as the prog- ress property of the CEGAR process. Despite the progress property, the CEGAR process offers no guarantee of terminating since the program P´ may have an intractably large or in- finite number of states; it can refine the Boolean program forever without discovering a proof of correctness or proof of error. However, as each Boolean program is guaranteed to overapproximate the behavior of the C program, stopping the CEGAR process before it terminates with a definitive result is no different from any terminating program analysis that produces false alarms. In practice, SLAM terminates with a definite result over 96% of the time on large classes of device drivers: for Windows Driver Framework (WDF) drivers, the figure is Figure 2. Graphical illustration and ML-style pseudocode of CEGAR loop. cprog P bprog bpredicates proof of unsat. trace cprog P′ P passes s spec s P fails s validated trace CEGAR instrument abstract symexec refine check type cprog, spec, predicates, bprog, trace, proof type result = Pass | Fail of trace type chkresult = AbstractPass | AbstractFail of trace type excresult = Satisable | Unsatisable of proof let rec cegar (P’:cprog) (preds :predicates) : result = let B: bprog = abstract (P’,preds) in match check(B) with | AbstractPass -> Pass | AbstractFail(trc) -> match symexec(P’, trc) with | Satisable -> Fail(trc) | Unsatisable(prf) -> cegar P’ ( preds ∪ (refine prf)) let slam ( P:cprog) (S:spec) : result = cegar (instrument (P,S)) (preds S) 72 CoMMunICATI onS o F ThE ACM | july 2011 | vol. 54 | no. 7 contributed articles Figure 3b is the Boolean program abstraction of the SLIC-instrumented C program from Figure 1c. Note the Boolean program has the same control flow as the C program, including proce- dure calls. However, the conditionals at lines 7 and 12 of the Boolean pro- gram are nondeterministic since the Boolean program does not have a pred- icate that refers to the value of variable x. Also note that the references to vari- ables count, devicebuffer, and lo- calbuffer are elided in lines 10 and 11 (replaced by skip statements in the Boolean program) since the Boolean program does not have predicates that refer to these variables. The abstraction in Figure 3b, though a valid abstraction of the instrumented C, is not strong enough to prove the program conforms to the SLIC rule. In particular, the reachability analysis of the Boolean program performed by the check function will find that slic _ error is reachable via the trace 1, 2, 3, 4, 5, 6, 7, 10, 11, 12, 13, which skips the call to SLIC _ Ke- AcquireSpinLock _ call at line 8 and performs the call to SLIC _ KeReleas- eSpinLock _ call at line 13. Since the Boolean variable state==Lock is false, slic _ error will be called in line 11 of Figure 3a. SLAM feeds this error trace to the symexec function that executes it symbolically over the instrumented C program in Figure 1c and determines the trace is not executable since the branches in “if” conditions are cor- 100%, and for Windows Driver Model (WDM) drivers, the figure is 97%. Example. We illustrate the CEGAR process using the SLIC rule from Fig- ure 1a and the example code fragment in Figure 1b. In the program, we have a single spinlock being initialized at line 4. The spinlock is acquired at line 8 and released at line 12. However, both calls KeAcquireSpinLock and KeR- eleaseSpinLock are guarded by the conditional (x > 0). Thus, tracking cor- relations between such conditionals is important for proving this property. Figures 3a and 3b show the Boolean program obtained by the first applica- tion of the abstract function to the code from Figures 1a and 1c, respectively. Figure 3a is the Boolean program abstraction of the SLIC event handler code. Recall that the instrumentation step guarantees there is a unique error state. The function slic _ error at line 1 represents that state; that is, the function slic _ error is unreach- able if and only if the program satis- fies the SLIC rule. There is one Boolean variable named {state==Locked}; by convention, we name each Boolean variable with the predicate it stands for, enclosed in curly braces. In this case, the predicate comes from the guard in the SLIC rule (Figure 1a, line 8). Lines 5–8 and lines 10–13 of Figure 3a show the Boolean procedures cor- responding to the SLIC event handlers SLIC _ KeAcquireSpinLock _ call and SLIC _ KeReleaseSpinLock_ call from Figure 1a. related. In particular, the trace is not executable because there does not ex- ist a value for variable x such that (x > 0) is false (skipping the body of the first conditional) and such that (x > 0) is true (entering the body of the sec- ond conditional). That is, the formula ∃x.(x ≤ 0) ^ (x > 0) is unsatisfiable. The result of the refine function is to add the predicate {x>0} to the Boolean program to refine it. This addition results in the Boolean program ab- straction in Figure 3c, including the Boolean variable {x>0}, in addition to {state==Locked}. Using these two Boolean variables, the abstraction in Figure 3c is strong enough to prove slic _ error is un- reachable for all possible executions of the Boolean program, and hence SLAM proves this Boolean program satisfies the SLIC rule. Since the Boolean pro- gram is constructed to be an overap- proximation of the C program in Fig- ure 1c, the C program indeed satisfies the SLIC rule. From SLAM to SDV SDV is a completely automatic tool (based on SLAM) device-driver devel- opers can use at compile time. Requir- ing nothing more than the build script of the driver, the SDV tool runs fully automatically and checks a set of pre- packaged API usage rules on the device driver. For every usage rule violated by the driver, SDV presents a possible ex- ecution trace through the driver that shows how the rule can be violated. Figure 3. (a) Boolean program abstraction for locking and unlocking routines; (b) Boolean program: CEGAR iteration 1; (c) Boolean program: CEGAR iteration 2. 1 slic_error() { assert(false); } 2 3 bool {state==Locked}; 4 5 SLIC_KeAcquireSpinLock_call() { 6 if( {state==Locked}) slic_error(); 7 else {state==Locked} := true; 8 } 9 10 SLIC_KeReleaseSpinLock_call() { 11 if( !{state==Locked}) slic_error(); 12 else {state==Locked} := false; 13 } 14 1 ... 2 ... 3 {state==Locked} := false; 4 KeInitializeSpinLock(); 5 ... 6 ... 7 if(*) 8 { SLIC_KeAcquireSpinLock_call(); 9 KeAcquireSpinLock(); } 10 skip; 11 skip; 12 if(*) 13 { SLIC_KeReleaseSpinLock_Call(); 14 KeReleaseSpinLock(); } 15 ... 16 ... 1 bool {x > 0}; 2 ... 3 {state==Locked} := false; 4 KeInitializeSpinLock(); 5 ... 6 ... 7 if({x>0}) 8 { SLIC_KeAcquireSpinLock_call(); 9 KeAcquireSpinLock(); } 10 skip; 11 skip; 12 if({x>0}) 13 { SLIC_KeReleaseSpinLock_Call(); 14 KeReleaseSpinLock(); } 15 .. 16 ... (a) (b) (c) contributed articles july 2011 | vol. 54 | no. 7 | CoMMunICATIonS oF ThE ACM 73 Model checking is often called “push-button” technology,16 giving the impression that the user simply gives the system to the model checker and receives useful output about er- rors in the system, with state-space explosion being the only obstacle. In practice, in addition to state-space explosion, several other obstacles can inhibit model checking being a “push- button” technology: First, users must specify the properties they want to check, without which there is nothing for a model checker to do. In complex systems (such as the Windows driver interface), specifying such properties is difficult, and these properties must be debugged. Second, due to the state- explosion problem, the code analyzed by the model checker is not the full sys- tem in all its gory complexity but rath- er the composition of some detailed component (like a device driver) with a so-called “environment model” that is a highly abstract, human-written description of the other components of the system—in our case, kernel procedures of the Windows operating system. Third, to be a practical tool in the toolbox of a driver developer, the model checker must be encapsulated in a script incorporating it in the driver development environment, then feed it with the driver’s source code and re- port results to the user. Thus, creating a push-button experience for users re- quires much more than just building a good model-checking engine. Here, we explore the various com- ponents of the SDV tool besides SLAM: driver API rules, environment models, scripts, and user interface, describ- ing how they’ve evolved over the years, starting with the formation of the SDV team in Windows in 2002 and several internal and external releases of SDV. API rules. Different classes of devic- es have different requirements, lead- ing to class-specific driver APIs. Thus, networking drivers use the NDIS API, storage drivers use the StorPort and MPIO APIs, and display drivers the WDDM API. A new API called WDF was designed to provide higher-level ab- stractions for common device drivers. As described earlier, SLIC rules capture API-level interactions, though they are not specific to a particular device driver but to a whole class of drivers that use a common API. Such a specification means the manual effort of writing rules can be amortized by checking the rules on thousands of device driv- ers using the API. The SDV team has made significant investment in writing API rules and teaching others in Micro- soft’s Windows organization to write API rules. Environment models. SLAM is de- signed as a generic engine for check- ing properties of a closed C program. However, a device driver is not a closed program with a main procedure but rather a library with many entry points (registered with and called by the op- erating system). This problem is stan- dard to both program analysis and model checking. Before applying SLAM to a driver’s code, we first “close” the driver pro- gram with a suitable environment con- sisting of a top layer called the harness, a main procedure that calls the driver’s entry points, and a bottom layer of stubs for the Windows API functions that can be called by the device driver. Thus, the harness calls into the driver, and the driver calls the stubs. Most API rules are local to a driver’s entry points, meaning a rule can be checked independently on each entry point. However, some complex rules deal with sequences of entry points. For the rules of the first type, the body of the harness is a nondeterministic switch in which each branch calls a single and different entry point of the driver. For more complex rules, the harness contains a sequence of such nondeterministic switches. A stub is a simplified implementa- tion of an API function intended to ap- proximate the input-output relation of the API function. Ideally, this relation should be an overapproximation of the API function. In many cases, a driver API function returns a scalar indicating success or failure. In these cases, the API stub usually ends with a nondeter- ministic switch over possible return val- ues. In many cases, a driver API function allocates a memory object and returns its address, sometimes through an out- put pointer parameter. In these cases, the harness allocates a small set of such memory objects, and the stub picks up one of them and returns its address. Scaling rules and models. Initially, we (the SDV team) wrote the API rules in SLIC based on input from driver API We wanted to build a verifier covering all possible behaviors of the program while checking the rule, as opposed to a testing tool that checks the rule on a subset of behaviors covered by the test. 74 CoMMunICATI onS o F ThE ACM | july 2011 | vol. 54 | no. 7 contributed articles experts. We tested them on drivers with injected bugs, then ran SDV with the rules on real Windows drivers. We dis- cussed the bugs found by the rules with driver owners and API experts to refine the rules. At that time, a senior manag- er said, “It takes a Ph.D. to develop API rules.” Since then, we’ve invested sig- nificant effort in creating a discipline for writing SLIC rules and spreading it among device-driver API developers and testers. In 2007, the SDV team refined the API rules and formulated a set of guidelines for rule development and driver environment model construc- tion. This helped us transfer rule de- velopment to two software engineers with backgrounds far removed from formal verification, enabling them to succeed and later spread this form of rule development to others. Since 2007, driver API teams have been us- ing summer interns to develop new API rules for WDF, NDIS, StorPort, and MPIO APIs and for an API used to write file system mini-filters (such as antivi- ruses) and Windows services. Remark- ably, all interns have written API rules that found true bugs in real drivers. SDV today includes more than 470 API rules. The latest version SDV 2.0 (released with Windows 7 in 2009) in- cludes more than 210 API rules for the WDM, WDF, and NDIS APIs, of which only 60 were written by formal verifica- tion experts. The remaining 150 were written or modified from earlier drafts by software engineers or interns with no experience in formal verification. Worth noting is that the SLIC rules for WDF were developed during the de- sign phase of WDF, whereas the WDM rules were developed long after WDM came into existence. The formaliza- tion of the WDF rules influenced WDF design; if a rule could not be expressed naturally in SLIC, the WDF designers tried to refactor the API to make it eas- ier to verify. This experience showed that verification tools (such as SLAM) can be forward-looking design aids, in addition to being checkers for legacy APIs (such as WDM). Scripts. SDV includes a set of scripts that perform various functions: com- bining rules and environment models; detecting source files of a driver and its build parameters; running the SLIC compiler on rules and the C compiler on a driver’s and environment model’s source code to generate an intermedi- ate representation (IR); invoking SLAM on the generated IR; and reporting the summary of the results and error traces for bugs found by SLAM in a GUI. The SDV team worked hard to en- sure these scripts would provide a very high degree of automation for the user. The user need not specify anything oth- er than the build scripts used to build the driver. SDV Experience The first version of SDV (1.3, not re- leased externally outside Microsoft) found, on average, one real bug per driver in 30 sample drivers shipped with the Driver Development Kit (DDK) for Windows Server 2003. These sample drivers were already well test- ed. Eliminating defects in the WDK samples is important since code from sample drivers is often copied by third- party driver developers. Versions 1.4 and 1.5 of SDV were ap- plied to Windows Vista drivers. In the sample WDM drivers shipped with the Vista WDK (WDK, the renamed DDK), SDV found, on average, approximately one real bug per two drivers. These samples were mostly modifications of sample drivers from the Windows Server 2003 DDK, with fixes applied for the defects found by SDV 1.3. The new- ly found defects were due to improve- ments in the set of SDV rules and to de- fects introduced due to modifications in the drivers. For Windows Server 2008, SDV ver- sion 1.6 contained new rules for WDF drivers, with which SDV found one real bug per three WDF sample drivers. The low bug count is explained by simplic- ity of the WDF driver model described earlier and co-development of sample drivers, together with the WDF rules. For the Windows 7 WDK, SDV 2.0 found, on average, one new real bug per WDF sample driver and few bugs on all the WDM sample drivers. This data is explained by more focused ef- forts to refine WDF rules and few mod- ifications in the WDM sample drivers. SDV 2.0 shipped with 74 WDM rules, 94 WDF rules, and 36 NDIS rules. On WDM drivers, 90% of the defects re- ported by SDV are true bugs, and the rest are false errors. Further, SDV re- ports nonresults (such as timeouts A unique SLAM contribution is the complete automation of CEGAR for software written in expressive programming languages (such as C). contributed articles july 2011 | vol. 54 | no. 7 | CoMMunICATIonS oF ThE ACM 75 and spaceouts) on only 3.5% of all checks. On WDF drivers, 98% of de- fects reported by SDV are true bugs, and non-results are reported on only 0.04% of all checks. During the devel- opment cycle of Windows 7, SDV 2.0 was applied as a quality gate to drivers written by Microsoft and sample driv- ers shipped with the WDK. SDV was applied later in the cycle after all other tools, yet found 270 real bugs in 140 WDM and WDF drivers. All bugs found by SDV in Microsoft drivers were fixed by Microsoft. We do not have reliable data on bugs found by SDV in third- party device drivers. Here, we give performance statis- tics from a recent run of SDV on 100 drivers and 80 SLIC rules. The largest driver in the set is about 30,000 lines of code, and the total size of all drivers is 450,000 lines of code. The total run- time for the 8,000 runs (each driver- rule combination is a run) is about 30 hours on an eight-core machine. We kill a run if it exceeds 20 minutes, and SDV yields useful results (either a bug or a pass) on over 97% of the runs. We thus find SDV checks drivers with ac- ceptable performance, yielding useful results on a large fraction of the runs. Limitations. SLAM and SDV also involve several notable limitations. Even with CEGAR, SLAM is unable to handle very large programs (with hun- dreds of thousands of lines of code). However, we also found SDV is able to give useful results for control-domi- nated properties and programs with tens of thousands of lines of code. Though SLAM handles pointers in a sound manner, in practice, it is un- able to prove properties that depend on establishing invariants of heap data structures. SLAM handles only sequential programs, though oth- ers have extended SLAM to deal with bounded context switches in concur- rent programs.40 Our experience with SDV shows that in spite of these limi- tations, SLAM is very successful in the domain of device-driver verification. Related Work SLAM builds on decades of research in formal methods. Model checking15,16,41 has been used extensively to algorith- mically check temporal logic proper- ties of models. Early applications of model checking were in hardware38 and protocol design.32 In compiler and programming languages, abstract in- terpretation21 provides a broad and ge- neric framework to compute fixpoints using abstract lattices. The particular abstraction used by SLAM was called “predicate abstraction” by Graf and Saïdi.29 Our contribution was to show how to perform predicate abstraction on C programs with such language features as pointers and procedure calls in a modular manner.2,3 The predicate-abstraction algorithm uses an automated theorem prover. Our ini- tial implementation of SLAM used the Simplify theorem prover.23 Our current implementation uses the Z3 theorem prover.22 The Bandera project explored the idea of user-guided finite-state abstrac- tions for Java programs 20 based on predicate abstraction and manual ab- straction but without automatic refine- ment of abstractions. It also explored the use of program slicing for reducing the state space of models. SLAM was influenced by techniques used in Ban- dera to check typestate properties on all objects of a given type. SLAM’s Boolean program model checker (Bebop) computes fixpoints on the state space of the generated Boolean program that can include re- cursive procedures. Bebop uses the Context Free Language Reachability al- gorithm, 42,43 implementing it symboli- cally using Binary Decision Diagrams.11 Bebop was the first symbolic model checker for pushdown systems. Since then, other symbolic checkers have been built for similar purposes, 25,36 and Boolean programs generated by SLAM have been used to study and improve their performance. SLAM and its practical application to checking device drivers has been enthusiastically received by the re- search community, and several related projects have been started by research groups in universities and industry. At Microsoft, the ESP and Vault proj- ects were started in the same group as SLAM, exploring different ways of checking API usage rules. 37 The Blast project31 at the University of Califor- nia, Berkeley, proposed a technique called “lazy abstraction” to optimize constructing and maintaining the ab- stractions across the iterations in the CEGAR loop. McMillan39 proposed “in- terpolants” as a more systematic and general way to perform refinement; Henzinger et al.30 found predicates generated from interpolants have nice local properties that were then used to implement local abstractions in Blast. Other contemporary techniques for analyzing C code against temporal rules include the meta-level compila- tion approach of Engler et al. 24 and an extension of SPIN developed by Holz- mann 33 to handle ANSI C. 33 The Cqual project uses “type qualifiers” to specify API usage rules, using type inference to check C code against the type-qualifier annotations. 26 SLAM works by computing an overapproximation of the C program, or a “may analysis,” as described by Godefroid et al.28 The may analysis is refined using symbolic execution on traces, as inspired by the PREfix tool,12 or a “must analysis.” In the past few years, must analysis using efficient symbolic execution on a subset of paths in the program has been shown to be very effective in finding bugs.27 The Yogi project has explored ways to combine may and must analysis in more general ways.28 Another way to perform underapproximation or must analysis is to unroll loops a fixed num- ber of times and perform “bounded model checking”14 using satisfiabil- ity solvers, an idea pursued by several projects, including CBMC,18 F-Soft,34 and Saturn.1 CEGAR has been generalized to check properties of heap-manipulat- ing programs,10 as well as the problem of program termination.19 The Magic model checker checks properties of concurrent programs where threads interact through message passing.13 And Qadeer and Wu40 used SLAM to analyze concurrent programs through an encoding that models all interleav- ings with two context switches as a se- quential program. Conclusion The past decade has seen a resurgence of interest in the automated analysis of software for the dual purpose of defect detection and program verification, as well as advances in program analysis, model checking, and automated theo- rem proving. A unique SLAM contri- bution is the complete automation of CEGAR for software written in expres- 76 CoMM unICATIonS o F ThE ACM | july 2011 | vol. 54 | no. 7 contributed articles 8. Ball, t. and rajamani, s.k. the slaM project: Debugging system software via static analysis. In Proceedings of the 29th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Portland, or, Jan. 16–18). acM Press, new york, Jan. 2002, 1–3. 9. Ball, t. and rajamani, s.k. SLIC: A Specification Language for Interface Checking. Technical Report MSR-TR-2001-21. Microsoft research, redmond, Wa, 2001. 10. Beyer, D., henzinger, t.a., théoduloz, g., and Zufferey, D. shape refinement through explicit heap analysis. In Proceedings of the 13th International Conference on Fundamental Approaches to Software Engineering (Paphos, cyprus, Mar. 20–28). springer, 2010, 263–277. 11. Bryant, r. graph-based algorithms for Boolean function manipulation. IEEE Transactions on Computers C-35, 8 (aug. 1986), 677–691. 12. Bush, W.r., Pincus, J.D., and siela, D.J. a static analyzer for finding dynamic programming errors. Software-Practice and Experience 30, 7 (June 2000), 775–802. 13. chaki, s., clarke, e., groce, a., Jha, s., and Veith, h. Modular verification of software components in c. In Proceedings of the 25th International Conference on Software Engineering (Portland, or, May 3–10). Ieee computer society, 2003, 385–395. 14. clarke, e., grumberg, o., and Peled, D. Model Checking. MIt Press, cambridge, Ma, 1999. 15. clarke, e.M. and emerson, e.a. synthesis of synchronization skeletons for branching time temporal logic. In Proceedings of the Workshop on Logic of Programs (yorktown heights, ny, May 1981). springer, 1982, 52–71. 16. clarke, e.M., emerson, e.a., and sifakis, J. Model checking: algorithmic verification and debugging. Commun. ACM 52, 11 (nov. 2009), 74–84. 17. clarke, e.M., grumberg, o., Jha, s., lu, y., and Veith, h. counterexample-guided abstraction refinement. In Proceedings of the 12 International Conference on Computer-Aided Verification (chicago, July 15–19). springer, 2000, 154–169. 18. clarke, e.M., kroening, D., and lerda, f. a tool for checking ansI-c programs. In Proceedings of the 10th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (Barcelona, Mar. 29–apr. 2). springer, 2004, 168–176. 19. cook, B., Podelski, a., and rybalchenko, a. abstraction refinement for termination. In Proceedings of the 12 th International Static Analysis Symposium (london, sept. 7–9). springer, 2005, 87–101. 20. corbett, J., Dwyer, M., hatcliff, J., Pasareanu, c., robby, laubach, s., and Zheng, h. Bandera: extracting finite-state models from Java source code. In Proceedings of the 22 nd International Conference on Software Engineering (limerick, Ireland, June 4–11). acM Press, new york, 2000, 439–448. 21. cousot, P. and cousot, r. abstract interpretation: a unified lattice model for the static analysis of programs by construction or approximation of fixpoints. In Proceedings of the Fourth ACM Symposium on Principles of Programming Languages (los angeles, Jan.). acM Press, new york, 1977, 238–252. 22. de Moura, l. and Bjørner, n. Z3: an efficient sMt solver. In Proceedings of the 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems (Budapest, Mar. 29–apr. 6). springer, 2008, 337–340. 23. Detlefs, D., nelson, g., and saxe, J.B. simplify: a theorem prover for program checking. Journal of the ACM 52, 3 (May 2005), 365–473. 24. engler, D., chelf, B., chou, a., and hallem, s. checking system rules using system-specific, programmer- written compiler extensions. In Proceedings of the Fourth Symposium on Operating System Design and Implementation (san Diego, oct. 23–25). usenix association, 2000, 1–16. 25. esparza, J. and schwoon, s. a BDD-based model checker for recursive programs. In Proceedings of the 13th International Conference on Computer Aided Verification (Paris, July 18–22). springer, 2001, 324–336. 26. foster, J.s., terauchi, t., and aiken, a. flow-sensitive type qualifiers. In Proceedings of the 2002 ACM SIGPLAN Conference on Programming Language Design and Implementation (Berlin, June 17–19). acM Press, new york, 2002, 1–12. 27. godefroid, P., levin, M.y., and Molnar, D.a. automated whitebox fuzz testing. In Proceedings of the Network and Distributed System Security Symposium (san sive programming languages (such as C). We achieved this automation by combining and extending such diverse ideas as predicate abstraction, inter- procedural data-flow analysis, symbol- ic model checking, and alias analysis. Windows device drivers provided the crucible in which SLAM was tested and refined, resulting in the SDV tool, which ships as part of the Windows Driver Kit. Acknowledgments For their many contributions to SLAM and SDV, directly and indirectly, we thank Nikolaj Bjørner, Ella Bounimova, Sagar Chaki, Byron Cook, Manuvir Das, Satyaki Das, Giorgio Delzanno, Leon- ardo de Moura, Manuel Fähndrich, Nar Ganapathy, Jon Hagen, Rahul Kumar, Shuvendu Lahiri, Jim Larus, Rustan Leino, Xavier Leroy, Juncao Li, Jakob Lichtenberg, Rupak Majumdar, Johan Marien, Con McGarvey, Todd Mill- stein, Arvind Murching, Mayur Naik, Aditya Nori, Bohus Ondrusek, Adrian Oney, Onur Oyzer, Edgar Pek, Andreas Podelski, Shaz Qadeer, Bob Rinne, Robby, Stefan Schwoon, Adam Sha- piro, Rob Short, Fabio Somenzi, Am- itabh Srivastava, Antonios Stampoulis, Donn Terry, Abdullah Ustuner, Westley Weimer, Georg Weissenbacher, Peter Wieland, and Fei Xie. References 1. aiken, a., Bugrara, s., Dillig, I., Dillig, t., hackett, B., and hawkins, P. an overview of the saturn project. In Proceedings of the 2007 ACM SIGPLAN-SIGSOFT Workshop on Program Analysis for Software Tools and Engineering (san Diego, June 13–14). acM Press, new york, 2007, 43–48. 2. Ball, t., Majumdar, r., Millstein, t., and rajamani, s.k. automatic predicate abstraction of c programs. In Proceedings of the 2001 ACM SIGPLAN Conference on Programming Language Design and Implementation (snowbird, ut, June 20–22). acM Press, new york, 2001, 203–213. 3. Ball, t., Millstein, t.D., and rajamani, s.k. Polymorphic predicate abstraction. ACM Transactions on Programming Languages and Systems 27, 2 (Mar. 2005), 314–343. 4. Ball, t., Podelski, a., and rajamani, s.k. Boolean and cartesian abstractions for model checking c programs. In Proceedings of the Seventh International Conference on Tools and Algorithms for Construction and Analysis of Systems (genova, Italy, apr. 2–6). springer, 2001, 268–283. 5. Ball, t. and rajamani, s.k. Bebop: a symbolic model checker for Boolean programs. In Proceedings of the Seventh International SPIN Workshop on Model Checking and Software Verification (stanford, ca, aug. 30–sept. 1). springer, 2000, 113–130. 6. Ball, t. and rajamani, s.k. Boolean Programs: A Model and Process for Software Analysis. Technical Report MSR-TR-2000-14. Microsoft research, redmond, Wa, feb. 2000. 7. Ball, t. and rajamani, s.k. automatically validating temporal safety properties of interfaces. In Proceedings of the Eighth International SPIN Workshop on Model Checking of Software Verification (toronto, May 19–20). springer, 2001, 103–122. Diego, ca, feb. 10–13). the Internet society, 2008. 28. godefroid, P., nori, a.V., rajamani, s.k., and tetali, s.D. compositional may-must program analysis: unleashing the power of alternation. In Proceedings of the 37 th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Madrid, Jan. 17–23). acM Press, new york, 2010, 43–56. 29. graf, s. and saïdi, h. construction of abstract state graphs with PVs. In Proceedings of the Ninth International Conference on Computer-Aided Verification (haifa, June 22–25). springer, 72–83. 30. henzinger, t.a., Jhala, r., Majumdar, r., and McMillan, k.l. abstractions from proofs. In Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (Venice, Jan. 14–16). acM Press, new york, 2004, 232–244. 31. henzinger, t.a., Jhala, r., Majumdar, r., and sutre, g. lazy abstraction. In Proceedings of the 29 th ACM SIGPLAN-SIGACT Symposium Principles of Programming Languages (Portland, or, Jan. 16–18). acM Press, new york, 2002, 58–70. 32. holzmann, g. the sPIn model checker. IEEE Transactions on Software Engineering 23, 5 (May 1997), 279–295. 33. holzmann, g. logic verification of ansI-c code with sPIn. In Proceedings of the Seventh International SPIN Workshop on Model Checking and Software Verification (stanford, ca, aug. 30–sept. 1). springer, 2000, 131–147. 34. Ivancic, f., yang, Z., ganai, M.k., gupta, a., and ashar, P. efficient sat-based bounded model checking for software verification. Theoretical Computer Science 404, 3 (sept. 2008), 256–274. 35. kurshan, r. Computer-aided Verification of Coordinating Processes. Princeton university Press, Princeton, nJ, 1994. 36. la torre, s., Parthasarathy, M., and Parlato, g. analyzing recursive programs using a fixed-point calculus. In Proceedings of the 2009 ACM SIGPLAN Conference on Programming Language Design and Implementation (Dublin, June 15–21). acM Press, new york, 2009, 211–222. 37. larus, J.r., Ball, t., Das, M., Deline, r., fähndrich, M., Pincus, J., rajamani, s.k., and Venkatapathy, r. righting software. IEEE Software 21, 3 (May/June 2004), 92–100. 38. McMillan, k. Symbolic Model Checking: An Approach to the State-Explosion Problem. kluwer academic Publishers, 1993. 39. McMillan, k.l. Interpolation and sat-based model checking. In Proceedings of the 15th International Conference on Computer-Aided Verification (Boulder, co, July 8–12). springer, 2003, 1–13. 40. Qadeer, s. and Wu, D. kIss: keep it simple and sequential. In Proceedings of the ACM SIGPLAN 2004 Conference on Programming Language Design and Implementation (Washington, D.c., June 9–12). acM Press, new york, 2004, 14–24. 41. Queille, J. and sifakis, J. specification and verification of concurrent systems in cesar. In Proceedings of the Fifth International Symposium on Programming (torino, Italy, apr. 6–8). springer, 1982, 337–350. 42. reps, t., horwitz, s., and sagiv, M. Precise interprocedural data flow analysis via graph reachability. In Proceedings of the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (san francisco, Jan. 23–25). acM Press, new york, 1995, 49–61. 43. sharir, M. and Pnueli, a. two approaches to interprocedural data flow analysis. In Program Flow Analysis: Theory and Applications, n.D. Jones and s.s. Muchnick, eds. Prentice-hall, 1981, 189–233. 44. Vardi, M.y. and Wolper, P. an automata theoretic approach to automatic program verification. In Proceedings of the Symposium Logic in Computer Science (cambridge, Ma, June 16–18). Ieee computer society Press, 1986, 332–344. Thomas Ball (tball@microsoft.com) is a principal researcher, managing the software reliability research group in Microsoft research, redmond, Wa. Vladimir Levin (vladlev@microsoft.com) is a principal software design engineer and the technical lead of the static Driver Verification project in Windows in Microsoft, redmond, Wa. Sriram Rajamani (sriram@microsoft.com) is assistant managing director of Microsoft research India, Bangalore. © 2011 acM 0001-0782/11/07 $10.00","libVersion":"0.2.1","langs":""}