{"path":".obsidian/plugins/text-extractor/cache/ce7d984ff78d962d6426b495ecef6903.json","text":"UBC CPSC 340 2017W1 FINAL EXAM TIME: 150 minutes Name: Student number: CWL username: Signature: By signing above, I hereby acknowledge that I did not / will not cheat on this exam. • Do not open the exam until you are directed to do so. • Once you open the exam, make sure that it contains this cover page plus ?? pages of exam questions. • Two letter-size sheets (both sides) of notes is allowed. No other material or accessories may be used. • You may use either pen or pencil, but exams written in pencil may not be eligible for regrading. • Please be prepared to present, upon request, a student card for identiﬁcation. • If you need more space, use the 2 blank page at the end of the exam, and clearly indicate that your work continues there. • Most questions require a short answer. Work eﬃciently and avoid writing lengthy answers. • Make sure to work quickly, and come back to diﬃcult questions if you don’t the answer right away. • If anything is unclear or seems ambiguous, state your assumptions. • Please look up occassionally in case there are clariﬁcations written on the projector. • You are weclome to (quietly) leave early if you ﬁnish in under 150 minutes, but please do not leave in the last 10 minutes as it is very distracting to those who want to work up to the last minute. Run LATEX again to produce the table G O O D L U C K !! CPSC 340 Midterm Exam Question 1. (?? points) Answer the questions below. Be concise: avoid spending valuable time on lengthy answers. (a)2 pts Describe a setting where using a validation set to choose hyper-parameters can lead to overﬁtting? (b)2 pts What is the diﬀerence between using a validation set and using cross-validation? (c)2 pts Why is the IID assumption important in supervised learning? Page 1 of ?? CPSC 340 Midterm Exam (d)2 pts Consider a variation on decision stumps where each stump can consider k diﬀerent features instead of just one. How would k aﬀect the two parts of the fundamental trade-oﬀ? (e)2 pts Consider the following variation on the “condensed” k-nearest neighbour algorithm from your assignment: we make our predictions based on the m “best” training examples rather than all n training examples (for a ﬁxed m). Would this be a parametric or a non- parametric model? (a)2 pts Supervised learning models take in an xi and output a yi, while many clustering methods also learn to predict a yi given an xi. What is the key diﬀerence in their training? Page 2 of ?? CPSC 340 Midterm Exam Question 2. (?? points) Answer the questions below. Be concise: avoid spending valuable time on lengthy answers. (a)2 pts Consider an ensemble clustering method that generates m diﬀerent boostraps of the data. It then ﬁts a k-means model (with a random initialization) to each of the boostraps. To form the ﬁnal clustering for example xi, it chooses the yi that is most common across the m clusterings. Would this be an eﬀective or an inneﬀective ensemble method? (b)2 pts Suppose we have a supervised learning problem where we think the examples xi form clusters. To deal with this, we combine our training and test data together and ﬁt a k-means classiﬁer. We then add the cluster number as an extra feature, ﬁt our supervised learning model based on the training data, then evaluate it on the test data. What have we done wrong? (c)2 pts What is the advantage of trying k-means with several diﬀerent initializations? Should we run DBSCAN (density-based clustering) with several diﬀerent initializations? Page 3 of ?? CPSC 340 Midterm Exam (d)2 pts What is an advantage of using hierarchical clustering over “ﬂat” clustering methods like k-means and DBSCAN. (e)2 pts How does λ in an L0-regularizer (like BIC) aﬀect the sparsity pattern and the two parts of the fundamental trade-oﬀ? (f)2 pts Instead of the L1-regularizer λ ∑d j=1 |wj|, consider a regularizer of the form λ ∑d j=1 √ |wj|. What would an advantage and disadvantage of this regularizer be compared to L1-regularization? (Hint: think about the diﬀerence between the L2-regularizer and the L1-regularizer.) Page 4 of ?? CPSC 340 Midterm Exam Question 3. (?? points) Answer the questions below. Be concise: avoid spending valuable time on lengthy answers. (a)2 pts In Part 3 of the course, why did we add a column of “1” values to our matrix X? Does it make sense to use this trick for the methods from Part 1 of the course (decision trees, naive Bayes, k-nearest neighbours, etc.)? (b)2 pts For supervised training of a linear model wT xi with yi ∈ {−1, +1}, why do we use the logistic loss intead of the squared error? (c)2 pts What is the key diﬀerence between “one vs. all” logistic regression and training using the softmax loss? Page 5 of ?? CPSC 340 Midterm Exam (d)2 pts Give a supervised learning scenario where you would use the Laplace likelihood and a scenario where you would use a Laplace prior. (e)2 pts What are 3 uses for latent-factor models? (f)2 pts What are two reasons that the minimizer W of the PCA objective is not unique? Page 6 of ?? CPSC 340 Midterm Exam Question 4. (?? points) Answer the questions below. Be concise: avoid spending valuable time on lengthy answers. (a)2 pts We said that PCA and k-means are trying to optimize the same objective function (sum of squared errors)? If we could ﬁnd the optimal k-means clustering, why would it have a higher value of this objective function than PCA? (b)2 pts In what setting would we prefer to use ISOMAP over PCA? (c)2 pts What do we use the backpropagation algorithm for? Page 7 of ?? CPSC 340 Midterm Exam (d)2 pts Consider a deep neural network with 1 million hidden units. Is this a parametric or a non-parametric classiﬁer? (e)2 pts A Coursera course says that convolutional neural networks can now achieve the best test error on every learning problem. Which theorem we discussed in class proves that this is non-sense? (f)2 pts What are two reasons that convolutional neural networks overﬁt less than classic neural networks? Page 8 of ?? CPSC 340 Midterm Exam Question 5. (?? points) Match each loss function and regularizer to the corresponding property by writing the corre- sponding number next to each item: (a)1 pt 0-1 loss. (b)1 pt Absolute loss. (c)1 pt Hinge loss. (d)1 pt Squared loss. (e)1 pt Softmax loss. (f)1 pt L0-regularization. (g)1 pt L1-regularization. (h)1 pt L2-regularization. (i)1 pt L∞-regularization. (j)1 pt Logistic loss. (k)1 pt Max of absolute residuals loss. 1. Gaussian likelihood. 2. Makes least squares solution unique. 3. Makes variables have the same magnitude. 4. Non-convex feature selection. 5. Number of training errors. 6. Regularization and feature selection. 7. Robust to outliers. 8. Smooth loss for binary classiﬁcation. 9. Training all wc simultaneously. 10. Tries to get the outliers right. 11. Upper bound on 0-1 loss. Page 9 of ?? CPSC 340 Midterm Exam Question 6. (?? points) Consider the dataset below, which has 10 training examples and 2 features: X =                 1 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0 1 1 1 0                 , y =                 1 1 1 1 1 1 −1 −1 −1 −1                 . Suppose you want to classify the following test example: ˆx = [ 1 1] . (a)2 pts What value of yi would a 4-nearest neighbour classiﬁer predict for this test example? (b)2 pts What value of yi would a logistic regression model predict if it had the below weights? w = [ 0.75 −0.80 ] Page 10 of ?? CPSC 340 Midterm Exam (c)3 pts Below are the probabilities needed in a naive Bayes classiﬁer to make a decision on the test example. Compute the missing values “?” values. • p(y = 1) = 3/5 • p(y = −1) =? • p(x1 = 1|y = 1) = 2/3 • p(x2 = 1|y = 1) =? • p(x1 = 1|y = −1) =? • p(x2 = 1|y = −1) =? (d)3 pts Under the naive Bayes model and your estimates of the above probabilities, what is the most likely label for the test example? (Show your work.) Page 11 of ?? CPSC 340 Midterm Exam Question 7. (?? points) Let n be the number of training examples, d be the number of features, and T be an upper bound on the number of iterations of gradient descent iterations we perform. Recall that the logistic regression model minimizes an objective of the form f (w) = n∑ i=1 log(1 + exp(−yiwT xi)), and its gradient has the form ∇f (w) = − n∑ i=1 yi 1 + exp(yiwT xi) xi. (a)3 pts What is the cost of training a logistic regression model on the training set? (State your reasoning.) (b)3 pts What is the cost using forward selection to try to select the features that optimize a logistic regression model with an L0-regularization penalty like BIC? (State your reasoning.) Page 12 of ?? CPSC 340 Midterm Exam Question 8. (?? points) (a)4 pts Consider a variation on L2-regularized least squares where we have weights on the training examples and within the regularizer, with an objective of the form f (w) = 1 2 (Xw − y) T Z(Xw − y) + λ 2 ∥Aw∥2. Here, Z is an n × n diagonal matrix with non-negative values zi along the diagonals and A is a k × d matrix. Write a linear system whose solution gives the minimum of this (convex quadratic) objective function. (b)4 pts Consider a function P (r) that takes an n × 1 vector r and returns an n × 1 vector u, where ui = ri if ri is positive and ui = 0 otherwise. Use this function to write the hinge loss with weighted L1-regularization, f (w) = n∑ i=1 max{0, 1 − yiwT xi} + λ d∑ j=1 vj|wj|, in matrix and norm notation. You may ﬁnd it helpful to use Y as a diagonal matrix with the yi along the diagonal, V as a diagonal matrix withthe non-negative values vj along the diagonal, and 1 as a vector containing all ones. Page 13 of ?? CPSC 340 Midterm Exam Question 9. (?? points) (a)3 pts Consider an L2-regularized multi-class objective function, where for each training example we have a set C of pairs of classes (c1, c2) where we prefer class c1 to c2, f (w) = n∑ i=1 ∑ (c1,c2)∈C max{0, 1 − wT c1xi + wT c2xi} + λ∥W ∥2 F . Show that this function is convex (you can assume λ > 0). In PCA we approximate a centered matrix X by the matrix product ZW . What are the sizes of the following quantities we used in describing the PCA model? (a)1 pt zi (b)1 pt zic (c)1 pt wj (d)1 pt Wc (e)1 pt wT j zi (f)1 pt W xi Page 14 of ?? CPSC 340 Midterm Exam Question 10. (?? points) Consider a supervised learning problem where we have training examples X with associated labels y, and test examples ˆX. Let Z denote a matrix obtained from a change of basis of X, and similarly let ˆZ be the same change of basis applied to ˆX. Let n be the number of training examples, t be the number of test examples, d be the number of features, and k be the number of features after the change of basis from X to Z. What is the cost in O() notation of the following operations in terms of n, d, t, and k? (a)3 pts Fitting an L2-regularized least squares model on the training data {X, y} and then apply- ing it to the test data ˆX, ˆy = ˆXw, where w = (X T X + λI) −1X T y. (b)2 pts Performing the same two operations under a change of basis (you can assume that forming each element zij of Z costs O(1), and forming an element of ˆZ has the same cost), ˆy = ˆZw, where w = (ZT Z + λI) −1ZT y. (c)2 pts Computing the same prediction using the “other” normal equations, ˆy = ˆZw, where w = ZT (ZZT + λI) −1y. Page 15 of ?? CPSC 340 Midterm Exam Question 11. (?? points) (a)4 pts The student-t likelihood has the form p(yi|xi, w) = Γ ( η+1 2 ) √ηπΓ ( η 2 ) ( 1 + (wT xi − yi)2 η )− η+1 2 , where η is the “degrees of freedom” (which is at least 1) and Γ is the “gamma” function. If we use an independent Laplace prior with a mean of 0 and scale of 1/λ for each wj, p(wj) = λ 2 exp(−λ|wj|), derive an objective function that is equivalent to performing MAP estimation on n IID training examples in the model (you should simplify as much as possible, but don’t have to use matrix/norm notation). (b)3 pts State whether the above objective function has or does not have each of the below 3 properties. • Robust to outliers. • Sets wj values to exactly 0. • Is convex. Page 16 of ?? CPSC 340 Midterm Exam This page is intentionally blank. You can use it for scratch work or to continue an answer if you run out of space somewhere. Page 17 of ?? CPSC 340 Midterm Exam This page is intentionally blank. You can use it for scratch work or to continue an answer if you run out of space somewhere. Page 18 of ??","libVersion":"0.2.1","langs":""}