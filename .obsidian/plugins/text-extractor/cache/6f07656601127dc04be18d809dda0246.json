{"path":".obsidian/plugins/text-extractor/cache/6f07656601127dc04be18d809dda0246.json","text":"CPSC 340: Machine Learning and Data Mining K-Means Clustering Fall 2022 Admin â€¢ If I forget to post the slides, virtually identical ones are here â€¢ https://www.cs.ubc.ca/~schmidtm/Courses/340-F22/ Last Time: Ensemble Methods â€¢ Ensemble methods are models that other models as input. â€“ The ensemble can often achieve higher accuracy than individual models. â€¢ One of the simplest ensemble methods is voting: â€“ Take the mode of the predictions across the classifiers. â€“ Higher accuracy than individual classifiers if error are independent. â€¢ Random forests: â€“ Ensemble method based on deep decesision trees, incorporating two forms of randomness. â€“ Each tree is trained on a boostrap sample of the data (â€˜nâ€™ examples sampled with replacement). â€“ We use random trees (covered today) to further encourage errors to be independent. Random Forest Ingredient 1: Bootstrap/Bagging â€¢ Bootstrap sample of a list of â€˜nâ€™ training examples: â€“ A new set of size â€˜nâ€™ chosen independently with replacement. â€“ Gives new dataset of â€˜nâ€™ examples, with some duplicated and some missing. â€¢ For large â€˜nâ€™, approximately 63% of original examples are included at least once in bootstrap. â€¢ Bagging: ensemble where you apply same classifier to different bootstraps. 1. Generate several bootstrap samples of the dataset. 2. Fit the classifier to each bootstrap sample. â€“ To make predictions, take vote based on the predictions. â€¢ Random forests are a special case of bagging, using random trees as the classifier. Random Forest Ingredient 2: Random Trees â€¢ For each split in a random tree model: â€“ Randomly sample a small number of possible features (typically ). â€“ Only consider these random features when searching for the optimal rule. â€¢ So splits will tend to use different features in different trees. âˆš ğ‘‘ Random Forest Ingredient 2: Random Trees â€¢ For each split in a random tree model: â€“ Randomly sample a small number of possible features (typically ). â€“ Only consider these random features when searching for the optimal rule. â€¢ So splits will tend to use different features in different trees. âˆš ğ‘‘ Random Forests: Putting it all Together â€¢ Training: Random Forests: Putting it all Together â€¢ Prediction: Random Forests: Discussion â€¢ Random forest implementations use deep random trees. â€“ Often splitting until all leafs have only one label. â€¢ So the individual trees tend to overfit. â€“ But bootstrapping and random trees makes errors more independent. â€¢ So the vote tends to have a much lower test error than individual trees. â€¢ Empirically, random forests are often one of the â€œbestâ€ classifiers. â€“ Fernandez-Delgado et al. [2014]: â€¢ Compared 179 classifiers on 121 datasets. â€¢ Random forests were most likely to be the best classifier. â€“ Grinsztajn et al. [2022]: â€¢ â€œWhy do tree-based models still outperform deep learning on tabular data?â€ Beyond Voting: Model Averaging â€¢ Voting is a special case of â€œaveragingâ€ ensemble methods. â€“ Where we somehow â€œaverageâ€ the predictions of different models. â€¢ Other averaging methods: â€“ For â€œregressionâ€ (where y i is continuous), take average y i predictions: â€“ With probabilistic classifiers, take the average probabilities: â€“ And there are variations where some classifiers get more weight (see bonus): Types and Goals of Ensemble Methods â€¢ Remember the fundamental trade-off: 1. E train : How small you can make the training error. vs. 2. E approx : How well training error approximates the test error. â€¢ Goal of ensemble methods is that meta-classifier: â€“ Does much better on one of these than individual classifiers. â€“ Does not do too much worse on the other. â€¢ This suggests two types of ensemble methods: 1. Averaging: improves approximation error of classifiers with high E approx . â€¢ This is the point of â€œvotingâ€. 2. Boosting: improves training error of classifiers with high E train . â€¢ Covered later in course. End of Part 1: Key Concepts â€¢ Fundamental ideas: â€“ Training vs. test error (memorization vs. learning). â€“ IID assumption (examples come independently from same distribution). â€“ Golden rule of ML (test set should not influence training). â€“ Fundamental trade-off (between training error vs. approximation error). â€“ Validation sets and cross-validation (can approximate test error) â€“ Optimization bias (we can overfit the training set and the validation set). â€“ Decision theory (we should consider costs of predictions). â€“ Parametric vs. non-parametric (whether model size depends on â€˜nâ€™). â€“ No free lunch theorem (there is no universally â€œbestâ€ model). End of Part 1: Key Concepts â€¢ We saw 3 ways of â€œlearningâ€: â€“ Searching for rules. â€¢ Decision trees (greedy recursive splitting using decision stumps). â€“ Counting frequencies. â€¢ NaÃ¯ve Bayes (probabilistic classifier based on conditional independence). â€“ Measuring distances. â€¢ K-nearest neigbbours (non-parametric classifier with universal consistency). â€¢ We saw 2 generic ways of improving performance: â€“ Encouraging invariances with data augmentation. â€“ Ensemble methods (combine predictions of several models). â€¢ Random forests (averaging plus randomization to reduce overfitting). Next Topic: Unsupervised Learning (Part 2) Application: Classifying Cancer Types â€¢ â€œI collected gene expression data for 1000 different types of cancer cells, can you tell me the different classes of cancer?â€ â€¢ We are not given the class labels y, but want meaningful labels. â€¢ An example of unsupervised learning. X = https://corelifesciences.com/human-long-non-coding-rna-expression-microarray-service.html Unsupervised Learning â€¢ Supervised learning: â€“ We have features x i and class labels y i . â€“ Write a program that produces y i from x i . â€¢ Unsupervised learning: â€“ We only have x i values, but no explicit target labels. â€“ You want to do â€œsomethingâ€ with them. â€¢ Some unsupervised learning tasks: â€“ Outlier detection: Is this a â€˜normalâ€™ x i ? â€“ Similarity search: Which examples look like this x i ? â€“ Association rules: Which x j occur together? â€“ Latent-factors: What â€˜partsâ€™ are the x i made from? â€“ Data visualization: What does the high-dimensional X look like? â€“ Ranking: Which are the most important x i ? â€“ Clustering: What types of x i are there? Clustering Example Input: data matrix â€˜Xâ€™. â€¢ In clustering we want to assign examples to â€œgroupsâ€: Clustering Example Input: data matrix â€˜Xâ€™. Output: clusters . ^ ğ‘¦ â€¢ In clustering we want to assign examples to â€œgroupsâ€: Clustering â€¢ Clustering: â€“ Input: set of examples described by features x i . â€“ Output: an assignment of examples to â€˜groupsâ€™. â€¢ Unlike classification, we are not given the â€˜groupsâ€™. â€“ Algorithm must discover groups. â€¢ Example of groups we might discover in e-mail spam: â€“ â€˜Lucky winnerâ€™ group. â€“ â€˜Weight lossâ€™ group. â€“ â€˜I need your helpâ€™ group. â€“ â€˜Mail-order brideâ€™ group. Data Clustering â€¢ General goal of clustering algorithms: â€“ Examples in the same group should be â€˜similarâ€™. â€“ Examples in different groups should be â€˜differentâ€™. â€¢ But the â€˜bestâ€™ clustering is hard to define: â€“ We donâ€™t have a test error. â€“ Generally, there is no â€˜bestâ€™ method in unsupervised learning. â€¢ So there are lots of methods: weâ€™ll focus on important/representative ones. â€¢ Why cluster? â€“ You could want to know what the groups are. â€“ You could want to find the group for a new example x i . â€“ You could want to find examples related to a new example x i . â€“ You could want a â€˜prototypeâ€™ example for each group. â€¢ For example, what does the a typical breakfast look like? Clustering of Epstein-Barr Virus http://jvi.asm.org/content/86/20/11096.abstract Other Clustering Applications â€¢ NASA: what types of stars are there? â€¢ Biology: are there sub-species? â€¢ Documents: what kinds of documents are on my HD? â€¢ Commercial: what kinds of customers do I have? http://www.eecs.wsu.edu/~cook/dm/lectures/l9/index.html http://www.biology-online.org/articles/canine_genomics_genetics_running/figures.html K-Means â€¢ Most popular clustering method is k-means. â€¢ Input: â€“ The number of clusters â€˜kâ€™ (hyper-parameter). â€“ Initial guess of the center (the â€œmeanâ€) of each cluster. â€¢ K-Means Algorithm for Finding Means: â€“ Assign each x i to its closest mean. â€“ Update the means based on the assignment. â€“ Repeat until convergence. K-Means Example Start with â€˜kâ€™ initial â€˜meansâ€™ (usually, random data points) Input: data matrix â€˜Xâ€™. K-Means Example Assign each example to the closest mean. Input: data matrix â€˜Xâ€™. K-Means Example Update the mean of each group. Input: data matrix â€˜Xâ€™. K-Means Example Assign each example to the closest mean. Input: data matrix â€˜Xâ€™. K-Means Example Update the mean of each group. Input: data matrix â€˜Xâ€™. K-Means Example Assign each example to the closest mean. Input: data matrix â€˜Xâ€™. K-Means Example Update the mean of each group. Input: data matrix â€˜Xâ€™. K-Means Example Assign each example to the closest mean. Input: data matrix â€˜Xâ€™. K-Means Example Stop if no examples change groups. Input: data matrix â€˜Xâ€™. K-Means Example Interactive demo: https://www.naftaliharris.com/blog/visualizing-k-means-clustering Input: data matrix â€˜Xâ€™. Output: - Clusters â€˜ â€™. - Means â€˜Wâ€™. ^ ğ‘¦ K-Means Issues â€¢ Guaranteed to converge when using Euclidean distance. â€¢ Given a new test example: â€“ Assign it to the nearest mean to cluster it. â€¢ Assumes you know number of clusters â€˜kâ€™. â€“ Lots of heuristics to pick â€˜kâ€™, none satisfying: â€¢ https://en.wikipedia.org/wiki/Determining_the_number_of_clusters_in_a_data_set â€¢ Each example is assigned to one (and only one) cluster: â€“ No possibility for overlapping clusters or leaving examples unassigned. â€¢ It may converge to sub-optimal solutionâ€¦ K-Means Clustering with Different Initialization â€¢ Classic approach to dealing with sensitivity to initialization: random restarts. â€“ Try several different random starting points, choose the â€œbestâ€. â€¢ See bonus slides for a more clever approach called k-means++. KNN vs. K-Means â€¢ Donâ€™t confuse KNN classification and k-means clustering: Property KNN Classification K-Means Clustering Task Supervised learning (given y i ) Unsupervised learning (no given y i ). Meaning of â€˜kâ€™ Number of neighbours to consider (not number of classes). Number of clusters (always consider single nearest mean). Initialization No training phase. Training that is sensitive to initialization. Model complexity Model is complicated for small â€˜kâ€™, simple for large â€˜kâ€™. Model is simple for small â€˜kâ€™, complicated for large â€˜kâ€™. Parametric? Non-parametric: - Stores data â€˜Xâ€™ Parametric (for â€˜kâ€™ not depending on â€˜nâ€™) - Stores means â€˜Wâ€™ What is K-Means Doing? â€¢ We can interpret K-means steps as minimizing an objective: â€“ Total sum of squared distances from each example x i to its center : â€¢ The k-means steps: â€“ Minimize â€˜fâ€™ in terms of the i (update cluster assignments). â€“ Minimize â€˜fâ€™ in terms of the w c (update means). â€¢ Termination of the algorithm follows because: â€“ Each step does not increase the objective. â€“ There are a finite number of assignments to k clusters. ğ‘¤ ^ ğ‘¦ ğ‘– ^ ğ‘¦ What is K-Means Doing? â€¢ We can interpret K-means steps as minimizing an objective: â€“ Total sum of squared distances from each example x i to its center : â€¢ The k-means steps: â€“ Minimize â€˜fâ€™ in terms of the i (update cluster assignments). â€“ Minimize â€˜fâ€™ in terms of the w c (update means). â€¢ Use â€˜fâ€™ to choose between initializations (fixed â€˜kâ€™). â€¢ Need to change w c update under other distances: â€“ L1-norm: set w c to median (â€œk-mediansâ€, see bonus). ğ‘¤ ^ ğ‘¦ ğ‘– ^ ğ‘¦ Cost of K-means â€¢ Bottleneck is calculating distance from each x i to each mean w c : Cost of K-means â€¢ Bottleneck is calculating distance from each x i to each mean w c : â€“ Each time we do this costs O(d). â€¢ We need to compute distance from â€˜nâ€™ examples to â€˜kâ€™ clusters. â€¢ Total cost of assigning examples to clusters is O(ndk). â€“ Fast if k is not too large. â€¢ Updating means is cheaper: O(nd). Summary â€¢ Bagging: â€¢ Ensemble method where we apply same classifier to bootstrap samples. â€¢ Random forests: bagging of deep randomized decision trees. â€¢ One of the best â€œout of the boxâ€ classifiers. â€¢ Type of ensemble methods: â€“ â€œBoostingâ€ reduces E train and â€œaveragingâ€ reduces E approx . â€¢ Unsupervised learning: fitting data without explicit labels. â€¢ Clustering: finding â€˜groupsâ€™ of related examples. â€¢ K-means: simple iterative clustering strategy. â€“ Fast but sensitive to initialization. â€¢ Next time: â€“ John Snow and non-parametric clustering. Extremely-Randomized Trees â€¢ Extremely-randomized trees add an extra level of randomization: 1. Each tree is fit to a bootstrap sample. 2. Each split only considers a random subset of the features. 3. Each split only considers a random subset of the possible thresholds. â€¢ So instead of considering up to â€˜nâ€™ thresholds, only consider 10 or something small. â€“ Leads to different partitions so potentially more independence. Bayesian Model Averaging â€¢ Recall the key observation regarding ensemble methods: â€“ If models overfit in â€œdifferentâ€ ways, averaging gives better performance. â€¢ But should all models get equal weight? â€“ E.g., decision trees of different depths, when lower depths have low training error. â€“ E.g., a random forest where one tree does very well (on validation error) and others do horribly. â€“ In science, research may be fraudulent or not based on evidence. â€¢ In these cases, naÃ¯ve averaging may do worse. Bayesian Model Averaging â€¢ Suppose we have a set of â€˜mâ€™ probabilistic binary classifiers w j . â€¢ If each one gets equal weight, then we predict using: â€¢ Bayesian model averaging treats model â€˜w j â€™ as a random variable: â€¢ So we should weight by probability that w j is the correct model: â€“ Equal weights assume all models are equally probable. Bayesian Model Averaging â€¢ Can get better weights by conditioning on training set: â€¢ The â€˜likelihoodâ€™ p(y | w j , X) makes sense: â€“ We should give more weight to models that predict â€˜yâ€™ well. â€“ Note that hidden denominator penalizes complex models. â€¢ The â€˜priorâ€™ p(w j ) is our â€˜beliefâ€™ that w j is the correct model. â€¢ This is how rules of probability say we should weigh models. â€“ The â€˜correctâ€™ way to predict given what we know. â€“ But it makes some people unhappy because it is subjective. What is K-Means Doing? â€¢ How are are k-means step decreasing this objective? â€¢ If we just write as function of a particular i , we get: â€“ The â€œconstantâ€ includes all other terms, and doesnâ€™t affect location of min. â€“ We can minimize in terms of i by setting it to the â€˜câ€™ with w c closest to x i . ^ ğ‘¦ ^ ğ‘¦ What is K-Means Doing? â€¢ How are are k-means step decreasing this objective? â€¢ If we just write as function of a particular w cj we get: â€¢ Derivative is given by: â€¢ Setting equal to 0 and solving for w cj gives: K-Medians Clustering â€¢ With other distances k-means may not converge. â€“ But we can make it converge by changing the updates so that they are minimizing an objective function. â€¢ E.g., we can use the L1-norm objective: â€¢ Minimizing the L1-norm objective gives the â€˜k-mediansâ€™ algorithm: â€“ Assign points to clusters by finding â€œmeanâ€ with smallest L1-norm distance. â€“ Update â€˜meansâ€™ as median value (dimension-wise) of each cluster. â€¢ This minimizes the L1-norm distance to all the points in the cluster. â€¢ This approach is more robust to outliers. What is the â€œL1-norm and medianâ€ connection? â€¢ Point that minimizes the sum of squared L2-norms to all points: â€“ Is given by the mean (just take derivative and set to 0): â€¢ Point that minimizes the sum of L1-norms to all all points: â€“ Is given by the median (derivative of absolute value is +1 if positive and -1 if negative, so any point with half of points larger and half of points smaller is a solution). K-Medoids Clustering â€¢ A disadvantage of k-means in some applications: â€“ The means might not be valid data points. â€“ May be important for vector quantiziation. â€¢ E.g., consider bag of words features like [0,0,1,1,0]. â€“ We have words 3 and 4 in the document. â€¢ A mean from k-means might look like [0.1 0.3 0.8 0.2 0.3]. â€“ What does it mean to have 0.3 of word 2 in a document? â€¢ Alternative to k-means is k-medoids: â€“ Same algorithm as k-means, except the means must be data points. â€“ Update the means by finding example in cluster minimizing squared L2- norm distance to all points in the cluster. K-Means Initialization â€¢ K-means is fast but sensitive to initialization. â€¢ Classic approach to initialization: random restarts. â€“ Run to convergence using different random initializations. â€“ Choose the one that minimizes average squared distance of data to means. â€¢ Newer approach: k-means++ â€“ Random initialization that prefers means that are far apart. â€“ Yields provable bounds on expected approximation ratio. K-Means++ â€¢ Steps of k-means++: 1. Select initial mean w 1 as a random x i . 2. Compute distance d ic of each example x i to each mean w c . 3. For each example â€˜iâ€™ set d i to the distance to the closest mean. 4. Choose next mean by sampling an example â€˜iâ€™ proportional to (d i ) 2 . 5. Keep returning to step 2 until we have k-means. â€¢ Expected approximation ratio is O(log(k)). K-Means++K-Means++ First mean is a random example. K-Means++ Weight examples by distance to mean squared. K-Means++ Sample mean proportional to distances squared. K-Means++ Weight examples by squared distance to nearest mean. K-Means++ Sample mean proportional to minimum distances squared. K-Means++ Weight examples by squared distance to mean. K-Means++ Sample mean proportional to distances squared. (Now hit chosen target k=4.) K-Means++ Start k-means: assign examples to the closest mean. K-Means++ Update the mean of each cluster. K-Means++ In this case: just 2 iterations! Update the mean of each cluster. Discussion of K-Means++ â€¢ Recall the objective function k-means tries to minimize: â€¢ The initialization of â€˜Wâ€™ and â€˜câ€™ given by k-means++ satisfies: â€¢ Get good clustering with high probability by re-running. â€¢ However, there is no guarantee that c * is a good clustering. Uniform Sampling â€¢ Standard approach to generating a random number from {1,2,â€¦,n}: 1. Generate a uniform random number â€˜uâ€™ in the interval [0,1]. 2. Return the largest index â€˜iâ€™ such that u â‰¤ i/n. â€¢ Conceptually, this divides interval [0,1] into â€˜nâ€™ equal-size pieces: â€¢ This assumes p i = 1/n for all â€˜iâ€™. Non-Uniform Sampling â€¢ Standard approach to generating a random number for general p i . 1. Generate a uniform random number â€˜uâ€™ in the interval [0,1]. 2. Return the largest index â€˜iâ€™ such that u â‰¤ â€¢ Conceptually, this divides interval [0,1] into non-equal-size pieces: â€¢ Can sample from a generic discrete probability distribution in O(n). â€¢ If you need to generate â€˜mâ€™ samples: â€“ Cost is O(n + m log (n)) with binary search and storing cumulative sums. How many iterations does k-means take? â€¢ Each update of the â€˜ i â€™ or â€˜w c â€™ does not increase the objective â€˜fâ€™. â€¢ And there are k n possible assignments of the i to â€˜kâ€™ clusters. â€¢ So within k n iterations you cannot improve the objective by changing i , and the algorithm stops. â€¢ Tighter-but-more-complicated â€œsmoothedâ€ analysis: â€“ https://arxiv.org/pdf/0904.1113.pdf ^ ğ‘¦ ^ ğ‘¦ ^ ğ‘¦ Vector Quantization: Image Colors â€¢ Usual RGB representation of a pixelâ€™s color: three 8-bit numbers. â€“ For example, [241 13 50] = . â€“ Can apply k-means to find set of prototype colours. Original: (24-bits/pixel) K-means predictions: (6-bits/pixel) Run k-means with 2 6 clusters: Vector Quantization: Image Colors â€¢ Usual RGB representation of a pixelâ€™s color: three 8-bit numbers. â€“ For example, [241 13 50] = . â€“ Can apply k-means to find set of prototype colours. Original: (24-bits/pixel) K-means predictions: (6-bits/pixel) Run k-means with 2 6 clusters: Vector Quantization: Image Colors â€¢ Usual RGB representation of a pixelâ€™s color: three 8-bit numbers. â€“ For example, [241 13 50] = . â€“ Can apply k-means to find set of prototype colours. Original: (24-bits/pixel) K-means predictions: (3-bits/pixel) Run k-means with 2 6 clusters: Vector Quantization: Image Colors â€¢ Usual RGB representation of a pixelâ€™s color: three 8-bit numbers. â€“ For example, [241 13 50] = . â€“ Can apply k-means to find set of prototype colours. Original: (24-bits/pixel) K-means predictions: (2-bits/pixel) Run k-means with 2 6 clusters: Vector Quantization: Image Colors â€¢ Usual RGB representation of a pixelâ€™s color: three 8-bit numbers. â€“ For example, [241 13 50] = . â€“ Can apply k-means to find set of prototype colours. Original: (24-bits/pixel) K-means predictions: (1-bit/pixel) Run k-means with 2 6 clusters:","libVersion":"0.2.1","langs":""}