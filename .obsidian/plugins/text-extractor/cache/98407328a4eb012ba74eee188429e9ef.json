{"path":".obsidian/plugins/text-extractor/cache/98407328a4eb012ba74eee188429e9ef.json","text":"CPSC 340: Machine Learning and Data Mining Linear Regression Admin • No class Monday. – Truth & Reconcillation Day. • Assignment 2: – Due tonight. – 1 late day to hand in Wednesday, 2 for next Friday. • We’re going to start using calculus and linear algebra a lot. – You should start reviewing these ASAP if you are rusty. – A review of relevant calculus concepts is here. – A review of relevant linear algebra concepts is here. Supervised Learning Round 2: Regression • We’re going to revisit supervised learning: • Previously, we considered classification: – We assumed yi was categorical: yi = ‘spam’ or yi = ‘not spam’. • Now we are going to consider regression: – We allow yi to be numerical: yi = 10.34cm. Example: Dependent vs. Explanatory Variables • We want to predict a numerical value given features: – Does number of lung cancer deaths change with number of cigarettes? – Does number of skin cancer deaths change with latitude? http://www.cvgs.k12.va.us:81/digstats/main/inferant/d_regrs.html https://onlinecourses.science.psu.edu/stat501/node/11 Example: Dependent vs. Explanatory Variables • We want to predict a numerical value given features: – Do people in big cities walk faster? – Is the universe expanding or shrinking or staying the same size? http://hosting.astro.cornell.edu/academics/courses/astro201/hubbles_law.htm https://www.nature.com/articles/259557a0.pdf Example: Dependent vs. Explanatory Variables • We want to predict a numerical value given features: – Does number of gun deaths change with gun ownership? – Does number violent crimes change with violent video games? http://www.vox.com/2015/10/3/9444417/gun-violence-united-states-america https://www.soundandvision.com/content/violence-and-video-games Spurious Assocations • Does number of pirates help predict global temperature? – Spurious association. • Variables that seem related, but this is a coincidence. – If comparing enough variables, can get strong relationships by chance. • Fun with spurious correlations here. https://www.forbes.com/sites/erikaandersen/2012/03/23/true-fact-the-lack-of-pirates-is-causing-global-warming Example: Dependent vs. Explanatory Variables • We want to predict a numerical value given features: – Does higher gender equality index lead to more women STEM grads? • Note that we are doing supervised learning: – Trying to predict value of 1 variable (the ‘yi’ values). (instead of measuring correlation between 2). • Supervised learning does not give causality: – OK: “Higher index is correlated with lower grad %”. – OK: “Higher index helps predict lower grad %”. – BAD: “Higher index leads to lower grads %”. • People/media get these confused all the time, be careful! • There are lots of potential reasons for this correlation. https://www.weforum.org/agenda/2018/02/does-gender-equality-result-in-fewer-female-stem-grads/ Handling Numerical Labels • One way to handle numerical yi: discretize. – E.g., for ‘age’ could we use {‘age ≤ 20’, ‘20 < age ≤ 30’, ‘age > 30’}. – Now we can apply methods for classification to do regression. – But coarse discretization loses resolution. – And fine discretization requires lots of data (“coupon collecting”). • There exist regression versions of classification methods: – Regression trees, neighbour-based methods, and so on. • Today: one of oldest, but still most popular/important methods: – Linear regression based on squared error. – Interpretable and the building block for more-complex methods. Linear Regression in 1 Dimension • Assume we only have 1 feature (d = 1): – E.g., xi is number of cigarettes and yi is number of lung cancer deaths. • Linear regression makes predictions ො𝑦i using a linear function of xi: • The parameter ‘w’ is the weight or regression coefficient of xi. – We are temporarily ignoring the y-intercept. • As xi changes, slope ‘w’ affects the rate that ො𝑦i increases/decreases: – Positive ‘w’: ො𝑦i increase as xi increases. – Negative ‘w’: ො𝑦i decreases as xi increases. Linear Regression in 1 DimensionAside: terminology woes • Different fields use different terminology and symbols. – Data points ‘i’ = objects = examples = rows = observations. – Inputs xi = predictors = features = explanatory variables= regressors = independent variables = covariates = columns. – Outputs yi = outcomes = targets = response variables = dependent variables (also called a “label” if it’s categorical). – Regression coefficients ‘w’ = weights = parameters = betas. • With linear regression, the symbols are inconsistent too: – In ML, the data is X and y, and the weights are w. – In statistics, the data is X and y, and the weights are β. – In optimization, the data is A and b, and the weights are x. Linear Regression Training Challenges • Linear regression makes predictions by using: • To train a linear regression model, we need to find weight/slope ‘w’. • Challenges in finding ‘w’ compared to fitting a decision stump: – Cannot enumerate all possible values of ‘w’ (could be any real number). • Instead, we will use calculus to find the best ‘w’. – It is unlikely that a line will go exactly through many data points. • Due to noise, relationship not being quite linear, or just floating-point issues. • So it does not make sense to find the ‘w’ minimizing how many times ො𝑦𝑖 ≠ 𝑦𝑖. Residuals and Sum of Squared Residuals • The residual is the difference between our prediction and true value: – This can be positive or negative. – If this is close to zero, then our prediction is close to the true value. • We typically look for a ‘w’ that makes residuals close to zero. – For example, many models minimize the sum of the squared residuals: • The smaller we make this, the smaller the distance between our predictions and targets. – Plugging in ො𝑦𝑖 = 𝑤𝑥𝑖 for the case of linear regression, we get: • The linear least squares model minimizes this function to choose the slope ‘w’. Linear Least Squares Objective Function • Linear least squares sets ‘w’ is to minimize sum of squared residuals: • If this is zero, we exactly fit data. If this small, line is “close” to data. • There are some justifications for choosing this function ‘f’. – A probabilistic interpretation is coming later in the course. • But usually, we choose this ‘f’ because it is easy to minimize. Linear Least Squares Objective Function • Linear least squares sets ‘w’ is to minimize sum of squared residuals: Linear Least Squares Objective Function • Linear least squares sets ‘w’ is to minimize sum of squared residuals: Minimizing a Differential Function • Math 101 approach to minimizing a differentiable function ‘f’: 1. Take the derivative of ‘f’. 2. Find points ‘w’ where the derivative f’(w) is equal to 0. 3. Choose the smallest one (and check that f’’(w) is positive). Digression: Multiplying by a Positive Constant • Note that this problem: • Has the same set of minimizers as this problem: • And these also have the same minimizers: • I can multiply ‘f’ by any positive constant and not change solution. – Derivative will still be zero at the same locations. – We will use this trick a lot! (Quora trolling on ethics of this) Deriving Least Squares SolutionFinding Least Squares Solution • Finding ‘w’ that minimizes sum of squared errors: • Let’s check that this is a minimizer by checking second derivative: – Since (anything)2 is non-negative, we have f’’(w) ≥ 0. – If at least one feature is not zero, then f’’(w) > 0 and ‘w’ is a minimizer. Next Topic: Least Squares in d-Dimensions Motivation: Combining Explanatory Variables • Smoking is not the only contributor to lung cancer. – For example, there environmental factors like exposure to asbestos. • How can we model the combined effect of smoking and asbestos? • A simple way is with a 2-dimensional linear function: • We have a weight w1 for feature ‘1’ and w2 for feature ‘2’: Linear Regression in 2-Dimensions • Linear model: • This defines a two-dimensional plane. Linear Regression in 2-Dimensions • Linear model: • This defines a two-dimensional plane. • Not just a line! Linear Regression in d-Dimensions • If we have ‘d’ features, the d-dimensional linear model is: – In words, prediction is a weighted sum of the features. • We can re-write this using summation notation as: • We can again choose ‘w’ to minimize the sum of squared residuals: – Dates back to 1801: Gauss used it to predict location of the asteroid Ceres. – We can use multi-variable calculus to minimize ‘f’ with respect to the parameters w1, w2,…, wd. Minimizing Multi-Variable Differentiable Function • With one variable, we “find ‘w’ where the derivative is equal to 0”. • The generalization of this idea to when we have ‘d’ variables: – “Find ‘w’ where the gradient vector is equal to the zero vector”. • Gradient is a vector with partial derivative ‘j’ in position ‘j’. Review: Partial Derivative • Partial derivative with respect to wj (written 𝜕𝑓 𝜕𝑤𝑗). – Derivative with respect to wj, keeping all others variables fixed. https://en.wikipedia.org/wiki/Partial_derivative Partial Derivative for Least Squares • Partial derivative with respect to w1 for least squares with n=1: Partial Derivative for Least Squares • Partial derivative with respect to wj for least squares with n=1: • Partial derivative with respect to wj for least squares for general ‘n’: Gradient Vector for Least Squares • The gradient vector is the concatenation of all partial derivatives: – At ‘w’, 𝛻𝑓(𝑤) is in the direction with most-positive slope. – At minimizers we have 𝛻𝑓 𝑤 = 0 (slope is 0 every direction). https://en.wikipedia.org/wiki/Gradient Gradient Vector for Least Squares • The gradient vector is the concatenation of all partial derivatives: – At ‘w’, 𝛻𝑓(𝑤) is in the direction with most-positive slope. – At minimizers we have 𝛻𝑓 𝑤 = 0 (slope is 0 every direction). • For linear least squares we have: – So to train a least squares model, we need this to equal the zero vector. Fitting a Linear Least Squares Model • Setting gradient to equal 0 vector for linear least squares gives: – This is a set of ‘d’ linear equations, with ‘d’ unknowns (w1, w2,…, wd). • You can solve these equations using Gaussian elimination (linear algebra). – Claim: all ‘w’ with 𝛻𝑓 𝑤 = 0 are minimizers (we will discuss why later). • May be more than one ‘w’ satisfying this, but all have the same minimum error. Next Topic: Matrix Notation Matrix Notation: Motivation • We have expressed linear least squares with summation notation: • But you often see it equivalently expressed using matrix notation: • Why do people use matrix notation? – Can be easier to understand and lead to “nicer” code (once you are used to it). – Makes it easier to see some properties (like the connection to norms above). • Or derive properties, like showing that all ‘w’ with 𝛻𝑓 𝑤 = 0 are minimizers. – Can lead to code with fewer bugs. • Since you can use existing implementations of standard operations. – Can lead to faster code. • If we are using packages that implement fast matrix operations. Matrix Notation (MEMORIZE/STUDY THIS) • In this course, all vectors are assumed to be column-vectors: • So rows of ‘X’ are actually transposes of the column-vectors xi: Matrix Notation (MEMORIZE/STUDY THIS) • Linear regression prediction for one example in matrix notation: • Why? • Using ො𝑦𝑖 = 𝑤𝑇𝑥𝑖, we can re-write sum of squared residuals as: Matrix Notation (MEMORIZE/STUDY THIS) • Linear regression prediction for all ‘n’ example in matrix notation: • Why? Matrix Notation (MEMORIZE/STUDY THIS) • Linear regression residual vector in matrix notation: • Why? Matrix Notation (MEMORIZE/STUDY THIS) • Different ways to write sum of residuals squared in linear regression model: • So least squares minimizes L2-norm between target and predictions. • Regression considers the case of a numerical yi. • Least squares is a classic method for fitting linear models. – Minimizes sum of squared residuals (prediction and true value difference). – With 1 feature, it has a simple closed-form solution. – Can be generalized to ‘d’ features, taking linear weighting of features. • Gradient is vector containing partial derivatives of all variables. • Matrix notation for expressing least squares problem: ||Xw – y||2. • Next time: Summary • In Smithsonian National Air and Space Museum (Washington, DC):Causality, Interventions, and RCTs • What if you want to assess causality? • You can sometimes do this by collecting data in specific ways. – You need to set the values of the features “by intervention”. • You do not passively observe, you *set* them and then watch the effect. – Most common way this is done is with a randomized control trial. • Say you want to evaluate the effectiveness of a pill for a certain disease. • You get a bunch of people with the disease for training data. • You randomly decide which of the people will take the pill, and which won’t. • If the people who got the pill did better/worse on average, it was caused by the pill. – The randomness takes away the possibility that certain groups are more/less likely to take the pill. – Group not taking the pill often given placebo, removing effect of “feel like you are being treated”. – Often the researchers do not even get to know who took the pills until after the study is over. » “Double blind”, to avoid the researchers giving hints about who got the pill. Converting Partial Derivative to Matrix Notation • Re-writing linear least squares partial derivative in matrix notation: Converting Gradient to Matrix Notation • Re-writing linear least squares gradient in matrix notation:","libVersion":"0.2.1","langs":""}