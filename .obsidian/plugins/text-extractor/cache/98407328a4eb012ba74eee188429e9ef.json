{"path":".obsidian/plugins/text-extractor/cache/98407328a4eb012ba74eee188429e9ef.json","text":"CPSC 340: Machine Learning and Data Mining Linear Regression Admin â€¢ No class Monday. â€“ Truth & Reconcillation Day. â€¢ Assignment 2: â€“ Due tonight. â€“ 1 late day to hand in Wednesday, 2 for next Friday. â€¢ Weâ€™re going to start using calculus and linear algebra a lot. â€“ You should start reviewing these ASAP if you are rusty. â€“ A review of relevant calculus concepts is here. â€“ A review of relevant linear algebra concepts is here. Supervised Learning Round 2: Regression â€¢ Weâ€™re going to revisit supervised learning: â€¢ Previously, we considered classification: â€“ We assumed yi was categorical: yi = â€˜spamâ€™ or yi = â€˜not spamâ€™. â€¢ Now we are going to consider regression: â€“ We allow yi to be numerical: yi = 10.34cm. Example: Dependent vs. Explanatory Variables â€¢ We want to predict a numerical value given features: â€“ Does number of lung cancer deaths change with number of cigarettes? â€“ Does number of skin cancer deaths change with latitude? http://www.cvgs.k12.va.us:81/digstats/main/inferant/d_regrs.html https://onlinecourses.science.psu.edu/stat501/node/11 Example: Dependent vs. Explanatory Variables â€¢ We want to predict a numerical value given features: â€“ Do people in big cities walk faster? â€“ Is the universe expanding or shrinking or staying the same size? http://hosting.astro.cornell.edu/academics/courses/astro201/hubbles_law.htm https://www.nature.com/articles/259557a0.pdf Example: Dependent vs. Explanatory Variables â€¢ We want to predict a numerical value given features: â€“ Does number of gun deaths change with gun ownership? â€“ Does number violent crimes change with violent video games? http://www.vox.com/2015/10/3/9444417/gun-violence-united-states-america https://www.soundandvision.com/content/violence-and-video-games Spurious Assocations â€¢ Does number of pirates help predict global temperature? â€“ Spurious association. â€¢ Variables that seem related, but this is a coincidence. â€“ If comparing enough variables, can get strong relationships by chance. â€¢ Fun with spurious correlations here. https://www.forbes.com/sites/erikaandersen/2012/03/23/true-fact-the-lack-of-pirates-is-causing-global-warming Example: Dependent vs. Explanatory Variables â€¢ We want to predict a numerical value given features: â€“ Does higher gender equality index lead to more women STEM grads? â€¢ Note that we are doing supervised learning: â€“ Trying to predict value of 1 variable (the â€˜yiâ€™ values). (instead of measuring correlation between 2). â€¢ Supervised learning does not give causality: â€“ OK: â€œHigher index is correlated with lower grad %â€. â€“ OK: â€œHigher index helps predict lower grad %â€. â€“ BAD: â€œHigher index leads to lower grads %â€. â€¢ People/media get these confused all the time, be careful! â€¢ There are lots of potential reasons for this correlation. https://www.weforum.org/agenda/2018/02/does-gender-equality-result-in-fewer-female-stem-grads/ Handling Numerical Labels â€¢ One way to handle numerical yi: discretize. â€“ E.g., for â€˜ageâ€™ could we use {â€˜age â‰¤ 20â€™, â€˜20 < age â‰¤ 30â€™, â€˜age > 30â€™}. â€“ Now we can apply methods for classification to do regression. â€“ But coarse discretization loses resolution. â€“ And fine discretization requires lots of data (â€œcoupon collectingâ€). â€¢ There exist regression versions of classification methods: â€“ Regression trees, neighbour-based methods, and so on. â€¢ Today: one of oldest, but still most popular/important methods: â€“ Linear regression based on squared error. â€“ Interpretable and the building block for more-complex methods. Linear Regression in 1 Dimension â€¢ Assume we only have 1 feature (d = 1): â€“ E.g., xi is number of cigarettes and yi is number of lung cancer deaths. â€¢ Linear regression makes predictions à·œğ‘¦i using a linear function of xi: â€¢ The parameter â€˜wâ€™ is the weight or regression coefficient of xi. â€“ We are temporarily ignoring the y-intercept. â€¢ As xi changes, slope â€˜wâ€™ affects the rate that à·œğ‘¦i increases/decreases: â€“ Positive â€˜wâ€™: à·œğ‘¦i increase as xi increases. â€“ Negative â€˜wâ€™: à·œğ‘¦i decreases as xi increases. Linear Regression in 1 DimensionAside: terminology woes â€¢ Different fields use different terminology and symbols. â€“ Data points â€˜iâ€™ = objects = examples = rows = observations. â€“ Inputs xi = predictors = features = explanatory variables= regressors = independent variables = covariates = columns. â€“ Outputs yi = outcomes = targets = response variables = dependent variables (also called a â€œlabelâ€ if itâ€™s categorical). â€“ Regression coefficients â€˜wâ€™ = weights = parameters = betas. â€¢ With linear regression, the symbols are inconsistent too: â€“ In ML, the data is X and y, and the weights are w. â€“ In statistics, the data is X and y, and the weights are Î². â€“ In optimization, the data is A and b, and the weights are x. Linear Regression Training Challenges â€¢ Linear regression makes predictions by using: â€¢ To train a linear regression model, we need to find weight/slope â€˜wâ€™. â€¢ Challenges in finding â€˜wâ€™ compared to fitting a decision stump: â€“ Cannot enumerate all possible values of â€˜wâ€™ (could be any real number). â€¢ Instead, we will use calculus to find the best â€˜wâ€™. â€“ It is unlikely that a line will go exactly through many data points. â€¢ Due to noise, relationship not being quite linear, or just floating-point issues. â€¢ So it does not make sense to find the â€˜wâ€™ minimizing how many times à·œğ‘¦ğ‘– â‰  ğ‘¦ğ‘–. Residuals and Sum of Squared Residuals â€¢ The residual is the difference between our prediction and true value: â€“ This can be positive or negative. â€“ If this is close to zero, then our prediction is close to the true value. â€¢ We typically look for a â€˜wâ€™ that makes residuals close to zero. â€“ For example, many models minimize the sum of the squared residuals: â€¢ The smaller we make this, the smaller the distance between our predictions and targets. â€“ Plugging in à·œğ‘¦ğ‘– = ğ‘¤ğ‘¥ğ‘– for the case of linear regression, we get: â€¢ The linear least squares model minimizes this function to choose the slope â€˜wâ€™. Linear Least Squares Objective Function â€¢ Linear least squares sets â€˜wâ€™ is to minimize sum of squared residuals: â€¢ If this is zero, we exactly fit data. If this small, line is â€œcloseâ€ to data. â€¢ There are some justifications for choosing this function â€˜fâ€™. â€“ A probabilistic interpretation is coming later in the course. â€¢ But usually, we choose this â€˜fâ€™ because it is easy to minimize. Linear Least Squares Objective Function â€¢ Linear least squares sets â€˜wâ€™ is to minimize sum of squared residuals: Linear Least Squares Objective Function â€¢ Linear least squares sets â€˜wâ€™ is to minimize sum of squared residuals: Minimizing a Differential Function â€¢ Math 101 approach to minimizing a differentiable function â€˜fâ€™: 1. Take the derivative of â€˜fâ€™. 2. Find points â€˜wâ€™ where the derivative fâ€™(w) is equal to 0. 3. Choose the smallest one (and check that fâ€™â€™(w) is positive). Digression: Multiplying by a Positive Constant â€¢ Note that this problem: â€¢ Has the same set of minimizers as this problem: â€¢ And these also have the same minimizers: â€¢ I can multiply â€˜fâ€™ by any positive constant and not change solution. â€“ Derivative will still be zero at the same locations. â€“ We will use this trick a lot! (Quora trolling on ethics of this) Deriving Least Squares SolutionFinding Least Squares Solution â€¢ Finding â€˜wâ€™ that minimizes sum of squared errors: â€¢ Letâ€™s check that this is a minimizer by checking second derivative: â€“ Since (anything)2 is non-negative, we have fâ€™â€™(w) â‰¥ 0. â€“ If at least one feature is not zero, then fâ€™â€™(w) > 0 and â€˜wâ€™ is a minimizer. Next Topic: Least Squares in d-Dimensions Motivation: Combining Explanatory Variables â€¢ Smoking is not the only contributor to lung cancer. â€“ For example, there environmental factors like exposure to asbestos. â€¢ How can we model the combined effect of smoking and asbestos? â€¢ A simple way is with a 2-dimensional linear function: â€¢ We have a weight w1 for feature â€˜1â€™ and w2 for feature â€˜2â€™: Linear Regression in 2-Dimensions â€¢ Linear model: â€¢ This defines a two-dimensional plane. Linear Regression in 2-Dimensions â€¢ Linear model: â€¢ This defines a two-dimensional plane. â€¢ Not just a line! Linear Regression in d-Dimensions â€¢ If we have â€˜dâ€™ features, the d-dimensional linear model is: â€“ In words, prediction is a weighted sum of the features. â€¢ We can re-write this using summation notation as: â€¢ We can again choose â€˜wâ€™ to minimize the sum of squared residuals: â€“ Dates back to 1801: Gauss used it to predict location of the asteroid Ceres. â€“ We can use multi-variable calculus to minimize â€˜fâ€™ with respect to the parameters w1, w2,â€¦, wd. Minimizing Multi-Variable Differentiable Function â€¢ With one variable, we â€œfind â€˜wâ€™ where the derivative is equal to 0â€. â€¢ The generalization of this idea to when we have â€˜dâ€™ variables: â€“ â€œFind â€˜wâ€™ where the gradient vector is equal to the zero vectorâ€. â€¢ Gradient is a vector with partial derivative â€˜jâ€™ in position â€˜jâ€™. Review: Partial Derivative â€¢ Partial derivative with respect to wj (written ğœ•ğ‘“ ğœ•ğ‘¤ğ‘—). â€“ Derivative with respect to wj, keeping all others variables fixed. https://en.wikipedia.org/wiki/Partial_derivative Partial Derivative for Least Squares â€¢ Partial derivative with respect to w1 for least squares with n=1: Partial Derivative for Least Squares â€¢ Partial derivative with respect to wj for least squares with n=1: â€¢ Partial derivative with respect to wj for least squares for general â€˜nâ€™: Gradient Vector for Least Squares â€¢ The gradient vector is the concatenation of all partial derivatives: â€“ At â€˜wâ€™, ğ›»ğ‘“(ğ‘¤) is in the direction with most-positive slope. â€“ At minimizers we have ğ›»ğ‘“ ğ‘¤ = 0 (slope is 0 every direction). https://en.wikipedia.org/wiki/Gradient Gradient Vector for Least Squares â€¢ The gradient vector is the concatenation of all partial derivatives: â€“ At â€˜wâ€™, ğ›»ğ‘“(ğ‘¤) is in the direction with most-positive slope. â€“ At minimizers we have ğ›»ğ‘“ ğ‘¤ = 0 (slope is 0 every direction). â€¢ For linear least squares we have: â€“ So to train a least squares model, we need this to equal the zero vector. Fitting a Linear Least Squares Model â€¢ Setting gradient to equal 0 vector for linear least squares gives: â€“ This is a set of â€˜dâ€™ linear equations, with â€˜dâ€™ unknowns (w1, w2,â€¦, wd). â€¢ You can solve these equations using Gaussian elimination (linear algebra). â€“ Claim: all â€˜wâ€™ with ğ›»ğ‘“ ğ‘¤ = 0 are minimizers (we will discuss why later). â€¢ May be more than one â€˜wâ€™ satisfying this, but all have the same minimum error. Next Topic: Matrix Notation Matrix Notation: Motivation â€¢ We have expressed linear least squares with summation notation: â€¢ But you often see it equivalently expressed using matrix notation: â€¢ Why do people use matrix notation? â€“ Can be easier to understand and lead to â€œnicerâ€ code (once you are used to it). â€“ Makes it easier to see some properties (like the connection to norms above). â€¢ Or derive properties, like showing that all â€˜wâ€™ with ğ›»ğ‘“ ğ‘¤ = 0 are minimizers. â€“ Can lead to code with fewer bugs. â€¢ Since you can use existing implementations of standard operations. â€“ Can lead to faster code. â€¢ If we are using packages that implement fast matrix operations. Matrix Notation (MEMORIZE/STUDY THIS) â€¢ In this course, all vectors are assumed to be column-vectors: â€¢ So rows of â€˜Xâ€™ are actually transposes of the column-vectors xi: Matrix Notation (MEMORIZE/STUDY THIS) â€¢ Linear regression prediction for one example in matrix notation: â€¢ Why? â€¢ Using à·œğ‘¦ğ‘– = ğ‘¤ğ‘‡ğ‘¥ğ‘–, we can re-write sum of squared residuals as: Matrix Notation (MEMORIZE/STUDY THIS) â€¢ Linear regression prediction for all â€˜nâ€™ example in matrix notation: â€¢ Why? Matrix Notation (MEMORIZE/STUDY THIS) â€¢ Linear regression residual vector in matrix notation: â€¢ Why? Matrix Notation (MEMORIZE/STUDY THIS) â€¢ Different ways to write sum of residuals squared in linear regression model: â€¢ So least squares minimizes L2-norm between target and predictions. â€¢ Regression considers the case of a numerical yi. â€¢ Least squares is a classic method for fitting linear models. â€“ Minimizes sum of squared residuals (prediction and true value difference). â€“ With 1 feature, it has a simple closed-form solution. â€“ Can be generalized to â€˜dâ€™ features, taking linear weighting of features. â€¢ Gradient is vector containing partial derivatives of all variables. â€¢ Matrix notation for expressing least squares problem: ||Xw â€“ y||2. â€¢ Next time: Summary â€¢ In Smithsonian National Air and Space Museum (Washington, DC):Causality, Interventions, and RCTs â€¢ What if you want to assess causality? â€¢ You can sometimes do this by collecting data in specific ways. â€“ You need to set the values of the features â€œby interventionâ€. â€¢ You do not passively observe, you *set* them and then watch the effect. â€“ Most common way this is done is with a randomized control trial. â€¢ Say you want to evaluate the effectiveness of a pill for a certain disease. â€¢ You get a bunch of people with the disease for training data. â€¢ You randomly decide which of the people will take the pill, and which wonâ€™t. â€¢ If the people who got the pill did better/worse on average, it was caused by the pill. â€“ The randomness takes away the possibility that certain groups are more/less likely to take the pill. â€“ Group not taking the pill often given placebo, removing effect of â€œfeel like you are being treatedâ€. â€“ Often the researchers do not even get to know who took the pills until after the study is over. Â» â€œDouble blindâ€, to avoid the researchers giving hints about who got the pill. Converting Partial Derivative to Matrix Notation â€¢ Re-writing linear least squares partial derivative in matrix notation: Converting Gradient to Matrix Notation â€¢ Re-writing linear least squares gradient in matrix notation:","libVersion":"0.2.1","langs":""}